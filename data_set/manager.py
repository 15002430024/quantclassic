"""
DataManager - æ•°æ®ç®¡ç†ä¸»æ§ç±»

æ•´åˆæ‰€æœ‰æ•°æ®ç®¡ç†ç»„ä»¶ï¼Œæä¾›ç»Ÿä¸€çš„æ¥å£
"""

import os
import pandas as pd
from typing import Optional, Dict, Any, List, Tuple, Union
import logging
from pathlib import Path
import pickle
from datetime import datetime

from .config import DataConfig, ConfigTemplates
from .loader import DataLoaderEngine
from .feature_engineer import FeatureEngineer
from .splitter import create_splitter, DataSplitter
from .validator import DataValidator, ValidationReport
from .factory import DatasetFactory, DatasetCollection, LoaderCollection
# âš ï¸ RollingWindowTrainer å·²ç§»é™¤ - è¯·ä½¿ç”¨ model.train.RollingWindowTrainer


# =============================================================================
# ğŸ†• æ—¥æ‰¹æ¬¡æ„å»ºè¾…åŠ©å‡½æ•°ï¼ˆå»é‡ create_daily_loaders ä¸ create_rolling_daily_loadersï¼‰
# =============================================================================
def _normalize_graph_builder_config(
    gb_config: Optional[Union[Dict, Any]],
    raw_data: Optional[pd.DataFrame] = None,
    stock_col: str = 'ts_code',
    logger: Optional[logging.Logger] = None
) -> Optional[Dict]:
    """
    ç»Ÿä¸€å¤„ç† graph_builder_configï¼Œç¡®ä¿è¿”å› dict ç±»å‹ï¼Œå¹¶æ³¨å…¥è¡Œä¸šæ˜ å°„ï¼ˆå¦‚éœ€è¦ï¼‰
    
    Args:
        gb_config: å›¾æ„å»ºé…ç½®ï¼ˆdict æˆ– dataclassï¼‰
        raw_data: åŸå§‹æ•°æ®ï¼ˆç”¨äºæå–è¡Œä¸šæ˜ å°„ï¼‰
        stock_col: è‚¡ç¥¨ä»£ç åˆ—å
        logger: æ—¥å¿—è®°å½•å™¨
        
    Returns:
        æ ‡å‡†åŒ–åçš„ dict é…ç½®ï¼Œæˆ– None
    """
    if gb_config is None:
        return None
    
    # ç»Ÿä¸€è½¬æ¢ä¸º dict
    if isinstance(gb_config, dict):
        gb_dict = gb_config.copy()
    elif hasattr(gb_config, 'to_dict'):
        gb_dict = gb_config.to_dict()
    else:
        gb_dict = dict(gb_config)
    
    # å¦‚æœæ˜¯è¡Œä¸šå›¾ï¼Œé¢„å…ˆæ„å»ºå…¨å±€è‚¡ç¥¨-è¡Œä¸šæ˜ å°„
    if gb_dict.get('type') == 'industry':
        industry_col = gb_dict.get('industry_col', 'industry_name')
        if raw_data is not None and industry_col in raw_data.columns:
            stock_industry_mapping = dict(zip(
                raw_data[stock_col],
                raw_data[industry_col]
            ))
            gb_dict['stock_industry_mapping'] = stock_industry_mapping
            if logger:
                logger.info(f"  å·²æ„å»ºå…¨å±€è‚¡ç¥¨-è¡Œä¸šæ˜ å°„: {len(stock_industry_mapping)} åªè‚¡ç¥¨")
    
    return gb_dict


class DataManager:
    """
    æ•°æ®ç®¡ç†ä¸»æ§ç±»
    
    æ•´åˆæ•°æ®åŠ è½½ã€ç‰¹å¾å·¥ç¨‹ã€æ•°æ®åˆ’åˆ†ã€éªŒè¯å’Œæ•°æ®é›†åˆ›å»ºçš„å®Œæ•´æµç¨‹
    """
    
    def __init__(self, config: Optional[DataConfig] = None, **kwargs):
        """
        Args:
            config: DataConfigé…ç½®å¯¹è±¡æˆ–å­—å…¸ï¼ˆNoneåˆ™ä½¿ç”¨é»˜è®¤é…ç½®ï¼‰
            **kwargs: é¢å¤–çš„é…ç½®å‚æ•°,ä¼šè¦†ç›– config ä¸­çš„å€¼
        """
        # æ”¯æŒä¸‰ç§åˆå§‹åŒ–æ–¹å¼:
        # 1. DataManager(config=DataConfig(...))
        # 2. DataManager(config={'base_dir': '...'})
        # 3. DataManager(base_dir='...', data_file='...', ...)
        
        if isinstance(config, dict):
            # å­—å…¸å½¢å¼: åˆå¹¶ config å’Œ kwargs
            merged_config = {**config, **kwargs}
            self.config = DataConfig(**merged_config)
        elif config is not None:
            # DataConfig å¯¹è±¡: ç”¨ kwargs æ›´æ–°
            self.config = config
            if kwargs:
                self.config.update(**kwargs)
        else:
            # ä»… kwargs: åˆ›å»ºæ–° DataConfig
            self.config = DataConfig(**kwargs) if kwargs else DataConfig()
        
        self.logger = self._setup_logger()
        
        # åˆå§‹åŒ–å„ç»„ä»¶
        self.loader = DataLoaderEngine(self.config)
        self.feature_engineer = FeatureEngineer(self.config)
        self.validator = DataValidator(self.config)
        self.factory = DatasetFactory(self.config)
        
        # æ•°æ®ç¼“å­˜
        self._raw_data: Optional[pd.DataFrame] = None
        self._train_df: Optional[pd.DataFrame] = None
        self._val_df: Optional[pd.DataFrame] = None
        self._test_df: Optional[pd.DataFrame] = None
        self._feature_cols: Optional[List[str]] = None
        self._datasets: Optional[DatasetCollection] = None
        
        self.logger.info("âœ… DataManager åˆå§‹åŒ–å®Œæˆ")
    
    def _setup_logger(self) -> logging.Logger:
        """é…ç½®æ—¥å¿—"""
        logger = logging.getLogger('DataManager')
        logger.setLevel(getattr(logging, self.config.log_level))
        
        if not logger.handlers:
            handler = logging.StreamHandler()
            formatter = logging.Formatter(
                '%(asctime)s - %(name)s - %(levelname)s - %(message)s',
                datefmt='%Y-%m-%d %H:%M:%S'
            )
            handler.setFormatter(formatter)
            logger.addHandler(handler)
        
        return logger
    
    def load_raw_data(self, file_path: Optional[str] = None,
                     use_cache: bool = True) -> pd.DataFrame:
        """
        åŠ è½½åŸå§‹æ•°æ®
        
        Args:
            file_path: æ•°æ®æ–‡ä»¶è·¯å¾„ï¼ˆNoneåˆ™ä½¿ç”¨é…ç½®è·¯å¾„ï¼‰
            use_cache: æ˜¯å¦ä½¿ç”¨ç¼“å­˜
            
        Returns:
            åŸå§‹æ•°æ®DataFrame
        """
        self.logger.info("=" * 80)
        self.logger.info("æ­¥éª¤ 1/5: åŠ è½½åŸå§‹æ•°æ®")
        self.logger.info("=" * 80)
        
        if use_cache and self._raw_data is not None:
            self.logger.info("ä½¿ç”¨ç¼“å­˜æ•°æ®")
            return self._raw_data
        
        # åŠ è½½æ•°æ®
        self._raw_data = self.loader.load_data(file_path, use_cache)
        
        # æ‰“å°æ•°æ®æ‘˜è¦
        if self.config.verbose:
            self.loader.print_data_summary(self._raw_data)
        
        return self._raw_data
    
    def validate_data_quality(self, df: Optional[pd.DataFrame] = None,
                             feature_cols: Optional[List[str]] = None) -> ValidationReport:
        """
        éªŒè¯æ•°æ®è´¨é‡
        
        Args:
            df: å¾…éªŒè¯æ•°æ®ï¼ˆNoneåˆ™ä½¿ç”¨å·²åŠ è½½çš„åŸå§‹æ•°æ®ï¼‰
            feature_cols: ç‰¹å¾åˆ—åˆ—è¡¨
            
        Returns:
            ValidationReportå¯¹è±¡
        """
        self.logger.info("=" * 80)
        self.logger.info("æ­¥éª¤ 2/5: éªŒè¯æ•°æ®è´¨é‡")
        self.logger.info("=" * 80)
        
        if df is None:
            if self._raw_data is None:
                raise ValueError("æœªåŠ è½½æ•°æ®ï¼Œè¯·å…ˆè°ƒç”¨ load_raw_data()")
            df = self._raw_data
        
        # æ‰§è¡ŒéªŒè¯
        report = self.validator.validate(df, feature_cols or self._feature_cols)
        
        # æ‰“å°æŠ¥å‘Š
        if self.config.verbose:
            report.print_report()
        
        # ä¿å­˜æŠ¥å‘Š
        if self.config.save_data_report:
            self._save_validation_report(report)
        
        return report
    
    def preprocess_features(self, df: Optional[pd.DataFrame] = None,
                          auto_filter: bool = True) -> List[str]:
        """
        ç‰¹å¾é¢„å¤„ç†å’Œé€‰æ‹©
        
        Args:
            df: æ•°æ®DataFrameï¼ˆNoneåˆ™ä½¿ç”¨å·²åŠ è½½çš„åŸå§‹æ•°æ®ï¼‰
            auto_filter: æ˜¯å¦è‡ªåŠ¨è¿‡æ»¤ä½è´¨é‡ç‰¹å¾
            
        Returns:
            ç‰¹å¾åˆ—åˆ—è¡¨
        """
        self.logger.info("=" * 80)
        self.logger.info("æ­¥éª¤ 3/5: ç‰¹å¾å·¥ç¨‹")
        self.logger.info("=" * 80)
        
        if df is None:
            if self._raw_data is None:
                raise ValueError("æœªåŠ è½½æ•°æ®ï¼Œè¯·å…ˆè°ƒç”¨ load_raw_data()")
            df = self._raw_data
        
        # é€‰æ‹©ç‰¹å¾
        self._feature_cols = self.feature_engineer.select_features(df)
        
        # è®¡ç®—ç‰¹å¾ç»Ÿè®¡
        self.feature_engineer.compute_feature_stats(df)
        
        # è¿‡æ»¤ç‰¹å¾
        if auto_filter:
            self._feature_cols = self.feature_engineer.filter_features(df)
        
        # ä¿å­˜ç‰¹å¾ä¿¡æ¯
        if self.config.enable_cache:
            self.feature_engineer.save_feature_info()
        
        return self._feature_cols
    
    def create_datasets(self, df: Optional[pd.DataFrame] = None,
                       feature_cols: Optional[List[str]] = None,
                       split_strategy: Optional[str] = None) -> DatasetCollection:
        """
        åˆ›å»ºè®­ç»ƒã€éªŒè¯ã€æµ‹è¯•æ•°æ®é›†
        
        Args:
            df: æ•°æ®DataFrameï¼ˆNoneåˆ™ä½¿ç”¨å·²åŠ è½½çš„åŸå§‹æ•°æ®ï¼‰
            feature_cols: ç‰¹å¾åˆ—åˆ—è¡¨ï¼ˆNoneåˆ™ä½¿ç”¨é¢„å¤„ç†çš„ç‰¹å¾ï¼‰
            split_strategy: åˆ’åˆ†ç­–ç•¥ï¼ˆNoneåˆ™ä½¿ç”¨é…ç½®çš„ç­–ç•¥ï¼‰
            
        Returns:
            DatasetCollectionå¯¹è±¡
        """
        self.logger.info("=" * 80)
        self.logger.info("æ­¥éª¤ 4/5: æ•°æ®åˆ’åˆ†")
        self.logger.info("=" * 80)
        
        if df is None:
            if self._raw_data is None:
                raise ValueError("æœªåŠ è½½æ•°æ®ï¼Œè¯·å…ˆè°ƒç”¨ load_raw_data()")
            df = self._raw_data
        
        if feature_cols is None:
            if self._feature_cols is None:
                raise ValueError("æœªé€‰æ‹©ç‰¹å¾ï¼Œè¯·å…ˆè°ƒç”¨ preprocess_features()")
            feature_cols = self._feature_cols
        
        # ã€ä¿®å¤ã€‘è¿‡æ»¤æ‰æ ‡ç­¾ç¼ºå¤±çš„æ•°æ®ï¼ˆé˜²æ­¢æ»šåŠ¨çª—å£è®­ç»ƒæ—¶å‡ºç°ç©ºæ•°æ®é›†ï¼‰
        original_len = len(df)
        df = df[df[self.config.label_col].notna()].copy()
        filtered_len = len(df)
        if filtered_len < original_len:
            self.logger.info(f"   è¿‡æ»¤æ ‡ç­¾ç¼ºå¤±æ•°æ®: {original_len:,} -> {filtered_len:,} (-{original_len-filtered_len:,})")
        
        # åˆ›å»ºåˆ’åˆ†å™¨
        if split_strategy:
            original_strategy = self.config.split_strategy
            self.config.split_strategy = split_strategy
            splitter = create_splitter(self.config)
            self.config.split_strategy = original_strategy
        else:
            splitter = create_splitter(self.config)
        
        # åˆ’åˆ†æ•°æ®
        split_result = splitter.split(df)
        
        # å¤„ç†ä¸åŒsplitterçš„è¿”å›å€¼
        if self.config.split_strategy == 'rolling':
            # RollingWindowSplitter è¿”å› List[Tuple[train, test]]
            # ç­–ç•¥: ä½¿ç”¨æ‰€æœ‰çª—å£çš„æ•°æ®è¿›è¡Œæ‰©å±•è®­ç»ƒ
            if not split_result:
                raise ValueError("æ»šåŠ¨çª—å£åˆ’åˆ†å¤±è´¥ï¼šæ— æœ‰æ•ˆçª—å£")
            
            self.logger.info(f"   ç”Ÿæˆ {len(split_result)} ä¸ªæ»šåŠ¨çª—å£")
            
            # ä¿å­˜æ‰€æœ‰çª—å£ä¾›åç»­walk-forwardä½¿ç”¨
            self._rolling_windows = split_result
            
            # åˆå¹¶ç­–ç•¥ï¼šä½¿ç”¨å‰80%çª—å£çš„è®­ç»ƒæ•°æ®ï¼Œå20%çª—å£çš„æµ‹è¯•æ•°æ®
            n_windows = len(split_result)
            train_window_count = max(1, int(n_windows * 0.8))
            
            self.logger.info(f"   ä½¿ç”¨å‰ {train_window_count} ä¸ªçª—å£çš„è®­ç»ƒæ•°æ®")
            self.logger.info(f"   ä½¿ç”¨å {n_windows - train_window_count} ä¸ªçª—å£çš„æµ‹è¯•æ•°æ®")
            
            # åˆå¹¶è®­ç»ƒæ•°æ®ï¼ˆå‰80%çª—å£ï¼‰
            train_dfs = []
            for i in range(train_window_count):
                train_df, _ = split_result[i]
                train_dfs.append(train_df)
            combined_train = pd.concat(train_dfs, ignore_index=True)
            
            # åˆå¹¶æµ‹è¯•æ•°æ®ï¼ˆå20%çª—å£ï¼‰
            # ğŸ”´ ä¿®å¤: æµ‹è¯•é›†éœ€è¦åŒ…å« lookback window çš„å†å²æ•°æ®
            # å¦åˆ™ TimeSeriesStockDataset ä¼šå› ä¸ºæ•°æ®é•¿åº¦ä¸è¶³è€Œä¸¢å¼ƒæ ·æœ¬
            test_dfs = []
            original_test_dfs = []  # ä¿å­˜åŸå§‹æµ‹è¯•é›†ï¼ˆç”¨äºç¡®å®šæœ‰æ•ˆæ ‡ç­¾æ—¥æœŸèŒƒå›´ï¼‰
            window_size = self.config.window_size
            
            for i in range(train_window_count, n_windows):
                _, test_df = split_result[i]
                original_test_dfs.append(test_df)  # ä¿å­˜åŸå§‹æµ‹è¯•é›†
                
                # è·å–æµ‹è¯•é›†çš„å¼€å§‹æ—¥æœŸ
                if not test_df.empty:
                    test_start_date = test_df[self.config.time_col].min()
                    test_end_date = test_df[self.config.time_col].max()
                    
                    # å‘å‰å›æº¯ window_size * 2 å¤©ï¼ˆç•™æœ‰ä½™é‡ï¼Œè€ƒè™‘éäº¤æ˜“æ—¥ï¼‰
                    lookback_date = test_start_date - pd.Timedelta(days=window_size * 2)
                    
                    extended_test_df = df[
                        (df[self.config.time_col] >= lookback_date) & 
                        (df[self.config.time_col] <= test_end_date)
                    ].copy()
                    
                    test_dfs.append(extended_test_df)
            
            if test_dfs:
                combined_test = pd.concat(test_dfs, ignore_index=True)
                # å»é‡ï¼Œå› ä¸ºå¯èƒ½æœ‰é‡å 
                combined_test = combined_test.drop_duplicates(subset=[self.config.stock_col, self.config.time_col])
                
                # ğŸ†• è®°å½•åŸå§‹æµ‹è¯•é›†çš„æœ‰æ•ˆæ ‡ç­¾èµ·å§‹æ—¥æœŸ
                if original_test_dfs:
                    original_test_combined = pd.concat(original_test_dfs, ignore_index=True)
                    self._test_valid_label_start_date = pd.Timestamp(original_test_combined[self.config.time_col].min())
                    self.logger.info(f"   æµ‹è¯•é›†æœ‰æ•ˆæ ‡ç­¾èµ·å§‹æ—¥æœŸ: {self._test_valid_label_start_date}")
                else:
                    self._test_valid_label_start_date = None
            else:
                combined_test = split_result[-1][1]
                self._test_valid_label_start_date = None
            
            # å°†è®­ç»ƒé›†è¿›ä¸€æ­¥åˆ’åˆ†ä¸º train/val
            n_train = len(combined_train)
            val_size = int(n_train * self.config.val_ratio / (self.config.train_ratio + self.config.val_ratio))
            
            self._train_df = combined_train.iloc[:-val_size].copy() if val_size > 0 else combined_train
            self._val_df = combined_train.iloc[-val_size:].copy() if val_size > 0 else combined_train.head(0)
            self._test_df = combined_test
            
            self.logger.info(f"   è®­ç»ƒé›†: {len(self._train_df):,} æ ·æœ¬")
            self.logger.info(f"   éªŒè¯é›†: {len(self._val_df):,} æ ·æœ¬")
            self.logger.info(f"   æµ‹è¯•é›†(å«å†å²): {len(self._test_df):,} æ ·æœ¬")
        else:
            # å…¶ä»–splitterè¿”å› (train, val, test)
            self._train_df, self._val_df, self._test_df = split_result
            self._test_valid_label_start_date = None
        
        # åˆ›å»ºæ•°æ®é›†
        self.logger.info("=" * 80)
        self.logger.info("æ­¥éª¤ 5/5: åˆ›å»ºæ•°æ®é›†")
        self.logger.info("=" * 80)
        
        # ğŸ†• ä¼ é€’æµ‹è¯•é›†çš„æœ‰æ•ˆæ ‡ç­¾èµ·å§‹æ—¥æœŸ
        self._datasets = self.factory.create_datasets(
            self._train_df, self._val_df, self._test_df, feature_cols,
            test_valid_label_start_date=getattr(self, '_test_valid_label_start_date', None)
        )
        
        return self._datasets
    
    def get_dataloaders(self, batch_size: Optional[int] = None,
                       num_workers: Optional[int] = None,
                       shuffle_train: Optional[bool] = None,
                       use_cross_sectional: bool = False) -> LoaderCollection:
        """
        è·å–æ•°æ®åŠ è½½å™¨
        
        Args:
            batch_size: æ‰¹é‡å¤§å°ï¼ˆNoneåˆ™ä½¿ç”¨é…ç½®å€¼ï¼‰
            num_workers: å·¥ä½œè¿›ç¨‹æ•°ï¼ˆNoneåˆ™ä½¿ç”¨é…ç½®å€¼ï¼‰
            shuffle_train: æ˜¯å¦æ‰“ä¹±è®­ç»ƒé›†ï¼ˆNoneåˆ™ä½¿ç”¨é…ç½®å€¼ï¼‰
            use_cross_sectional: ğŸ†• æ˜¯å¦ä½¿ç”¨æˆªé¢æ‰¹é‡‡æ ·ï¼ˆIC/ç›¸å…³æ€§æŸå¤±åœºæ™¯å¿…é¡»å¼€å¯ï¼‰
            
        Returns:
            LoaderCollectionå¯¹è±¡
        """
        if self._datasets is None:
            raise ValueError("æœªåˆ›å»ºæ•°æ®é›†ï¼Œè¯·å…ˆè°ƒç”¨ create_datasets()")
        
        return self._datasets.get_loaders(
            batch_size=batch_size or self.config.batch_size,
            num_workers=num_workers or self.config.num_workers,
            shuffle_train=shuffle_train if shuffle_train is not None else self.config.shuffle_train,
            use_cross_sectional=use_cross_sectional  # ğŸ†• é€ä¼ æˆªé¢é‡‡æ ·å‚æ•°
        )
    
    def create_daily_loaders(
        self,
        graph_builder_config: Optional[Dict] = None,
        shuffle_dates: Optional[bool] = None,
        device: str = 'cuda'
    ):
        """
        åˆ›å»ºæ—¥æ‰¹æ¬¡æ•°æ®åŠ è½½å™¨ï¼ˆç”¨äºåŠ¨æ€å›¾ GNN è®­ç»ƒï¼‰
        
        æ¯ä¸ª batch æ˜¯ä¸€ä¸ªäº¤æ˜“æ—¥çš„æ‰€æœ‰è‚¡ç¥¨æ•°æ®ï¼Œæ”¯æŒåŠ¨æ€å›¾æ„å»ºã€‚
        
        Args:
            graph_builder_config: å›¾æ„å»ºå™¨é…ç½®ï¼ŒNone åˆ™ä½¿ç”¨ self.config.graph_builder_config
            shuffle_dates: æ˜¯å¦æ‰“ä¹±æ—¥æœŸé¡ºåºï¼ŒNone åˆ™ä½¿ç”¨ self.config.shuffle_dates
            device: è®¡ç®—è®¾å¤‡
            
        Returns:
            NamedTuple(train, val, test) åŒ…å«ä¸‰ä¸ª DailyGraphDataLoader
            
        Example:
            >>> dm = DataManager(config=data_config)
            >>> dm.run_full_pipeline()
            >>> daily_loaders = dm.create_daily_loaders(
            ...     graph_builder_config={'type': 'hybrid', 'alpha': 0.7, 'top_k': 10}
            ... )
            >>> for X, y, adj, stocks, date in daily_loaders.train:
            ...     pred = model(X, adj)
        """
        if self._train_df is None or self._feature_cols is None:
            raise ValueError("æœªå‡†å¤‡æ•°æ®ï¼Œè¯·å…ˆè°ƒç”¨ run_full_pipeline()")
        
        from quantclassic.data_set.graph import (
            DailyBatchDataset, DailyGraphDataLoader
        )
        from quantclassic.data_processor.graph_builder import GraphBuilderFactory
        
        # ä½¿ç”¨é…ç½®
        gb_config = graph_builder_config or getattr(self.config, 'graph_builder_config', None)
        shuffle = shuffle_dates if shuffle_dates is not None else getattr(self.config, 'shuffle_dates', True)
        
        # åˆ›å»ºå›¾æ„å»ºå™¨
        graph_builder = None
        if gb_config:
            # ğŸ†• ä½¿ç”¨å…¬å…±è¾…åŠ©å‡½æ•°ç»Ÿä¸€å¤„ç†é…ç½®
            gb_dict = _normalize_graph_builder_config(
                gb_config, self._raw_data, self.config.stock_col, self.logger
            )
            # ğŸ†• ç¡®ä¿ stock_col é€ä¼ åˆ°å›¾æ„å»ºå™¨ï¼Œé¿å… ts_code åœºæ™¯é€€å›é»˜è®¤å€¼
            gb_dict.setdefault('stock_col', self.config.stock_col)
            graph_builder = GraphBuilderFactory.create(gb_dict)
            self.logger.info(f"å›¾æ„å»ºå™¨ç±»å‹: {gb_dict.get('type', 'corr')}, stock_col: {gb_dict.get('stock_col')}")
        
        # åˆ›å»ºæ•°æ®é›†
        def make_daily_dataset(df):
            return DailyBatchDataset(
                df=df,
                feature_cols=self._feature_cols,
                label_col=self.config.label_col,
                window_size=self.config.window_size,
                time_col=self.config.time_col,
                stock_col=self.config.stock_col,
                enable_window_transform=self.config.enable_window_transform,
                window_price_log=self.config.window_price_log,
                window_volume_norm=self.config.window_volume_norm,
                price_cols=self.config.price_cols,
                close_col=self.config.close_col,
                volume_cols=self.config.volume_cols,
                label_rank_normalize=self.config.label_rank_normalize,
                label_rank_output_range=self.config.label_rank_output_range,
            )
        
        # åˆ›å»ºä¸‰ä¸ªæ•°æ®é›†
        train_dataset = make_daily_dataset(self._train_df)
        val_dataset = make_daily_dataset(self._val_df) if len(self._val_df) > 0 else None
        test_dataset = make_daily_dataset(self._test_df) if len(self._test_df) > 0 else None
        
        # åˆ›å»ºåŠ è½½å™¨
        train_loader = DailyGraphDataLoader(
            dataset=train_dataset,
            graph_builder=graph_builder,
            feature_cols=self._feature_cols,
            shuffle_dates=shuffle,
            device=device
        )
        
        val_loader = None
        if val_dataset and len(val_dataset) > 0:
            val_loader = DailyGraphDataLoader(
                dataset=val_dataset,
                graph_builder=graph_builder,
                feature_cols=self._feature_cols,
                shuffle_dates=False,  # éªŒè¯é›†ä¸æ‰“ä¹±
                device=device
            )
        
        test_loader = None
        if test_dataset and len(test_dataset) > 0:
            test_loader = DailyGraphDataLoader(
                dataset=test_dataset,
                graph_builder=graph_builder,
                feature_cols=self._feature_cols,
                shuffle_dates=False,  # æµ‹è¯•é›†ä¸æ‰“ä¹±
                device=device
            )
        
        self.logger.info(f"æ—¥æ‰¹æ¬¡åŠ è½½å™¨åˆ›å»ºå®Œæˆ:")
        self.logger.info(f"  è®­ç»ƒé›†: {len(train_loader)} å¤©")
        if val_loader:
            self.logger.info(f"  éªŒè¯é›†: {len(val_loader)} å¤©")
        if test_loader:
            self.logger.info(f"  æµ‹è¯•é›†: {len(test_loader)} å¤©")
        
        # è¿”å›å‘½åå…ƒç»„
        from collections import namedtuple
        DailyLoaderCollection = namedtuple('DailyLoaderCollection', ['train', 'val', 'test'])
        return DailyLoaderCollection(train_loader, val_loader, test_loader)
    
    def create_rolling_daily_loaders(
        self,
        graph_builder_config: Optional[Dict] = None,
        val_ratio: float = 0.15,
        device: str = 'cuda'
    ):
        """
        åˆ›å»ºæ»šåŠ¨çª—å£çš„æ—¥æ‰¹æ¬¡æ•°æ®åŠ è½½å™¨åˆ—è¡¨ï¼ˆçœŸæ­£çš„ Walk-Forwardï¼‰
        
        ä¸ create_daily_loaders çš„åŒºåˆ«ï¼š
        - create_daily_loaders: åˆå¹¶æ‰€æœ‰çª—å£ï¼Œè¿”å›å•ä¸ª loader ä¸‰å…ƒç»„
        - create_rolling_daily_loaders: ä¿ç•™çª—å£ç‹¬ç«‹æ€§ï¼Œè¿”å› loader ä¸‰å…ƒç»„åˆ—è¡¨
        
        Args:
            graph_builder_config: å›¾æ„å»ºå™¨é…ç½®
            val_ratio: ä»æ¯ä¸ªçª—å£çš„è®­ç»ƒé›†ä¸­åˆ’åˆ†éªŒè¯é›†çš„æ¯”ä¾‹
            device: è®¡ç®—è®¾å¤‡
            
        Returns:
            RollingDailyLoaderCollection å¯¹è±¡ï¼ŒåŒ…å«:
            - windows: List[DailyLoaderCollection]ï¼Œæ¯ä¸ªå…ƒç´ æ˜¯ (train, val, test)
            - n_windows: çª—å£æ•°é‡
            - __iter__: æ”¯æŒéå†
            
        Example:
            >>> rolling_loaders = dm.create_rolling_daily_loaders()
            >>> for i, loaders in enumerate(rolling_loaders):
            ...     print(f"Window {i+1}: train={len(loaders.train)} days")
            ...     trainer.fit(loaders.train, loaders.val)
            ...     preds = trainer.predict(loaders.test)
        """
        if self.config.split_strategy != 'rolling':
            raise ValueError(
                f"å½“å‰ split_strategy='{self.config.split_strategy}'ï¼Œ"
                "è¯·ä½¿ç”¨ split_strategy='rolling' ä»¥å¯ç”¨æ»šåŠ¨çª—å£æ¨¡å¼"
            )
        
        if not hasattr(self, '_rolling_windows') or not self._rolling_windows:
            raise ValueError(
                "æ»šåŠ¨çª—å£æ•°æ®ä¸å¯ç”¨ã€‚è¯·å…ˆè°ƒç”¨ run_full_pipeline() ç”Ÿæˆæ»šåŠ¨çª—å£ã€‚"
            )
        
        if self._feature_cols is None:
            raise ValueError("ç‰¹å¾åˆ—ä¸å¯ç”¨ï¼Œè¯·å…ˆè°ƒç”¨ run_full_pipeline()")
        
        from quantclassic.data_set.graph import (
            DailyBatchDataset, DailyGraphDataLoader
        )
        from quantclassic.data_processor.graph_builder import GraphBuilderFactory
        from collections import namedtuple
        
        # ğŸ†• ä½¿ç”¨å…¬å…±è¾…åŠ©å‡½æ•°ç»Ÿä¸€å¤„ç†é…ç½®
        gb_config = graph_builder_config or getattr(self.config, 'graph_builder_config', None)
        gb_dict = _normalize_graph_builder_config(
            gb_config, self._raw_data, self.config.stock_col, self.logger
        )
        graph_builder = GraphBuilderFactory.create(gb_dict) if gb_dict else None
        
        DailyLoaderCollection = namedtuple('DailyLoaderCollection', ['train', 'val', 'test'])
        
        def make_daily_dataset(df, valid_label_start_date=None):
            """åˆ›å»ºæ—¥æ‰¹æ¬¡æ•°æ®é›†"""
            return DailyBatchDataset(
                df=df,
                feature_cols=self._feature_cols,
                label_col=self.config.label_col,
                window_size=self.config.window_size,
                time_col=self.config.time_col,
                stock_col=self.config.stock_col,
                enable_window_transform=self.config.enable_window_transform,
                window_price_log=self.config.window_price_log,
                window_volume_norm=self.config.window_volume_norm,
                price_cols=self.config.price_cols,
                close_col=self.config.close_col,
                volume_cols=self.config.volume_cols,
                label_rank_normalize=self.config.label_rank_normalize,
                label_rank_output_range=self.config.label_rank_output_range,
                valid_label_start_date=valid_label_start_date
            )
        
        def make_loader(dataset, shuffle):
            """åˆ›å»ºæ—¥æ‰¹æ¬¡åŠ è½½å™¨"""
            if dataset is None or len(dataset) == 0:
                return None
            return DailyGraphDataLoader(
                dataset=dataset,
                graph_builder=graph_builder,
                feature_cols=self._feature_cols,
                shuffle_dates=shuffle,
                device=device
            )
        
        # éå†æ¯ä¸ªçª—å£ï¼Œåˆ›å»ºç‹¬ç«‹çš„ loader ä¸‰å…ƒç»„
        window_loaders = []
        
        self.logger.info("\n" + "=" * 80)
        self.logger.info("ğŸ”„ åˆ›å»ºæ»šåŠ¨çª—å£æ—¥æ‰¹æ¬¡åŠ è½½å™¨")
        self.logger.info("=" * 80)
        self.logger.info(f"  æ€»çª—å£æ•°: {len(self._rolling_windows)}")
        
        for i, (train_df, test_df) in enumerate(self._rolling_windows):
            # 1. ä» train_df ä¸­æŒ‰æ—¶é—´åˆ’åˆ†å‡º val_df
            all_dates = sorted(train_df[self.config.time_col].unique())
            n_dates = len(all_dates)
            
            # éœ€è¦è¶³å¤Ÿçš„æ—¥æœŸæ¥åˆ’åˆ†éªŒè¯é›†
            min_dates_for_val = self.config.window_size + 20
            if n_dates > min_dates_for_val and val_ratio > 0:
                split_idx = int(n_dates * (1 - val_ratio))
                split_date = all_dates[split_idx]
                
                # è®­ç»ƒé›†: split_date ä¹‹å‰
                window_train_df = train_df[train_df[self.config.time_col] < split_date].copy()
                
                # éªŒè¯é›†: éœ€è¦åŒ…å«å›çœ‹çª—å£
                lookback_idx = max(0, split_idx - self.config.window_size)
                lookback_date = all_dates[lookback_idx]
                window_val_df = train_df[train_df[self.config.time_col] >= lookback_date].copy()
                val_valid_start = pd.Timestamp(split_date)
            else:
                window_train_df = train_df
                window_val_df = None
                val_valid_start = None
            
            # 2. å¤„ç†æµ‹è¯•é›†ï¼šéœ€è¦åŒ…å«å›çœ‹çª—å£çš„å†å²æ•°æ®
            if not test_df.empty:
                test_start_date = test_df[self.config.time_col].min()
                test_end_date = test_df[self.config.time_col].max()
                
                # ä» train_df æœ«å°¾è·å–å›çœ‹çª—å£
                lookback_start = test_start_date - pd.Timedelta(days=self.config.window_size * 2)
                lookback_df = train_df[train_df[self.config.time_col] >= lookback_start].copy()
                
                if not lookback_df.empty:
                    extended_test_df = pd.concat([lookback_df, test_df], ignore_index=True)
                    extended_test_df = extended_test_df.drop_duplicates(
                        subset=[self.config.stock_col, self.config.time_col]
                    ).sort_values([self.config.stock_col, self.config.time_col])
                    test_valid_start = pd.Timestamp(test_start_date)
                else:
                    extended_test_df = test_df
                    test_valid_start = None
            else:
                extended_test_df = test_df
                test_valid_start = None
            
            # 3. åˆ›å»ºæ•°æ®é›†
            train_dataset = make_daily_dataset(window_train_df)
            val_dataset = make_daily_dataset(window_val_df, val_valid_start) if window_val_df is not None else None
            test_dataset = make_daily_dataset(extended_test_df, test_valid_start)
            
            # 4. åˆ›å»ºåŠ è½½å™¨
            train_loader = make_loader(train_dataset, shuffle=True)
            
            # å¦‚æœè®­ç»ƒé›†ä¸ºç©ºï¼Œè·³è¿‡è¯¥çª—å£
            if train_loader is None or len(train_loader) == 0:
                self.logger.warning(f"  âš ï¸ çª—å£ {i+1} è®­ç»ƒé›†ä¸ºç©º (å¯èƒ½çª—å£å¤ªå°)ï¼Œè·³è¿‡")
                continue
                
            val_loader = make_loader(val_dataset, shuffle=False)
            test_loader = make_loader(test_dataset, shuffle=False)
            
            window_loaders.append(DailyLoaderCollection(train_loader, val_loader, test_loader))
            
            if (i + 1) % 5 == 0 or i == 0:
                self.logger.info(
                    f"  çª—å£ {i+1}: train={len(train_loader) if train_loader else 0} days, "
                    f"val={len(val_loader) if val_loader else 0} days, "
                    f"test={len(test_loader) if test_loader else 0} days"
                )
        
        self.logger.info(f"\nâœ… å·²åˆ›å»º {len(window_loaders)} ä¸ªçª—å£çš„æ—¥æ‰¹æ¬¡åŠ è½½å™¨")
        
        # è¿”å›å¯è¿­ä»£çš„é›†åˆç±»
        class RollingDailyLoaderCollection:
            """æ»šåŠ¨çª—å£æ—¥æ‰¹æ¬¡åŠ è½½å™¨é›†åˆ"""
            def __init__(self, windows):
                self.windows = windows
                self.n_windows = len(windows)
            
            def __len__(self):
                return self.n_windows
            
            def __iter__(self):
                return iter(self.windows)
            
            def __getitem__(self, idx):
                return self.windows[idx]
            
            def enumerate(self):
                """è¿”å› (window_idx, loaders) è¿­ä»£å™¨"""
                return enumerate(self.windows)
        
        return RollingDailyLoaderCollection(window_loaders)
    
    def create_rolling_daily_loaders_from_test(
        self,
        graph_builder=None,
        graph_builder_config: Optional[Dict] = None,
        rolling_window_size: Optional[int] = None,
        rolling_step: Optional[int] = None,
        val_ratio: float = 0.15,
        device: str = 'cuda',
    ):
        """
        ä»å·²æœ‰çš„ train/val/test åˆ’åˆ†åˆ›å»ºæ»šåŠ¨çª—å£æ—¥æ‰¹æ¬¡åŠ è½½å™¨
        
        ä¸ create_rolling_daily_loaders çš„åŒºåˆ«ï¼š
        - create_rolling_daily_loaders: è¦æ±‚ split_strategy='rolling'ï¼Œä» _rolling_windows è·å–çª—å£
        - create_rolling_daily_loaders_from_test: æ”¯æŒä»»æ„ split_strategyï¼Œåœ¨æµ‹è¯•é›†ä¸Šæ»šåŠ¨ç”Ÿæˆçª—å£
        
        æ»šåŠ¨é€»è¾‘ï¼š
        - åˆå¹¶ train/val/test ä¸ºå®Œæ•´æ•°æ®é›†
        - ä» test_start_date å¼€å§‹ï¼Œæ¯éš” rolling_step ç”Ÿæˆä¸€ä¸ªæµ‹è¯•çª—å£
        - æ¯ä¸ªçª—å£çš„è®­ç»ƒé›†å–æµ‹è¯•æœŸå‰ rolling_window_size å¤©ï¼Œå¹¶æŒ‰ val_ratio åˆ’åˆ†éªŒè¯é›†
        
        Args:
            graph_builder: å›¾æ„å»ºå™¨å®ä¾‹ï¼ˆç›´æ¥ä¼ å…¥ï¼‰ï¼Œä¼˜å…ˆäº graph_builder_config
            graph_builder_config: å›¾æ„å»ºå™¨é…ç½® dict
            rolling_window_size: æ»šåŠ¨çª—å£è®­ç»ƒé›†å¤§å°ï¼ˆå¤©ï¼‰ï¼Œé»˜è®¤ config.rolling_window_size
            rolling_step: æ»šåŠ¨æ­¥é•¿ï¼ˆå¤©ï¼‰ï¼Œé»˜è®¤ config.rolling_step
            val_ratio: ä»è®­ç»ƒé›†ä¸­åˆ’åˆ†éªŒè¯é›†çš„æ¯”ä¾‹
            device: è®¡ç®—è®¾å¤‡
            
        Returns:
            RollingDailyLoaderCollection å¯¹è±¡ï¼Œå¯ç›´æ¥ä¼ ç»™ RollingDailyTrainer
            
        Example:
            >>> dm.run_full_pipeline()  # split_strategy='time' æˆ– 'ratio'
            >>> loaders = dm.create_rolling_daily_loaders_from_test(
            ...     graph_builder=my_graph_builder,
            ...     rolling_window_size=120,
            ...     rolling_step=20,
            ... )
            >>> results = rolling_trainer.train(loaders, save_dir='...')
        """
        if self._train_df is None or self._feature_cols is None:
            raise ValueError("æœªå‡†å¤‡æ•°æ®ï¼Œè¯·å…ˆè°ƒç”¨ run_full_pipeline()")
        
        from quantclassic.data_set.graph import DailyBatchDataset, DailyGraphDataLoader
        from quantclassic.data_processor.graph_builder import GraphBuilderFactory
        from collections import namedtuple
        from dataclasses import dataclass
        
        # å‚æ•°é»˜è®¤å€¼
        rolling_window_size = rolling_window_size or getattr(self.config, 'rolling_window_size', 120)
        rolling_step = rolling_step or getattr(self.config, 'rolling_step', 20)
        test_size = rolling_step  # æµ‹è¯•æœŸé•¿åº¦ = æ»šåŠ¨æ­¥é•¿
        
        # åˆ›å»ºå›¾æ„å»ºå™¨
        if graph_builder is None and graph_builder_config is not None:
            gb_dict = _normalize_graph_builder_config(
                graph_builder_config, self._raw_data, self.config.stock_col, self.logger
            )
            gb_dict.setdefault('stock_col', self.config.stock_col)
            graph_builder = GraphBuilderFactory.create(gb_dict)
        
        # åˆå¹¶æ•°æ®
        df_full = pd.concat([self._train_df, self._val_df, self._test_df], ignore_index=True)
        df_full[self.config.time_col] = pd.to_datetime(df_full[self.config.time_col])
        all_dates = sorted(df_full[self.config.time_col].unique())
        
        # æ¨æ–­æµ‹è¯•èµ·å§‹æ—¥æœŸ
        test_start_date = pd.to_datetime(self._test_df[self.config.time_col].min())
        
        # è®¡ç®—éªŒè¯é›†å¤§å°
        val_size = int(rolling_window_size * val_ratio)
        
        self.logger.info("\n" + "=" * 80)
        self.logger.info("ğŸ”„ åˆ›å»ºæ»šåŠ¨çª—å£æ—¥æ‰¹æ¬¡åŠ è½½å™¨ (from_test æ¨¡å¼)")
        self.logger.info("=" * 80)
        self.logger.info(f"  rolling_window_size={rolling_window_size}, rolling_step={rolling_step}")
        self.logger.info(f"  val_size={val_size}, test_size={test_size}")
        self.logger.info(f"  æµ‹è¯•èµ·å§‹æ—¥æœŸ: {test_start_date}")
        
        # ç”Ÿæˆæ»šåŠ¨çª—å£æ—¥æœŸåˆ‡åˆ†
        test_period_dates = [d for d in all_dates if d >= test_start_date]
        n_windows = (len(test_period_dates) - test_size) // rolling_step + 1
        
        rolling_windows = []
        for w_idx in range(n_windows):
            test_start_idx = w_idx * rolling_step
            test_end_idx = test_start_idx + test_size
            if test_end_idx > len(test_period_dates):
                break
            
            test_dates_w = test_period_dates[test_start_idx:test_end_idx]
            test_start = test_dates_w[0]
            test_start_pos = list(all_dates).index(test_start)
            
            val_start_pos = max(0, test_start_pos - val_size)
            train_end_pos = max(0, val_start_pos)
            train_start_pos = max(0, train_end_pos - rolling_window_size)
            
            train_dates = list(all_dates[train_start_pos:train_end_pos])
            val_dates = list(all_dates[val_start_pos:test_start_pos])
            
            if train_dates and val_dates and test_dates_w:
                rolling_windows.append((train_dates, val_dates, test_dates_w))
        
        self.logger.info(f"  ç”Ÿæˆ {len(rolling_windows)} ä¸ªæ»šåŠ¨çª—å£")
        
        # å…¬å…±æ•°æ®é›†å‚æ•°
        common_kwargs = dict(
            feature_cols=self._feature_cols,
            label_col=self.config.label_col,
            window_size=self.config.window_size,
            time_col=self.config.time_col,
            stock_col=self.config.stock_col,
            enable_window_transform=self.config.enable_window_transform,
            window_price_log=self.config.window_price_log,
            window_volume_norm=self.config.window_volume_norm,
            price_cols=self.config.price_cols,
            close_col=self.config.close_col,
            volume_cols=self.config.volume_cols,
            label_rank_normalize=self.config.label_rank_normalize,
            label_rank_output_range=self.config.label_rank_output_range,
        )
        
        def make_daily_dataset(dates_list, valid_label_start_date=None):
            df_subset = df_full[df_full[self.config.time_col].isin(dates_list)].copy()
            return DailyBatchDataset(df=df_subset, valid_label_start_date=valid_label_start_date, **common_kwargs)
        
        def make_loader(dataset, shuffle):
            if dataset is None or len(dataset) == 0:
                return None
            return DailyGraphDataLoader(
                dataset=dataset,
                graph_builder=graph_builder,
                feature_cols=self._feature_cols,
                shuffle_dates=shuffle,
                device=device,
                num_workers=0,
                pin_memory=False,
            )
        
        # ç”¨äºå…¼å®¹ RollingDailyTrainer çš„ WindowLoaders ç±»
        @dataclass
        class WindowLoaders:
            train: DailyGraphDataLoader
            val: DailyGraphDataLoader
            test: DailyGraphDataLoader
            train_dates: list
            val_dates: list
            test_dates: list
        
        DailyLoaderCollection = namedtuple('DailyLoaderCollection', ['train', 'val', 'test'])
        
        # è®¡ç®—æœ‰æ•ˆæ ‡ç­¾èµ·å§‹æ—¥æœŸï¼ˆé¿å…çª—å£é¦–éƒ¨æ— æ ‡ç­¾ï¼‰
        valid_label_start_date = all_dates[self.config.window_size] if len(all_dates) > self.config.window_size else None
        
        window_loaders = []
        for w_idx, (train_dates, val_dates, test_dates_w) in enumerate(rolling_windows):
            train_dataset = make_daily_dataset(train_dates, valid_label_start_date if w_idx == 0 else None)
            val_dataset = make_daily_dataset(val_dates)
            test_dataset = make_daily_dataset(test_dates_w)
            
            train_loader = make_loader(train_dataset, shuffle=True)
            val_loader = make_loader(val_dataset, shuffle=False)
            test_loader = make_loader(test_dataset, shuffle=False)
            
            if train_loader is None or len(train_loader) == 0:
                self.logger.warning(f"  âš ï¸ çª—å£ {w_idx+1} è®­ç»ƒé›†ä¸ºç©ºï¼Œè·³è¿‡")
                continue
            
            window_loaders.append(WindowLoaders(
                train=train_loader, val=val_loader, test=test_loader,
                train_dates=train_dates, val_dates=val_dates, test_dates=test_dates_w
            ))
            
            if w_idx == 0:
                self.logger.info(f"  çª—å£ 1: train={len(train_dates)}å¤©, val={len(val_dates)}å¤©, test={len(test_dates_w)}å¤©")
        
        self.logger.info(f"\nâœ… å·²åˆ›å»º {len(window_loaders)} ä¸ªçª—å£çš„æ—¥æ‰¹æ¬¡åŠ è½½å™¨")
        
        # è¿”å›å¯è¿­ä»£é›†åˆ
        class RollingDailyLoaderCollection:
            def __init__(self, windows):
                self.windows = windows
                self.n_windows = len(windows)
            def __len__(self):
                return self.n_windows
            def __iter__(self):
                return iter(self.windows)
            def __getitem__(self, idx):
                return self.windows[idx]
            def enumerate(self):
                return enumerate(self.windows)
        
        return RollingDailyLoaderCollection(window_loaders)
    
    def run_full_pipeline(self, file_path: Optional[str] = None,
                         validate: bool = True,
                         auto_filter_features: bool = True) -> LoaderCollection:
        """
        è¿è¡Œå®Œæ•´çš„æ•°æ®å¤„ç†æµæ°´çº¿
        
        Args:
            file_path: æ•°æ®æ–‡ä»¶è·¯å¾„
            validate: æ˜¯å¦éªŒè¯æ•°æ®è´¨é‡
            auto_filter_features: æ˜¯å¦è‡ªåŠ¨è¿‡æ»¤ç‰¹å¾
            
        Returns:
            LoaderCollectionå¯¹è±¡
        """
        self.logger.info("\n" + "=" * 80)
        self.logger.info("ğŸš€ å¼€å§‹å®Œæ•´æ•°æ®å¤„ç†æµæ°´çº¿")
        self.logger.info("=" * 80 + "\n")
        
        # 1. åŠ è½½æ•°æ®
        self.load_raw_data(file_path)
        
        # 2. éªŒè¯æ•°æ®ï¼ˆå¯é€‰ï¼‰
        if validate and self.config.enable_validation:
            report = self.validate_data_quality()
            if not report.is_valid:
                self.logger.warning("âš ï¸  æ•°æ®éªŒè¯æœªé€šè¿‡ï¼Œä½†ç»§ç»­å¤„ç†")
        
        # 3. ç‰¹å¾å·¥ç¨‹
        self.preprocess_features(auto_filter=auto_filter_features)
        
        # 4-5. åˆ›å»ºæ•°æ®é›†
        self.create_datasets()
        
        # 6. åˆ›å»ºæ•°æ®åŠ è½½å™¨
        # ğŸ†• æ ¹æ® use_daily_batch é…ç½®å†³å®šè¿”å›ç±»å‹
        use_daily = getattr(self.config, 'use_daily_batch', False)
        if use_daily:
            self.logger.info("ğŸ†• use_daily_batch=Trueï¼Œåˆ›å»ºæ—¥æ‰¹æ¬¡åŠ è½½å™¨")
            loaders = self.create_daily_loaders(
                graph_builder_config=getattr(self.config, 'graph_builder_config', None),
                shuffle_dates=getattr(self.config, 'shuffle_dates', True)
            )
        else:
            loaders = self.get_dataloaders()
        
        self.logger.info("\n" + "=" * 80)
        self.logger.info("âœ… å®Œæ•´æ•°æ®å¤„ç†æµæ°´çº¿å®Œæˆ")
        self.logger.info("=" * 80 + "\n")
        
        # æ‰“å°æ‘˜è¦
        self._print_pipeline_summary()
        
        return loaders
    
    def _print_pipeline_summary(self):
        """æ‰“å°æµæ°´çº¿æ‘˜è¦"""
        print("\n" + "=" * 80)
        print("ğŸ“Š æ•°æ®å¤„ç†æ‘˜è¦")
        print("=" * 80)
        
        if self._raw_data is not None:
            print(f"åŸå§‹æ•°æ®: {len(self._raw_data):,} è¡Œ")
        
        if self._feature_cols is not None:
            print(f"ç‰¹å¾æ•°é‡: {len(self._feature_cols)}")
        
        if self._datasets is not None:
            print(f"\næ•°æ®é›†:")
            print(f"  è®­ç»ƒé›†: {self._datasets.metadata['train_samples']:,} æ ·æœ¬")
            print(f"  éªŒè¯é›†: {self._datasets.metadata['val_samples']:,} æ ·æœ¬")
            print(f"  æµ‹è¯•é›†: {self._datasets.metadata['test_samples']:,} æ ·æœ¬")
        
        print(f"\né…ç½®:")
        print(f"  çª—å£å¤§å°: {self.config.window_size}")
        print(f"  æ‰¹é‡å¤§å°: {self.config.batch_size}")
        print(f"  åˆ’åˆ†ç­–ç•¥: {self.config.split_strategy}")
        
        print("=" * 80 + "\n")
    
    def _save_validation_report(self, report: ValidationReport):
        """ä¿å­˜éªŒè¯æŠ¥å‘Š"""
        report_dir = os.path.join(self.config.output_dir, 'reports')
        Path(report_dir).mkdir(parents=True, exist_ok=True)
        
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        report_path = os.path.join(report_dir, f'validation_report_{timestamp}.txt')
        
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write("=" * 80 + "\n")
            f.write("æ•°æ®éªŒè¯æŠ¥å‘Š\n")
            f.write("=" * 80 + "\n\n")
            f.write(f"æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"çŠ¶æ€: {'é€šè¿‡' if report.is_valid else 'å¤±è´¥'}\n\n")
            
            if report.errors:
                f.write(f"é”™è¯¯ ({len(report.errors)}):\n")
                for i, error in enumerate(report.errors, 1):
                    f.write(f"  {i}. {error}\n")
                f.write("\n")
            
            if report.warnings:
                f.write(f"è­¦å‘Š ({len(report.warnings)}):\n")
                for i, warning in enumerate(report.warnings, 1):
                    f.write(f"  {i}. {warning}\n")
                f.write("\n")
            
            if report.stats:
                f.write("ç»Ÿè®¡ä¿¡æ¯:\n")
                for key, value in report.stats.items():
                    f.write(f"  {key}: {value}\n")
        
        self.logger.info(f"ğŸ“„ éªŒè¯æŠ¥å‘Šå·²ä¿å­˜: {report_path}")
    
    def save_state(self, save_path: Optional[str] = None):
        """ä¿å­˜ç®¡ç†å™¨çŠ¶æ€"""
        if save_path is None:
            save_path = os.path.join(self.config.cache_dir, 'manager_state.pkl')
        
        state = {
            'config': self.config,
            'feature_cols': self._feature_cols,
            'train_df': self._train_df,
            'val_df': self._val_df,
            'test_df': self._test_df,
        }
        
        Path(save_path).parent.mkdir(parents=True, exist_ok=True)
        
        with open(save_path, 'wb') as f:
            pickle.dump(state, f)
        
        self.logger.info(f"ğŸ’¾ çŠ¶æ€å·²ä¿å­˜: {save_path}")
    
    def load_state(self, load_path: Optional[str] = None):
        """åŠ è½½ç®¡ç†å™¨çŠ¶æ€"""
        if load_path is None:
            load_path = os.path.join(self.config.cache_dir, 'manager_state.pkl')
        
        if not os.path.exists(load_path):
            raise FileNotFoundError(f"çŠ¶æ€æ–‡ä»¶ä¸å­˜åœ¨: {load_path}")
        
        with open(load_path, 'rb') as f:
            state = pickle.load(f)
        
        self.config = state['config']
        self._feature_cols = state['feature_cols']
        self._train_df = state['train_df']
        self._val_df = state['val_df']
        self._test_df = state['test_df']
        
        self.logger.info(f"ğŸ“ çŠ¶æ€å·²åŠ è½½: {load_path}")
    
    @property
    def raw_data(self) -> Optional[pd.DataFrame]:
        """è·å–åŸå§‹æ•°æ®"""
        return self._raw_data
    
    @property
    def feature_cols(self) -> Optional[List[str]]:
        """è·å–ç‰¹å¾åˆ—"""
        return self._feature_cols
    
    @property
    def datasets(self) -> Optional[DatasetCollection]:
        """è·å–æ•°æ®é›†é›†åˆ"""
        return self._datasets
    
    @property
    def split_data(self) -> Tuple[Optional[pd.DataFrame], Optional[pd.DataFrame], Optional[pd.DataFrame]]:
        """è·å–åˆ’åˆ†åçš„æ•°æ®"""
        return self._train_df, self._val_df, self._test_df
    
    def create_rolling_window_trainer(
        self, 
        stock_universe: Optional[List[str]] = None
    ):
        """
        âš ï¸ å·²åºŸå¼ƒå¹¶ç§»é™¤ - è¯·ä½¿ç”¨ model.train.RollingWindowTrainer æˆ– RollingDailyTrainer
        
        .. deprecated:: 2026.01
            æ•°æ®å±‚ä¸åº”åŒ…å«è®­ç»ƒå¾ªç¯ã€‚æ­¤æ–¹æ³•å·²ç§»é™¤ã€‚
            
            è¯·æ”¹ç”¨:
            >>> from quantclassic.model.train import RollingWindowTrainer, RollingDailyTrainer
            >>> rolling_loaders = dm.create_rolling_daily_loaders()
            >>> trainer = RollingDailyTrainer(model_factory=..., config=...)
            >>> trainer.fit(rolling_loaders)
        
        Raises:
            DeprecationWarning: å§‹ç»ˆæŠ›å‡ºï¼ŒæŒ‡å¯¼ç”¨æˆ·è¿ç§»åˆ°æ–° API
        """
        raise NotImplementedError(
            "\n" + "=" * 70 + "\n"
            "âš ï¸  DataManager.create_rolling_window_trainer() å·²ç§»é™¤ï¼\n\n"
            "æ•°æ®å±‚ä¸åº”åŒ…å«è®­ç»ƒå¾ªç¯ã€‚è¯·æ”¹ç”¨ model.train æ¨¡å—:\n\n"
            "    from quantclassic.model.train import RollingDailyTrainer, RollingTrainerConfig\n\n"
            "    # 1. åˆ›å»ºæ»šåŠ¨æ—¥æ‰¹æ¬¡åŠ è½½å™¨\n"
            "    rolling_loaders = dm.create_rolling_daily_loaders()\n\n"
            "    # 2. å®šä¹‰æ¨¡å‹å·¥å‚\n"
            "    def model_factory():\n"
            "        return MyModel(d_feat=len(feature_cols))\n\n"
            "    # 3. åˆ›å»ºè®­ç»ƒå™¨å¹¶è®­ç»ƒ\n"
            "    config = RollingTrainerConfig(n_epochs=20, weight_inheritance=True)\n"
            "    trainer = RollingDailyTrainer(model_factory, config)\n"
            "    trainer.fit(rolling_loaders)\n"
            + "=" * 70
        )


if __name__ == '__main__':
    # æµ‹è¯•DataManager
    print("=" * 80)
    print("DataManager æµ‹è¯•")
    print("=" * 80)
    
    # åˆ›å»ºé…ç½®
    config = DataConfig(
        base_dir='rq_data_parquet',
        data_file='train_data_final.parquet',
        window_size=40,
        batch_size=256,
        split_strategy='time_series',
        enable_validation=True,
        verbose=True
    )
    
    # åˆ›å»ºç®¡ç†å™¨
    manager = DataManager(config)
    
    try:
        # è¿è¡Œå®Œæ•´æµæ°´çº¿
        loaders = manager.run_full_pipeline()
        
        # æµ‹è¯•æ•°æ®åŠ è½½å™¨
        print("\næµ‹è¯•æ•°æ®åŠ è½½å™¨:")
        batch_x, batch_y = next(iter(loaders.train))
        print(f"  æ‰¹æ¬¡ç‰¹å¾å½¢çŠ¶: {batch_x.shape}")
        print(f"  æ‰¹æ¬¡æ ‡ç­¾å½¢çŠ¶: {batch_y.shape}")
        
        print("\nâœ… DataManager æµ‹è¯•å®Œæˆ")
        
    except FileNotFoundError:
        print("\nâš ï¸  æµ‹è¯•æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨")
        print("âœ… DataManager ç±»å®šä¹‰å®Œæˆ")
