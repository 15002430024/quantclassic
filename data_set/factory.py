"""
DatasetFactory - æ•°æ®é›†å·¥å‚

åˆ›å»ºä¸åŒç±»å‹çš„æ•°æ®é›†å¯¹è±¡
"""

import torch
import pandas as pd
import numpy as np
from torch.utils.data import Dataset, DataLoader, Sampler
from typing import List, Tuple, Optional, Dict, Any, Iterator
from dataclasses import dataclass
import logging
from collections import defaultdict
from .config import DataConfig


# =============================================================================
# æˆªé¢æ‰¹é‡‡æ ·å™¨ - è§£å†³ IC Loss çš„æ—¶é—´æ··ä¹±é—®é¢˜
# =============================================================================
class CrossSectionalBatchSampler(Sampler):
    """
    æˆªé¢æ‰¹é‡‡æ ·å™¨ - ä¿è¯æ¯ä¸ª batch æ¥è‡ªåŒä¸€äº¤æ˜“æ—¥
    
    ğŸ”´ æ ¸å¿ƒä¿®å¤ï¼š
    ä¼ ç»Ÿçš„ DataLoader(shuffle=True) ä¼šå°†ä¸åŒæ—¥æœŸçš„æ ·æœ¬æ··åˆåœ¨ä¸€ä¸ª batch ä¸­ã€‚
    è¿™ä¼šå¯¼è‡´ IC Loss è®¡ç®—çš„æ˜¯"è·¨æ—¶é—´"çš„æ’åºï¼Œæ¯«æ— é‡‘èæ„ä¹‰ã€‚
    
    æœ¬é‡‡æ ·å™¨ç¡®ä¿ï¼š
    1. æ¯ä¸ª batch å†…çš„æ ·æœ¬æ¥è‡ªåŒä¸€äº¤æ˜“æ—¥ï¼ˆæˆªé¢æ•°æ®ï¼‰
    2. æ—¥æœŸé¡ºåºéšæœºæ‰“ä¹±ï¼Œé¿å…æ¨¡å‹å­¦ä¹ æ—¶é—´è¶‹åŠ¿
    3. æ¯å¤©çš„è‚¡ç¥¨é¡ºåºéšæœºæ‰“ä¹±ï¼Œå¢åŠ æ ·æœ¬å¤šæ ·æ€§
    
    ä½¿ç”¨æ–¹å¼ï¼š
        sampler = CrossSectionalBatchSampler(dataset, batch_size=256)
        loader = DataLoader(dataset, batch_sampler=sampler)
    """
    
    def __init__(self, dataset: 'TimeSeriesStockDatasetWithDate', 
                 batch_size: int = 256,
                 shuffle_dates: bool = True,
                 drop_last: bool = False):
        """
        Args:
            dataset: å¿…é¡»æ˜¯ TimeSeriesStockDatasetWithDate ç±»å‹
            batch_size: æ¯ä¸ª batch çš„æœ€å¤§æ ·æœ¬æ•°
            shuffle_dates: æ˜¯å¦æ‰“ä¹±æ—¥æœŸé¡ºåº
            drop_last: æ˜¯å¦ä¸¢å¼ƒæ¯å¤©æœ€åä¸€ä¸ªä¸è¶³ batch_size çš„ batch
        """
        self.dataset = dataset
        self.batch_size = batch_size
        self.shuffle_dates = shuffle_dates
        self.drop_last = drop_last
        
        # æ„å»ºæ—¥æœŸ -> æ ·æœ¬ç´¢å¼•çš„æ˜ å°„
        self.date_to_indices = self._build_date_index()
        self.dates = list(self.date_to_indices.keys())
        
    def _build_date_index(self) -> Dict[Any, List[int]]:
        """æ„å»ºæ—¥æœŸåˆ°æ ·æœ¬ç´¢å¼•çš„æ˜ å°„"""
        date_to_indices = defaultdict(list)
        
        for idx in range(len(self.dataset)):
            stock_idx, time_idx = self.dataset.sample_index[idx]
            stock_info = self.dataset.stock_data[stock_idx]
            # æ ‡ç­¾å¯¹åº”çš„æ—¥æœŸæ˜¯ time_idx + 1
            date = stock_info['dates'][time_idx + 1]
            date_to_indices[date].append(idx)
        
        return dict(date_to_indices)
    
    def __iter__(self) -> Iterator[List[int]]:
        """ç”Ÿæˆæ‰¹æ¬¡"""
        dates = self.dates.copy()
        
        if self.shuffle_dates:
            np.random.shuffle(dates)
        
        for date in dates:
            indices = self.date_to_indices[date].copy()
            np.random.shuffle(indices)  # æ‰“ä¹±åŒä¸€å¤©å†…çš„è‚¡ç¥¨é¡ºåº
            
            # æŒ‰ batch_size åˆ†å‰²
            for i in range(0, len(indices), self.batch_size):
                batch = indices[i:i + self.batch_size]
                
                if self.drop_last and len(batch) < self.batch_size:
                    continue
                    
                yield batch
    
    def __len__(self) -> int:
        """è¿”å›æ€»æ‰¹æ¬¡æ•°"""
        total = 0
        for date, indices in self.date_to_indices.items():
            n_batches = len(indices) // self.batch_size
            if not self.drop_last and len(indices) % self.batch_size > 0:
                n_batches += 1
            total += n_batches
        return total


class TimeSeriesStockDatasetWithDate(Dataset):
    """
    æ—¶åºè‚¡ç¥¨æ•°æ®é›† - å¢å¼ºç‰ˆï¼ˆè¿”å›æ—¥æœŸä¿¡æ¯ + çª—å£çº§å˜æ¢ï¼‰
    
    ğŸ”´ å…³é”®å¢å¼ºï¼š
    1. __getitem__ è¿”å› (X, y, date_idx) ä¸‰å…ƒç»„
    2. date_idx ç”¨äºåœ¨ Loss è®¡ç®—æ—¶è¯†åˆ«åŒä¸€æˆªé¢çš„æ ·æœ¬
    3. æä¾› get_date_for_idx() æ–¹æ³•è·å–å…·ä½“æ—¥æœŸ
    4. ğŸ†• çª—å£çº§æ•°æ®å˜æ¢ï¼ˆç ”æŠ¥æ ‡å‡†ï¼‰ï¼š
       - ä»·æ ¼å¯¹æ•°å˜æ¢: log(price / close_t)
       - æˆäº¤é‡æ ‡å‡†åŒ–: volume / mean(volume_in_window)
    5. ğŸ†• valid_label_start_date: åªä¸ºè¯¥æ—¥æœŸä¹‹åçš„æ ‡ç­¾ç”Ÿæˆæ ·æœ¬ï¼ˆè§£å†³æµ‹è¯•é›†å†å²æ•°æ®é—®é¢˜ï¼‰
    """
    
    def __init__(self, df: pd.DataFrame, feature_cols: List[str],
                 label_col: str, window_size: int, stock_col: str = 'ts_code',
                 time_col: str = 'trade_date', return_date: bool = False,
                 return_stock_id: bool = False,
                 # ğŸ†• çª—å£å˜æ¢é…ç½®
                 enable_window_transform: bool = False,
                 window_price_log: bool = False,
                 window_volume_norm: bool = False,
                 price_cols: Optional[List[str]] = None,
                 close_col: str = 'close',
                 volume_cols: Optional[List[str]] = None,
                 # ğŸ†• æ ‡ç­¾çª—å£çº§æ’åæ ‡å‡†åŒ–
                 label_rank_normalize: bool = False,
                 label_rank_output_range: Tuple[float, float] = (-1, 1),
                 # ğŸ†• å…¨å±€è‚¡ç¥¨æ˜ å°„
                 stock_map: Optional[Dict[str, int]] = None,
                 # ğŸ†• æœ‰æ•ˆæ ‡ç­¾èµ·å§‹æ—¥æœŸï¼ˆåªä¸ºè¯¥æ—¥æœŸä¹‹åçš„æ ‡ç­¾ç”Ÿæˆæ ·æœ¬ï¼‰
                 valid_label_start_date: Optional[pd.Timestamp] = None):
        """
        Args:
            df: æ•°æ®DataFrame
            feature_cols: ç‰¹å¾åˆ—åˆ—è¡¨
            label_col: æ ‡ç­¾åˆ—
            window_size: æ—¶é—´çª—å£å¤§å°
            stock_col: è‚¡ç¥¨ä»£ç åˆ—
            time_col: æ—¶é—´åˆ—
            return_date: æ˜¯å¦åœ¨ __getitem__ ä¸­è¿”å›æ—¥æœŸç´¢å¼•
            return_stock_id: æ˜¯å¦åœ¨ __getitem__ ä¸­è¿”å›è‚¡ç¥¨ID
            enable_window_transform: æ˜¯å¦å¯ç”¨çª—å£çº§å˜æ¢
            window_price_log: æ˜¯å¦å¯¹ä»·æ ¼åšå¯¹æ•°å˜æ¢ log(price/close_t)
            window_volume_norm: æ˜¯å¦å¯¹æˆäº¤é‡åšçª—å£å†…å‡å€¼æ ‡å‡†åŒ–
            price_cols: ä»·æ ¼åˆ—ååˆ—è¡¨
            close_col: åŸºå‡†æ”¶ç›˜ä»·åˆ—å
            volume_cols: æˆäº¤é‡åˆ—ååˆ—è¡¨
            stock_map: è‚¡ç¥¨ä»£ç åˆ°IDçš„æ˜ å°„å­—å…¸ (å¯é€‰ï¼Œç”¨äºç»Ÿä¸€å…¨å±€ID)
            label_rank_normalize: æ˜¯å¦å¯¹æ ‡ç­¾åšçª—å£å†…æ—¶åºæ’åæ ‡å‡†åŒ–
            label_rank_output_range: æ’åæ ‡å‡†åŒ–è¾“å‡ºèŒƒå›´ï¼Œé»˜è®¤(-1, 1)
            valid_label_start_date: ğŸ†• åªä¸ºè¯¥æ—¥æœŸä¹‹åçš„æ ‡ç­¾ç”Ÿæˆæ ·æœ¬ï¼ˆç”¨äºæµ‹è¯•é›†åŒ…å«å†å²æ•°æ®çš„æƒ…å†µï¼‰
        """
        self.window_size = window_size
        self.feature_cols = feature_cols
        self.label_col = label_col
        self.stock_col = stock_col
        self.time_col = time_col
        self.return_date = return_date
        self.return_stock_id = return_stock_id
        self.stock_map = stock_map
        
        # ğŸ†• æœ‰æ•ˆæ ‡ç­¾èµ·å§‹æ—¥æœŸ
        self.valid_label_start_date = valid_label_start_date
        
        # ğŸ†• æ ‡ç­¾çª—å£çº§æ’åæ ‡å‡†åŒ–é…ç½®
        self.label_rank_normalize = label_rank_normalize
        self.label_rank_output_range = label_rank_output_range
        
        # ğŸ†• é¢„è®¡ç®—çš„æ ‡ç­¾æ’åï¼ˆåœ¨ _build_sample_index ä¸­å¡«å……ï¼‰
        self._precomputed_label_ranks = {}  # stock_idx -> np.ndarray
        
        # ğŸ†• çª—å£å˜æ¢é…ç½®
        self.enable_window_transform = enable_window_transform
        self.window_price_log = window_price_log
        self.window_volume_norm = window_volume_norm
        self.price_cols = price_cols or ['open', 'high', 'low', 'close', 'vwap']
        self.close_col = close_col
        self.volume_cols = volume_cols or ['vol', 'amount']
        
        # è®¡ç®—ä»·æ ¼å’Œæˆäº¤é‡åˆ—åœ¨ feature_cols ä¸­çš„ç´¢å¼•ä½ç½®
        self._price_indices = []
        self._close_index = None
        self._volume_indices = []
        
        if self.enable_window_transform:
            for i, col in enumerate(feature_cols):
                if col in self.price_cols:
                    self._price_indices.append(i)
                    if col == self.close_col:
                        self._close_index = i
                if col in self.volume_cols:
                    self._volume_indices.append(i)
        
        self._build_sample_index(df)
    
    def _build_sample_index(self, df: pd.DataFrame):
        """é¢„å…ˆæ„å»ºæ ·æœ¬ç´¢å¼•"""
        import logging
        logger = logging.getLogger(__name__)
        
        df = df.copy()
        df = df.dropna(subset=self.feature_cols + [self.label_col])
        df = df.sort_values([self.stock_col, self.time_col]).reset_index(drop=True)
        
        self.stock_data = {}
        self.sample_index = []
        
        # æ„å»ºæ—¥æœŸåˆ°ç´¢å¼•çš„æ˜ å°„
        all_dates = sorted(df[self.time_col].unique())
        self.date_to_idx = {date: idx for idx, date in enumerate(all_dates)}
        self.idx_to_date = {idx: date for date, idx in self.date_to_idx.items()}
        
        # ğŸ†• è®°å½•æ•°æ®é›†ä¸­çš„è‚¡ç¥¨æ•°é‡ï¼ˆç”¨äºåç»­æ£€æŸ¥ï¼‰
        all_stocks_in_data = set(df[self.stock_col].unique())
        
        # ğŸ†• å¦‚æœæ²¡æœ‰æä¾› stock_mapï¼Œåˆ™æ„å»ºå±€éƒ¨æ˜ å°„
        if self.stock_map is None:
            # æŒ‰å­—æ¯é¡ºåºæ’åºè‚¡ç¥¨ä»£ç 
            all_stocks = sorted(all_stocks_in_data)
            self.stock_map = {stock: i for i, stock in enumerate(all_stocks)}
            
        # ğŸ†• è®°å½•è¢«ä¸¢å¼ƒçš„è‚¡ç¥¨
        skipped_stocks_not_in_map = []
        skipped_stocks_insufficient_data = []
        samples_filtered_by_date = 0
        total_potential_samples = 0
        
        # éå†æ¯åªè‚¡ç¥¨
        for ts_code, stock_df in df.groupby(self.stock_col, observed=False):
            # è·å–å…¨å±€ID
            if ts_code not in self.stock_map:
                skipped_stocks_not_in_map.append(ts_code)  # ğŸ†• è®°å½•è¢«è·³è¿‡çš„è‚¡ç¥¨
                continue # è·³è¿‡ä¸åœ¨æ˜ å°„ä¸­çš„è‚¡ç¥¨
                
            stock_idx = self.stock_map[ts_code]
            n = len(stock_df)
            
            if n < self.window_size + 1:
                skipped_stocks_insufficient_data.append(ts_code)  # ğŸ†• è®°å½•æ•°æ®ä¸è¶³çš„è‚¡ç¥¨
                continue
            
            features = stock_df[self.feature_cols].values.astype(np.float32)
            labels = stock_df[self.label_col].values.astype(np.float32)
            dates = stock_df[self.time_col].values
            
            self.stock_data[stock_idx] = {
                'ts_code': ts_code,
                'features': features,
                'labels': labels,
                'dates': dates,
                'n': n
            }
            
            # ğŸ†• é¢„è®¡ç®—æ ‡ç­¾çš„çª—å£çº§æ’åï¼ˆé¿å…è¿è¡Œæ—¶è®¡ç®—å¼€é”€ï¼‰
            if self.label_rank_normalize:
                self._precomputed_label_ranks[stock_idx] = self._precompute_label_ranks(
                    labels, self.window_size
                )
            
            # ğŸ†• è€ƒè™‘ valid_label_start_dateï¼šåªä¸ºæœ‰æ•ˆæ—¥æœŸèŒƒå›´å†…çš„æ ‡ç­¾ç”Ÿæˆæ ·æœ¬
            for t in range(self.window_size - 1, n - 1):
                total_potential_samples += 1  # ğŸ†• è®°å½•æ€»æ½œåœ¨æ ·æœ¬æ•°
                
                # æ ‡ç­¾å¯¹åº”çš„æ—¥æœŸæ˜¯ dates[t + 1]
                label_date = dates[t + 1]
                
                # å¦‚æœè®¾ç½®äº†æœ‰æ•ˆæ ‡ç­¾èµ·å§‹æ—¥æœŸï¼Œè·³è¿‡è¯¥æ—¥æœŸä¹‹å‰çš„æ ·æœ¬
                if self.valid_label_start_date is not None:
                    # å°† numpy datetime64 è½¬æ¢ä¸º pandas Timestamp è¿›è¡Œæ¯”è¾ƒ
                    label_date_ts = pd.Timestamp(label_date)
                    if label_date_ts < self.valid_label_start_date:
                        samples_filtered_by_date += 1  # ğŸ†• è®°å½•è¢«æ—¥æœŸè¿‡æ»¤æ‰çš„æ ·æœ¬
                        continue
                
                self.sample_index.append((stock_idx, t))
        
        # ğŸ†• è¾“å‡ºæ•°æ®é›†æ„å»ºç»Ÿè®¡ä¿¡æ¯
        logger.info(f"\n====== æ•°æ®é›†æ„å»ºç»Ÿè®¡ ======")
        logger.info(f"æ•°æ®ä¸­æ€»è‚¡ç¥¨æ•°: {len(all_stocks_in_data)}")
        logger.info(f"æˆåŠŸåŠ è½½è‚¡ç¥¨æ•°: {len(self.stock_data)}")
        logger.info(f"ç”Ÿæˆæ ·æœ¬æ•°: {len(self.sample_index):,}")
        
        # ğŸ†• è­¦å‘Šï¼šstock_map è¦†ç›–ç‡
        if skipped_stocks_not_in_map:
            logger.warning(
                f"\u26a0\ufe0f {len(skipped_stocks_not_in_map)} åªè‚¡ç¥¨å› ä¸åœ¨ stock_map ä¸­è€Œè¢«è·³è¿‡ã€‚"
                f"\n   ç¤ºä¾‹: {skipped_stocks_not_in_map[:5]}"
            )
        
        # ğŸ†• è­¦å‘Šï¼šæ•°æ®ä¸è¶³
        if skipped_stocks_insufficient_data:
            logger.info(
                f"{len(skipped_stocks_insufficient_data)} åªè‚¡ç¥¨å› æ•°æ®ç‚¹ä¸è¶³ (<{self.window_size + 1}) è€Œè¢«è·³è¿‡ã€‚"
            )
        
        # ğŸ†• è­¦å‘Šï¼švalid_label_start_date è¿‡æ»¤
        if self.valid_label_start_date is not None:
            logger.info(
                f"valid_label_start_date è¿‡æ»¤: {samples_filtered_by_date:,} / {total_potential_samples:,} "
                f"({100 * samples_filtered_by_date / total_potential_samples:.1f}%) æ ·æœ¬è¢«è¿‡æ»¤"
            )
            
            if len(self.sample_index) == 0:
                logger.error(
                    f"\u274c æ•°æ®é›†ä¸ºç©ºï¼valid_label_start_date={self.valid_label_start_date} "
                    f"è¿‡æ»¤æ‰äº†æ‰€æœ‰æ ·æœ¬ã€‚è¯·æ£€æŸ¥é…ç½®ã€‚"
                )
            elif len(self.sample_index) < 100:
                logger.warning(
                    f"\u26a0\ufe0f æ•°æ®é›†æ ·æœ¬é‡è¿‡å° ({len(self.sample_index)} ä¸ª)ï¼Œå¯èƒ½å½±å“è®­ç»ƒæ•ˆæœã€‚"
                )
        
        logger.info("========================\n")
    
    def _precompute_label_ranks(self, labels: np.ndarray, window_size: int) -> np.ndarray:
        """
        é¢„è®¡ç®—æ‰€æœ‰ä½ç½®çš„çª—å£çº§æ ‡ç­¾æ’å
        
        å¯¹äºæ¯ä¸ªä½ç½® tï¼Œè®¡ç®— labels[t] åœ¨çª—å£ [t-window_size+1, t] å†…çš„æ’å
        
        Args:
            labels: è¯¥è‚¡ç¥¨çš„å…¨éƒ¨æ ‡ç­¾åºåˆ—
            window_size: çª—å£å¤§å°
            
        Returns:
            ranks: ä¸ labels ç­‰é•¿çš„æ•°ç»„ï¼Œæ¯ä¸ªä½ç½®å­˜å‚¨è¯¥ä½ç½®æ ‡ç­¾åœ¨çª—å£å†…çš„å½’ä¸€åŒ–æ’å [0, 1]
        """
        n = len(labels)
        ranks = np.full(n, 0.5, dtype=np.float32)  # é»˜è®¤å€¼ 0.5 (ä¸­é—´å€¼)
        
        low, high = self.label_rank_output_range
        
        for t in range(window_size, n):
            # çª—å£èŒƒå›´: [t - window_size + 1, t] (åŒ…å« t)
            # ä½†æ ‡ç­¾é¢„æµ‹çš„æ˜¯ t+1 æ—¶åˆ»ï¼Œæ‰€ä»¥æˆ‘ä»¬éœ€è¦ç”¨ [start, t] å†…çš„æ ‡ç­¾è®¡ç®— t ä½ç½®çš„æ’å
            # å®é™…ä¸Šï¼Œå¯¹äº sample_index ä¸­çš„ (stock_idx, time_idx)ï¼š
            #   - è¾“å…¥çª—å£æ˜¯ [time_idx - window_size + 1, time_idx]
            #   - æ ‡ç­¾æ˜¯ labels[time_idx + 1]
            # æ‰€ä»¥é¢„è®¡ç®—æ—¶ï¼Œå¯¹äº labels[t]ï¼Œæˆ‘ä»¬éœ€è¦ç”¨ [t - window_size, t-1] çª—å£
            # å³ï¼šæ¯”è¾ƒçš„æ˜¯ labels[t] ä¸å…¶ä¹‹å‰ window_size ä¸ªå†å²æ ‡ç­¾
            
            start = t - window_size
            window_labels = labels[start:t]  # å†å²çª—å£ï¼ˆä¸å«å½“å‰ï¼‰
            target_label = labels[t]
            
            # å¤„ç† NaN
            valid_mask = ~np.isnan(window_labels)
            if not np.any(valid_mask) or np.isnan(target_label):
                ranks[t] = (low + high) / 2  # ä¸­é—´å€¼
                continue
            
            valid_labels = window_labels[valid_mask]
            n_valid = len(valid_labels)
            
            if n_valid == 0:
                ranks[t] = (low + high) / 2
                continue
            
            # ä½¿ç”¨ searchsorted è®¡ç®—æ’å
            sorted_labels = np.sort(valid_labels)
            left_pos = np.searchsorted(sorted_labels, target_label, side='left')
            right_pos = np.searchsorted(sorted_labels, target_label, side='right')
            
            # å¹³å‡æ’åï¼Œå½’ä¸€åŒ–åˆ° [0, 1]
            rank = (left_pos + right_pos) / 2.0
            rank_normalized = rank / n_valid if n_valid > 0 else 0.5
            
            # æ˜ å°„åˆ°ç›®æ ‡èŒƒå›´
            ranks[t] = low + rank_normalized * (high - low)
        
        return ranks
    
    def __len__(self):
        return len(self.sample_index)
    
    def __getitem__(self, idx: int):
        stock_idx, time_idx = self.sample_index[idx]
        stock_info = self.stock_data[stock_idx]
        
        start_idx = time_idx - (self.window_size - 1)
        end_idx = time_idx + 1
        
        # è·å–çª—å£æ•°æ®ï¼ˆå¤åˆ¶ä»¥é¿å…ä¿®æ”¹åŸå§‹æ•°æ®ï¼‰
        X_seq = stock_info['features'][start_idx:end_idx].copy()
        y = stock_info['labels'][time_idx + 1]
        
        # ğŸ†• çª—å£çº§å˜æ¢ï¼ˆç ”æŠ¥æ ‡å‡†ï¼‰
        if self.enable_window_transform:
            X_seq = self._apply_window_transform(X_seq)
        
        # ğŸ†• æ ‡ç­¾çª—å£å†…æ—¶åºæ’åæ ‡å‡†åŒ–ï¼ˆä½¿ç”¨é¢„è®¡ç®—çš„æ’åï¼ŒO(1) æŸ¥è¡¨ï¼‰
        if self.label_rank_normalize:
            y = self._precomputed_label_ranks[stock_idx][time_idx + 1]
        
        X_tensor = torch.from_numpy(X_seq)
        y_tensor = torch.tensor(y, dtype=torch.float32)
        
        # æ„å»ºè¿”å›å…ƒç»„
        result = [X_tensor, y_tensor]
        
        # é™„åŠ ä¿¡æ¯ï¼ˆé¡ºåºï¼šX, y, [date_idx], [stock_idx]ï¼‰
        # æ³¨æ„ï¼šå¦‚æœåŒæ—¶å¯ç”¨ï¼Œé¡ºåºå¾ˆé‡è¦ï¼Œéœ€è¦åœ¨ Trainer ä¸­å¯¹åº”è§£åŒ…
        
        if self.return_date:
            # è¿”å›æ ‡ç­¾å¯¹åº”æ—¥æœŸçš„ç´¢å¼•
            label_date = stock_info['dates'][time_idx + 1]
            date_idx = self.date_to_idx.get(label_date, -1)
            result.append(torch.tensor(date_idx, dtype=torch.long))
            
        if self.return_stock_id:
            # è¿”å›è‚¡ç¥¨IDï¼ˆå…¨å±€ç´¢å¼•ï¼‰
            result.append(torch.tensor(stock_idx, dtype=torch.long))
            
        return tuple(result)
    
    def _apply_window_transform(self, X_seq: np.ndarray) -> np.ndarray:
        """
        å¯¹çª—å£æ•°æ®è¿›è¡Œç ”æŠ¥æ ‡å‡†çš„å˜æ¢
        
        Args:
            X_seq: çª—å£ç‰¹å¾åºåˆ—, shape=(window_size, num_features)
            
        Returns:
            å˜æ¢åçš„ç‰¹å¾åºåˆ—
            
        ç ”æŠ¥æ ‡å‡†ï¼š
        1. ä»·æ ¼å¯¹æ•°å˜æ¢: log(price_{t-i} / close_t)
           - å°†çª—å£å†…æ‰€æœ‰ä»·æ ¼é™¤ä»¥çª—å£æœ«ç«¯çš„æ”¶ç›˜ä»·
           - ç„¶åå–å¯¹æ•°
           - ç»“æœï¼šclose_t = 0, å…¶ä»–ä»·æ ¼ä¸ºç›¸å¯¹åå·®
           
        2. æˆäº¤é‡æ ‡å‡†åŒ–: volume_{t-i} / mean(volume_in_window)
           - å°†çª—å£å†…çš„æˆäº¤é‡é™¤ä»¥è¯¥çª—å£çš„å¹³å‡æˆäº¤é‡
           - ç»“æœï¼šå‡å€¼é™„è¿‘ â‰ˆ 1.0
        """
        # 1. ä»·æ ¼å¯¹æ•°å˜æ¢
        if self.window_price_log and self._close_index is not None:
            # è·å–çª—å£æœ«ç«¯ï¼ˆå½“å‰æ—¶åˆ»ï¼‰çš„æ”¶ç›˜ä»·ä½œä¸ºåŸºå‡†
            close_t = X_seq[-1, self._close_index]
            
            # ğŸ†• ä¿®å¤ï¼šè·³è¿‡ close_t <= 0 çš„çª—å£ï¼Œé¿å…é‡çº²ä¸ä¸€è‡´
            if close_t > 0 and not np.isnan(close_t):
                for col_idx in self._price_indices:
                    with np.errstate(divide='ignore', invalid='ignore'):
                        X_seq[:, col_idx] = np.log(X_seq[:, col_idx] / close_t)
                    # å¤„ç†æ— æ•ˆå€¼
                    X_seq[:, col_idx] = np.nan_to_num(
                        X_seq[:, col_idx], 
                        nan=0.0, posinf=0.0, neginf=0.0
                    )
            else:
                # ğŸ†• close_t <= 0 æˆ– NaNï¼Œè·³è¿‡ä»·æ ¼å˜æ¢ï¼Œä¿æŒåŸå§‹å€¼
                # æ³¨æ„ï¼šè¿™ä¼šå¯¼è‡´è¯¥æ ·æœ¬çš„ä»·æ ¼ç‰¹å¾ä¸å…¶ä»–æ ·æœ¬é‡çº²ä¸åŒ
                # å»ºè®®åœ¨æ•°æ®é¢„å¤„ç†é˜¶æ®µè¿‡æ»¤æ‰ close <= 0 çš„è®°å½•
                if not hasattr(self, '_close_t_warning_count'):
                    self._close_t_warning_count = 0
                self._close_t_warning_count += 1
                
                # åªè¾“å‡ºå‰å‡ æ¬¡è­¦å‘Šï¼Œé¿å…æ—¥å¿—æ´ªæ°´
                if self._close_t_warning_count <= 5:
                    import logging
                    logger = logging.getLogger(__name__)
                    logger.warning(
                        f"\u26a0\ufe0f çª—å£æœ«ç«¯ close_t={close_t:.4f} <= 0 æˆ– NaNï¼Œè·³è¿‡ä»·æ ¼å¯¹æ•°å˜æ¢ã€‚"
                        f"\n   å»ºè®®åœ¨æ•°æ®é¢„å¤„ç†é˜¶æ®µè¿‡æ»¤æ‰ close <= 0 çš„è®°å½•ã€‚"
                    )
                    if self._close_t_warning_count == 5:
                        logger.warning("   (åç»­ç›¸åŒè­¦å‘Šå°†ä¸å†æ˜¾ç¤º)")
                pass  # ä¿æŒåŸå§‹ç‰¹å¾å€¼
        
        # 2. æˆäº¤é‡æ ‡å‡†åŒ–
        if self.window_volume_norm and len(self._volume_indices) > 0:
            for col_idx in self._volume_indices:
                vol_window = X_seq[:, col_idx]
                vol_mean = np.nanmean(vol_window)
                
                if vol_mean > 0 and not np.isnan(vol_mean):
                    X_seq[:, col_idx] = vol_window / vol_mean
                    # å¤„ç†æ— æ•ˆå€¼
                    X_seq[:, col_idx] = np.nan_to_num(
                        X_seq[:, col_idx],
                        nan=1.0, posinf=1.0, neginf=1.0
                    )
        
        return X_seq
    
    def _apply_label_rank_normalize(self, labels: np.ndarray, 
                                    start_idx: int, end_idx: int, 
                                    target_idx: int) -> float:
        """
        å¯¹æ ‡ç­¾è¿›è¡Œçª—å£å†…æ—¶åºæ’åæ ‡å‡†åŒ–
        
        ğŸ”´ å…³é”®ï¼šé¿å…æœªæ¥ä¿¡æ¯æ³„éœ²
        - åªä½¿ç”¨ [start_idx, target_idx] èŒƒå›´å†…çš„å†å²æ ‡ç­¾è¿›è¡Œæ’å
        - ä¸ä½¿ç”¨ target_idx ä¹‹åçš„ä»»ä½•æ•°æ®
        
        Args:
            labels: è¯¥è‚¡ç¥¨çš„å…¨éƒ¨æ ‡ç­¾åºåˆ—
            start_idx: çª—å£èµ·å§‹ç´¢å¼•
            end_idx: çª—å£ç»“æŸç´¢å¼•ï¼ˆä¸å«ï¼‰
            target_idx: ç›®æ ‡æ ‡ç­¾çš„ç´¢å¼•
            
        Returns:
            æ ‡å‡†åŒ–åçš„æ ‡ç­¾å€¼ï¼ŒèŒƒå›´ä¸º label_rank_output_range
            
        ç®—æ³•:
        1. å–çª—å£å†…çš„å†å²æ ‡ç­¾ labels[start_idx:target_idx+1]
        2. å¯¹ç›®æ ‡æ ‡ç­¾åœ¨è¿™ä¸ªå†å²åºåˆ—ä¸­è¿›è¡Œæ’å
        3. å°†æ’åæ˜ å°„åˆ° output_range (é»˜è®¤ -1 åˆ° 1)
        """
        # å–çª—å£å†…çš„æ ‡ç­¾ï¼ˆåŒ…å«å½“å‰ç›®æ ‡æ ‡ç­¾ï¼‰
        # æ³¨æ„: è¿™é‡Œæˆ‘ä»¬ç”¨çš„æ˜¯å†å²çª—å£å†…çš„æ ‡ç­¾æ¥è®¡ç®—rankï¼Œç¡®ä¿æ— æœªæ¥ä¿¡æ¯æ³„éœ²
        window_labels = labels[start_idx:target_idx + 1]
        target_label = labels[target_idx]
        
        # å¤„ç† NaN å€¼
        valid_mask = ~np.isnan(window_labels)
        if not np.any(valid_mask) or np.isnan(target_label):
            return 0.0  # é»˜è®¤è¿”å›ä¸­é—´å€¼
        
        valid_labels = window_labels[valid_mask]
        n_valid = len(valid_labels)
        
        if n_valid <= 1:
            return 0.0  # åªæœ‰ä¸€ä¸ªæœ‰æ•ˆå€¼ï¼Œè¿”å›ä¸­é—´å€¼
        
        # è®¡ç®—æ’åï¼ˆä»å°åˆ°å¤§æ’åºï¼‰
        # ä½¿ç”¨ scipy.stats.rankdata é£æ ¼çš„æ’åè®¡ç®—
        # å¯¹äºé‡å¤å€¼ä½¿ç”¨å¹³å‡æ’å
        sorted_labels = np.sort(valid_labels)
        
        # æ‰¾åˆ° target_label çš„æ’åä½ç½®
        # ä½¿ç”¨äºŒåˆ†æŸ¥æ‰¾æ‰¾å·¦è¾¹ç•Œå’Œå³è¾¹ç•Œæ¥å¤„ç†é‡å¤å€¼
        left_pos = np.searchsorted(sorted_labels, target_label, side='left')
        right_pos = np.searchsorted(sorted_labels, target_label, side='right')
        
        # å¹³å‡æ’åï¼ˆå¤„ç†é‡å¤å€¼ï¼‰
        rank = (left_pos + right_pos) / 2.0  # èŒƒå›´ [0, n_valid-1]
        
        # å½’ä¸€åŒ–åˆ° [0, 1]
        rank_normalized = rank / (n_valid - 1) if n_valid > 1 else 0.5
        
        # æ˜ å°„åˆ°ç›®æ ‡èŒƒå›´ [low, high]
        low, high = self.label_rank_output_range
        result = low + rank_normalized * (high - low)
        
        return float(result)
    
    def get_date_for_idx(self, date_idx: int):
        """æ ¹æ®æ—¥æœŸç´¢å¼•è·å–å®é™…æ—¥æœŸ"""
        return self.idx_to_date.get(date_idx, None)
    
    def get_num_dates(self) -> int:
        """è·å–ä¸åŒæ—¥æœŸçš„æ•°é‡"""
        return len(self.date_to_idx)


# ä¸ºäº†å‘åå…¼å®¹ï¼Œä¿ç•™åŸæ¥çš„ TimeSeriesStockDataset åç§°
TimeSeriesStockDataset = TimeSeriesStockDatasetWithDate


@dataclass
class DatasetCollection:
    """æ•°æ®é›†é›†åˆ"""
    train: Dataset
    val: Dataset
    test: Dataset
    metadata: Dict[str, Any]
    
    def get_loaders(self, batch_size: Optional[int] = None,
                   num_workers: Optional[int] = None,
                   shuffle_train: Optional[bool] = None,
                   use_cross_sectional: bool = False) -> 'LoaderCollection':
        """
        åˆ›å»ºæ•°æ®åŠ è½½å™¨
        
        Args:
            batch_size: æ‰¹æ¬¡å¤§å°
            num_workers: å·¥ä½œè¿›ç¨‹æ•°
            shuffle_train: æ˜¯å¦æ‰“ä¹±è®­ç»ƒæ•°æ®ï¼ˆä¼ ç»Ÿæ¨¡å¼ï¼‰
            use_cross_sectional: ğŸ”´ æ˜¯å¦ä½¿ç”¨æˆªé¢æ‰¹é‡‡æ ·ï¼ˆIC Loss å¿…é¡»å¼€å¯ï¼‰
        """
        bs = batch_size or 256
        nw = num_workers or 0
        
        if use_cross_sectional and isinstance(self.train, TimeSeriesStockDatasetWithDate):
            # ä½¿ç”¨æˆªé¢æ‰¹é‡‡æ ·å™¨
            train_sampler = CrossSectionalBatchSampler(
                self.train, batch_size=bs, shuffle_dates=True
            )
            train_loader = DataLoader(
                self.train,
                batch_sampler=train_sampler,
                num_workers=nw
            )
        else:
            # ä¼ ç»Ÿéšæœºé‡‡æ ·
            train_loader = DataLoader(
                self.train,
                batch_size=bs,
                shuffle=shuffle_train if shuffle_train is not None else True,
                num_workers=nw
            )
        
        return LoaderCollection(
            train=train_loader,
            val=DataLoader(
                self.val,
                batch_size=bs,
                shuffle=False,
                num_workers=nw
            ),
            test=DataLoader(
                self.test,
                batch_size=bs,
                shuffle=False,
                num_workers=nw
            ),
            metadata=self.metadata
        )


@dataclass
class LoaderCollection:
    """æ•°æ®åŠ è½½å™¨é›†åˆ"""
    train: DataLoader
    val: DataLoader
    test: DataLoader
    metadata: Dict[str, Any]


class InferenceDataset(Dataset):
    """æ¨ç†æ•°æ®é›†ï¼ˆä»…ç‰¹å¾ï¼Œæ— æ ‡ç­¾ï¼‰"""
    
    def __init__(self, df: pd.DataFrame, feature_cols: List[str],
                 window_size: int, stock_col: str = 'ts_code',
                 time_col: str = 'trade_date'):
        """
        Args:
            df: æ•°æ®DataFrame
            feature_cols: ç‰¹å¾åˆ—åˆ—è¡¨
            window_size: æ—¶é—´çª—å£å¤§å°
            stock_col: è‚¡ç¥¨ä»£ç åˆ—
            time_col: æ—¶é—´åˆ—
        """
        self.window_size = window_size
        self.feature_cols = feature_cols
        self.stock_col = stock_col
        self.time_col = time_col
        
        self._build_sample_index(df)
    
    def _build_sample_index(self, df: pd.DataFrame):
        """é¢„å…ˆæ„å»ºæ ·æœ¬ç´¢å¼•"""
        df = df.copy()
        df = df.dropna(subset=self.feature_cols)
        df = df.sort_values([self.stock_col, self.time_col]).reset_index(drop=True)
        
        self.stock_data = {}
        self.sample_index = []
        self.sample_info = []  # å­˜å‚¨æ ·æœ¬å…ƒä¿¡æ¯
        
        stock_idx = 0
        for ts_code, stock_df in df.groupby(self.stock_col, observed=False):
            n = len(stock_df)
            
            if n < self.window_size:
                continue
            
            features = stock_df[self.feature_cols].values.astype(np.float32)
            dates = stock_df[self.time_col].values
            
            self.stock_data[stock_idx] = {
                'ts_code': ts_code,
                'features': features,
                'dates': dates,
                'n': n
            }
            
            # æ„å»ºæ ·æœ¬ç´¢å¼•
            for t in range(self.window_size - 1, n):
                self.sample_index.append((stock_idx, t))
                self.sample_info.append({
                    'ts_code': ts_code,
                    'date': dates[t]
                })
            
            stock_idx += 1
    
    def __len__(self):
        return len(self.sample_index)
    
    def __getitem__(self, idx: int) -> torch.Tensor:
        stock_idx, time_idx = self.sample_index[idx]
        stock_info = self.stock_data[stock_idx]
        
        start_idx = time_idx - (self.window_size - 1)
        end_idx = time_idx + 1
        
        X_seq = stock_info['features'][start_idx:end_idx]
        
        return torch.from_numpy(X_seq)
    
    def get_sample_info(self, idx: int) -> Dict[str, Any]:
        """è·å–æ ·æœ¬å…ƒä¿¡æ¯"""
        return self.sample_info[idx]


class DatasetFactory:
    """æ•°æ®é›†å·¥å‚"""
    
    def __init__(self, config: DataConfig):
        """
        Args:
            config: DataConfigé…ç½®å¯¹è±¡
        """
        self.config = config
        self.logger = self._setup_logger()
    
    def _setup_logger(self) -> logging.Logger:
        """é…ç½®æ—¥å¿—"""
        logger = logging.getLogger('DatasetFactory')
        logger.setLevel(getattr(logging, self.config.log_level))
        
        if not logger.handlers:
            handler = logging.StreamHandler()
            formatter = logging.Formatter(
                '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
            )
            handler.setFormatter(formatter)
            logger.addHandler(handler)
        
        return logger
    
    def create_datasets(self,
                       train_df: pd.DataFrame,
                       val_df: pd.DataFrame,
                       test_df: pd.DataFrame,
                       feature_cols: List[str],
                       test_valid_label_start_date: Optional[pd.Timestamp] = None) -> DatasetCollection:
        """
        åˆ›å»ºè®­ç»ƒã€éªŒè¯ã€æµ‹è¯•æ•°æ®é›†
        
        Args:
            train_df: è®­ç»ƒæ•°æ®
            val_df: éªŒè¯æ•°æ®
            test_df: æµ‹è¯•æ•°æ®
            feature_cols: ç‰¹å¾åˆ—åˆ—è¡¨
            test_valid_label_start_date: ğŸ†• æµ‹è¯•é›†çš„æœ‰æ•ˆæ ‡ç­¾èµ·å§‹æ—¥æœŸ
                                         å¦‚æœè®¾ç½®ï¼Œåªä¸ºè¯¥æ—¥æœŸä¹‹åçš„æ ‡ç­¾ç”Ÿæˆæ ·æœ¬
            
        Returns:
            DatasetCollectionå¯¹è±¡
        """
        self.logger.info("ğŸ­ åˆ›å»ºæ•°æ®é›†...")
        
        # ğŸ†• ä»é…ç½®ä¸­è·å–çª—å£å˜æ¢å‚æ•°
        enable_wt = getattr(self.config, 'enable_window_transform', False)
        price_log = getattr(self.config, 'window_price_log', False)
        vol_norm = getattr(self.config, 'window_volume_norm', False)
        price_cols = getattr(self.config, 'price_cols', ['open', 'high', 'low', 'close', 'vwap'])
        close_col = getattr(self.config, 'close_col', 'close')
        volume_cols = getattr(self.config, 'volume_cols', ['vol', 'amount'])
        
        if enable_wt:
            self.logger.info(f"   ğŸ”„ å¯ç”¨çª—å£çº§å˜æ¢:")
            self.logger.info(f"      ä»·æ ¼å¯¹æ•°å˜æ¢: {price_log} ({price_cols})")
            self.logger.info(f"      æˆäº¤é‡æ ‡å‡†åŒ–: {vol_norm} ({volume_cols})")
        
        # åˆ›å»ºæ•°æ®é›†
        train_dataset = TimeSeriesStockDataset(
            train_df, feature_cols, self.config.label_col,
            self.config.window_size, self.config.stock_col, self.config.time_col,
            # ğŸ†• ä¼ é€’çª—å£å˜æ¢é…ç½®
            enable_window_transform=enable_wt,
            window_price_log=price_log,
            window_volume_norm=vol_norm,
            price_cols=price_cols,
            close_col=close_col,
            volume_cols=volume_cols
        )
        
        val_dataset = TimeSeriesStockDataset(
            val_df, feature_cols, self.config.label_col,
            self.config.window_size, self.config.stock_col, self.config.time_col,
            enable_window_transform=enable_wt,
            window_price_log=price_log,
            window_volume_norm=vol_norm,
            price_cols=price_cols,
            close_col=close_col,
            volume_cols=volume_cols
        )
        
        # ğŸ†• æµ‹è¯•é›†ï¼šä¼ é€’æœ‰æ•ˆæ ‡ç­¾èµ·å§‹æ—¥æœŸ
        test_dataset = TimeSeriesStockDataset(
            test_df, feature_cols, self.config.label_col,
            self.config.window_size, self.config.stock_col, self.config.time_col,
            enable_window_transform=enable_wt,
            window_price_log=price_log,
            window_volume_norm=vol_norm,
            price_cols=price_cols,
            close_col=close_col,
            volume_cols=volume_cols,
            valid_label_start_date=test_valid_label_start_date  # ğŸ†• åªä¸ºè¯¥æ—¥æœŸåçš„æ ‡ç­¾ç”Ÿæˆæ ·æœ¬
        )
        
        if test_valid_label_start_date is not None:
            self.logger.info(f"   ğŸ”´ æµ‹è¯•é›†æœ‰æ•ˆæ ‡ç­¾èµ·å§‹: {test_valid_label_start_date}")
        
        # æ”¶é›†å…ƒæ•°æ®
        metadata = {
            'feature_cols': feature_cols,
            'num_features': len(feature_cols),
            'window_size': self.config.window_size,
            'label_col': self.config.label_col,
            'train_samples': len(train_dataset),
            'val_samples': len(val_dataset),
            'test_samples': len(test_dataset),
            'test_valid_label_start_date': test_valid_label_start_date,  # ğŸ†•
        }
        
        self.logger.info(f"   è®­ç»ƒé›†: {len(train_dataset):,} æ ·æœ¬")
        self.logger.info(f"   éªŒè¯é›†: {len(val_dataset):,} æ ·æœ¬")
        self.logger.info(f"   æµ‹è¯•é›†: {len(test_dataset):,} æ ·æœ¬")
        
        return DatasetCollection(
            train=train_dataset,
            val=val_dataset,
            test=test_dataset,
            metadata=metadata
        )
    
    def create_inference_dataset(self, df: pd.DataFrame,
                                feature_cols: List[str]) -> InferenceDataset:
        """
        åˆ›å»ºæ¨ç†æ•°æ®é›†
        
        Args:
            df: æ•°æ®DataFrame
            feature_cols: ç‰¹å¾åˆ—åˆ—è¡¨
            
        Returns:
            InferenceDatasetå¯¹è±¡
        """
        self.logger.info("ğŸ”® åˆ›å»ºæ¨ç†æ•°æ®é›†...")
        
        dataset = InferenceDataset(
            df, feature_cols, self.config.window_size,
            self.config.stock_col, self.config.time_col
        )
        
        self.logger.info(f"   æ¨ç†æ ·æœ¬: {len(dataset):,}")
        
        return dataset


if __name__ == '__main__':
    # æµ‹è¯•æ•°æ®é›†å·¥å‚
    from config import DataConfig
    
    print("=" * 80)
    print("DatasetFactory æµ‹è¯•")
    print("=" * 80)
    
    # åˆ›å»ºé…ç½®
    config = DataConfig(window_size=40)
    
    # åˆ›å»ºå·¥å‚
    factory = DatasetFactory(config)
    
    # åˆ›å»ºæ¨¡æ‹Ÿæ•°æ®
    np.random.seed(42)
    dates = pd.date_range('2020-01-01', periods=200)
    stocks = ['000001.SZ', '000002.SZ']
    
    data = []
    for stock in stocks:
        for date in dates:
            data.append({
                'ts_code': stock,
                'trade_date': date,
                'y_processed': np.random.randn(),
                'feature1': np.random.randn(),
                'feature2': np.random.randn(),
            })
    
    df = pd.DataFrame(data)
    
    # åˆ’åˆ†æ•°æ®
    n = len(dates)
    train_end = dates[int(n * 0.7)]
    val_end = dates[int(n * 0.85)]
    
    train_df = df[df['trade_date'] <= train_end]
    val_df = df[(df['trade_date'] > train_end) & (df['trade_date'] <= val_end)]
    test_df = df[df['trade_date'] > val_end]
    
    # åˆ›å»ºæ•°æ®é›†
    datasets = factory.create_datasets(
        train_df, val_df, test_df,
        feature_cols=['feature1', 'feature2']
    )
    
    print(f"\næ•°æ®é›†å…ƒæ•°æ®:")
    for key, value in datasets.metadata.items():
        print(f"  {key}: {value}")
    
    # æµ‹è¯•æ•°æ®åŠ è½½å™¨
    print(f"\nåˆ›å»ºæ•°æ®åŠ è½½å™¨...")
    loaders = datasets.get_loaders(batch_size=32)
    
    # æµ‹è¯•ä¸€ä¸ªæ‰¹æ¬¡
    batch_x, batch_y = next(iter(loaders.train))
    print(f"  æ‰¹æ¬¡ç‰¹å¾å½¢çŠ¶: {batch_x.shape}")
    print(f"  æ‰¹æ¬¡æ ‡ç­¾å½¢çŠ¶: {batch_y.shape}")
    
    # æµ‹è¯•æ¨ç†æ•°æ®é›†
    print(f"\nåˆ›å»ºæ¨ç†æ•°æ®é›†...")
    inference_dataset = factory.create_inference_dataset(
        test_df, feature_cols=['feature1', 'feature2']
    )
    
    sample = inference_dataset[0]
    info = inference_dataset.get_sample_info(0)
    print(f"  æ¨ç†æ ·æœ¬å½¢çŠ¶: {sample.shape}")
    print(f"  æ ·æœ¬ä¿¡æ¯: {info}")
    
    print("\nâœ… æ•°æ®é›†å·¥å‚æµ‹è¯•å®Œæˆ")
