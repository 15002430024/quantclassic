# DataManager ä½¿ç”¨æŒ‡å—

## ğŸ¯ ç®€ä»‹

DataManager æ˜¯ä¸€ä¸ªå®Œæ•´çš„å·¥ç¨‹åŒ–æ•°æ®ç®¡ç†æ¨¡å—ï¼Œå·²æˆåŠŸå°è£…å¹¶é€šè¿‡æ‰€æœ‰æµ‹è¯•ã€‚

## âœ… æ¨¡å—çŠ¶æ€

```
âœ… æ‰€æœ‰æµ‹è¯•é€šè¿‡ (6/6)
âœ… å®Œæ•´æ–‡æ¡£
âœ… 10ä¸ªä½¿ç”¨ç¤ºä¾‹
âœ… å¿«é€Ÿå¼€å§‹è„šæœ¬
âœ… æ»šåŠ¨çª—å£è®­ç»ƒæ”¯æŒï¼ˆæ–°å¢ï¼‰
```

## ğŸ†• æ–°åŠŸèƒ½ï¼šæ»šåŠ¨çª—å£è®­ç»ƒ

DataManager ç°åœ¨æ”¯æŒ**æ»šåŠ¨çª—å£ï¼ˆWalk-Forwardï¼‰æ¨¡å‹è®­ç»ƒ**ï¼Œè¿™æ˜¯é‡åŒ–é‡‘èä¸­æœ€ä¸¥è°¨çš„æ—¶é—´åºåˆ—éªŒè¯æ–¹æ³•ã€‚

**å¿«é€Ÿä½¿ç”¨**:
```python
# 1. é…ç½®rollingç­–ç•¥
config = DataConfig(split_strategy='rolling', rolling_window_size=252, rolling_step=63)

# 2. åˆ›å»ºè®­ç»ƒå™¨
dm = DataManager(config)
dm.run_full_pipeline()
trainer = dm.create_rolling_window_trainer()

# 3. è®­ç»ƒæ‰€æœ‰çª—å£
results = trainer.train_all_windows(model_class=GRUModel, model_config=gru_config)

# 4. é¢„æµ‹å¹¶åˆå¹¶
predictions = trainer.predict_all_windows(results)
```

**è¯¦ç»†æ–‡æ¡£**: å‚è§ [æ»šåŠ¨çª—å£è®­ç»ƒæŒ‡å—](./ROLLING_WINDOW_GUIDE.md)

## ğŸš€ ä¸‰ç§ä½¿ç”¨æ–¹å¼

### æ–¹å¼1: å¿«é€Ÿå¼€å§‹è„šæœ¬ï¼ˆæ¨èæ–°æ‰‹ï¼‰

```bash
cd /home/u2025210237/jupyterlab/quantclassic/data_manager
python quickstart.py
```

### æ–¹å¼2: ä¸€é”®è¿è¡Œå®Œæ•´æµæ°´çº¿

```python
from data_manager import DataManager, DataConfig

# åˆ›å»ºé…ç½®
config = DataConfig(
    base_dir='rq_data_parquet',
    data_file='train_data_final.parquet',
    window_size=40,
    batch_size=256
)

# ä¸€é”®è¿è¡Œ
manager = DataManager(config)
loaders = manager.run_full_pipeline()

# å¼€å§‹è®­ç»ƒ
for batch_x, batch_y in loaders.train:
    # æ‚¨çš„è®­ç»ƒä»£ç 
    pass
```

### æ–¹å¼3: åˆ†æ­¥éª¤æ‰§è¡Œï¼ˆæ¨èé«˜çº§ç”¨æˆ·ï¼‰

```python
from data_manager import DataManager, DataConfig

config = DataConfig()
manager = DataManager(config)

# æ­¥éª¤1: åŠ è½½æ•°æ®
raw_data = manager.load_raw_data()

# æ­¥éª¤2: éªŒè¯æ•°æ®ï¼ˆå¯é€‰ï¼‰
report = manager.validate_data_quality()

# æ­¥éª¤3: ç‰¹å¾å·¥ç¨‹
feature_cols = manager.preprocess_features(auto_filter=True)

# æ­¥éª¤4-5: åˆ›å»ºæ•°æ®é›†
datasets = manager.create_datasets()

# æ­¥éª¤6: è·å–æ•°æ®åŠ è½½å™¨
loaders = manager.get_dataloaders(batch_size=256)
```

## ğŸ“Š å®é™…ä½¿ç”¨ç¤ºä¾‹

### ç¤ºä¾‹1: è®­ç»ƒVAE-NeuralODEæ¨¡å‹

```python
from data_manager import DataManager, DataConfig
import torch
import torch.nn as nn

# 1. å‡†å¤‡æ•°æ®
config = DataConfig(
    base_dir='rq_data_parquet',
    data_file='train_data_final.parquet',
    window_size=40,
    batch_size=256,
    split_strategy='time_series'
)

manager = DataManager(config)
loaders = manager.run_full_pipeline()

# 2. åˆ›å»ºæ¨¡å‹ï¼ˆä½¿ç”¨data.pyä¸­çš„æ¨¡å‹ï¼‰
from data_manager.data import VAE_NeuralODE, Config

model_config = Config()
model_config.LATENT_DIM = 32
model_config.HIDDEN_DIM = 64
model_config.WINDOW_SIZE = config.window_size

input_dim = len(manager.feature_cols)
model = VAE_NeuralODE(model_config, input_dim).to(config.DEVICE)

# 3. è®­ç»ƒ
optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)

for epoch in range(num_epochs):
    for batch_x, batch_y in loaders.train:
        batch_x = batch_x.to(config.DEVICE)
        batch_y = batch_y.to(config.DEVICE)
        
        # å‰å‘ä¼ æ’­
        x_recon, y_pred, mu, logvar = model(batch_x)
        
        # è®¡ç®—æŸå¤±ï¼ˆä½¿ç”¨data.pyä¸­çš„æŸå¤±å‡½æ•°ï¼‰
        # loss = compute_loss(...)
        
        # åå‘ä¼ æ’­
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
```

### ç¤ºä¾‹2: å›æµ‹åœºæ™¯ï¼ˆæ»šåŠ¨çª—å£ï¼‰

```python
from data_manager import DataManager, DataConfig

# é…ç½®æ»šåŠ¨çª—å£
config = DataConfig(
    split_strategy='rolling',
    rolling_window_size=252,  # 1å¹´è®­ç»ƒçª—å£
    rolling_step=63           # æ¯å­£åº¦æ»šåŠ¨ä¸€æ¬¡
)

manager = DataManager(config)
# æ³¨æ„ï¼šæ»šåŠ¨çª—å£è¿”å›å¤šä¸ªåˆ’åˆ†ï¼Œéœ€è¦ç‰¹æ®Šå¤„ç†
```

### ç¤ºä¾‹3: ä¸åŒæ•°æ®æº

```python
# CSVæ ¼å¼
config = DataConfig(
    base_dir='data',
    data_file='stock_data.csv',
    data_format='csv'
)

# HDF5æ ¼å¼
config = DataConfig(
    base_dir='data',
    data_file='stock_data.h5',
    data_format='hdf5'
)
```

## ğŸ”§ å¸¸ç”¨é…ç½®

### å¿«é€Ÿæµ‹è¯•é…ç½®

```python
from data_manager import ConfigTemplates

config = ConfigTemplates.quick_test()
# ç‰¹ç‚¹: å°æ‰¹é‡ï¼Œå¿«é€ŸéªŒè¯ä»£ç 
```

### ç”Ÿäº§ç¯å¢ƒé…ç½®

```python
config = ConfigTemplates.production()
# ç‰¹ç‚¹: å¤§æ‰¹é‡ï¼Œå¤šè¿›ç¨‹ï¼Œé«˜æ€§èƒ½
```

### è‡ªå®šä¹‰é…ç½®

```python
config = DataConfig(
    # æ•°æ®
    base_dir='rq_data_parquet',
    data_file='train_data_final.parquet',
    
    # ç‰¹å¾
    window_size=60,
    label_col='y_processed',
    
    # åˆ’åˆ†
    split_strategy='time_series',
    train_ratio=0.7,
    val_ratio=0.15,
    test_ratio=0.15,
    
    # åŠ è½½
    batch_size=512,
    num_workers=4,
    
    # ä¼˜åŒ–
    use_dtype_optimization=True,
    enable_cache=True,
)
```

## ğŸ“ ä¸data.pyçš„é›†æˆ

DataManager å¯ä»¥å®Œå…¨æ›¿ä»£ data.py ä¸­çš„æ•°æ®ç®¡ç†éƒ¨åˆ†ï¼š

### ä¹‹å‰ï¼ˆdata.pyï¼‰

```python
# data.py ä¸­çš„ä»£ç 
df = pd.read_parquet('rq_data_parquet/train_data_final.parquet')
train_loader, val_loader, test_loader = create_dataloaders(df, config)
```

### ä¹‹åï¼ˆä½¿ç”¨DataManagerï¼‰

```python
# ä½¿ç”¨DataManager
from data_manager import DataManager, DataConfig

config = DataConfig()
manager = DataManager(config)
loaders = manager.run_full_pipeline()

# loaders.train, loaders.val, loaders.test
# å®Œå…¨å…¼å®¹åŸæœ‰çš„è®­ç»ƒä»£ç 
```

### ä¿ç•™data.pyçš„æ¨¡å‹éƒ¨åˆ†

```python
# ç»§ç»­ä½¿ç”¨data.pyä¸­çš„æ¨¡å‹å’Œè®­ç»ƒå™¨
from data_manager.data import VAE_NeuralODE, Trainer, FactorGenerator

# åªæ›¿æ¢æ•°æ®ç®¡ç†éƒ¨åˆ†
from data_manager import DataManager, DataConfig

# æ•°æ®å‡†å¤‡
manager = DataManager(DataConfig())
loaders = manager.run_full_pipeline()

# æ¨¡å‹è®­ç»ƒï¼ˆä½¿ç”¨data.pyçš„ä»£ç ï¼‰
trainer = Trainer(config)
model, train_losses, val_losses = trainer.train(loaders.train, loaders.val)
```

## ğŸ¨ é«˜çº§åŠŸèƒ½

### 1. ç‰¹å¾è¿‡æ»¤

```python
manager = DataManager(config)
raw_data = manager.load_raw_data()

# è‡ªå®šä¹‰è¿‡æ»¤æ¡ä»¶
filtered_features = manager.feature_engineer.filter_features(
    raw_data,
    min_variance=1e-5,
    max_missing_ratio=0.3,
    max_correlation=0.95
)
```

### 2. æ•°æ®éªŒè¯

```python
# è·å–è¯¦ç»†éªŒè¯æŠ¥å‘Š
report = manager.validate_data_quality()

if not report.is_valid:
    print("æ•°æ®è´¨é‡é—®é¢˜:")
    for error in report.errors:
        print(f"  âŒ {error}")
    
    for warning in report.warnings:
        print(f"  âš ï¸ {warning}")
```

### 3. çŠ¶æ€ä¿å­˜

```python
# ç¬¬ä¸€æ¬¡è¿è¡Œ
manager = DataManager(config)
loaders = manager.run_full_pipeline()
manager.save_state('cache/my_state.pkl')

# åç»­è¿è¡Œï¼ˆå¿«é€ŸåŠ è½½ï¼‰
manager = DataManager()
manager.load_state('cache/my_state.pkl')
loaders = manager.get_dataloaders()
```

### 4. æ¨ç†æ•°æ®é›†

```python
# åˆ›å»ºæ¨ç†æ•°æ®é›†ï¼ˆæ— æ ‡ç­¾ï¼‰
inference_dataset = manager.factory.create_inference_dataset(
    df=test_data,
    feature_cols=manager.feature_cols
)

# ç”¨äºå› å­ç”Ÿæˆ
for sample in inference_dataset:
    factor = model.predict(sample)
```

## ğŸ“š æ›´å¤šèµ„æº

- **å®Œæ•´æ–‡æ¡£**: `README.md`
- **ä½¿ç”¨ç¤ºä¾‹**: `examples.py`ï¼ˆ10ä¸ªç¤ºä¾‹ï¼‰
- **å¿«é€Ÿå¼€å§‹**: `quickstart.py`
- **é¡¹ç›®æ€»ç»“**: `SUMMARY.md`
- **æµ‹è¯•**: `test_module.py`

## ğŸ”— æ¨¡å—å¯¼å…¥

```python
# ä¸»è¦ç±»
from data_manager import DataManager, DataConfig, ConfigTemplates

# ç»„ä»¶ç±»
from data_manager import (
    DataLoaderEngine,      # æ•°æ®åŠ è½½å™¨
    FeatureEngineer,       # ç‰¹å¾å·¥ç¨‹å¸ˆ
    DataValidator,         # æ•°æ®éªŒè¯å™¨
    DatasetFactory,        # æ•°æ®é›†å·¥å‚
)

# åˆ’åˆ†å™¨
from data_manager import (
    TimeSeriesSplitter,         # æ—¶é—´åºåˆ—åˆ’åˆ†
    StratifiedStockSplitter,    # åˆ†å±‚åˆ’åˆ†
    RollingWindowSplitter,      # æ»šåŠ¨çª—å£
    create_splitter,            # å·¥å‚å‡½æ•°
)

# æ•°æ®ç»“æ„
from data_manager import (
    DatasetCollection,     # æ•°æ®é›†é›†åˆ
    LoaderCollection,      # åŠ è½½å™¨é›†åˆ
    ValidationReport,      # éªŒè¯æŠ¥å‘Š
)
```

## âš ï¸ æ³¨æ„äº‹é¡¹

1. **è·¯å¾„é…ç½®**: ç¡®ä¿ `base_dir` å’Œ `data_file` æŒ‡å‘æ­£ç¡®çš„æ•°æ®æ–‡ä»¶
2. **å†…å­˜ç®¡ç†**: å¤„ç†å¤§æ•°æ®é›†æ—¶è®¾ç½® `chunk_size` å’Œ `use_dtype_optimization=True`
3. **æ—¶åºæ•°æ®**: ä½¿ç”¨ `time_series` æˆ– `stratified` åˆ’åˆ†ï¼Œé¿å… `random`
4. **GPUè®­ç»ƒ**: è®¾ç½® `pin_memory=True` å’Œ `num_workers>0`

## ğŸ› æ•…éšœæ’é™¤

### é—®é¢˜: æ‰¾ä¸åˆ°æ•°æ®æ–‡ä»¶

```python
# æ£€æŸ¥è·¯å¾„
config = DataConfig()
print(f"æ•°æ®è·¯å¾„: {config.data_path}")

# ä½¿ç”¨ç»å¯¹è·¯å¾„
config = DataConfig(
    base_dir='/home/u2025210237/jupyterlab/rq_data_parquet',
    data_file='train_data_final.parquet'
)
```

### é—®é¢˜: å†…å­˜ä¸è¶³

```python
config = DataConfig(
    batch_size=128,           # å‡å°æ‰¹é‡
    chunk_size=50000,         # åˆ†å—åŠ è½½
    use_dtype_optimization=True,
)
```

### é—®é¢˜: ç‰¹å¾åˆ—ä¸ºç©º

```python
# æ‰‹åŠ¨æŒ‡å®šç‰¹å¾åˆ—
config = DataConfig(
    feature_cols=['feature1', 'feature2', 'feature3']
)
```

## ğŸ“ è·å–å¸®åŠ©

```bash
# æŸ¥çœ‹æ¨¡å—ä¿¡æ¯
python -c "from data_manager import get_version; print(get_version())"

# è¿è¡Œæµ‹è¯•
python test_module.py

# æŸ¥çœ‹ç¤ºä¾‹
python examples.py
```

---

**æœ€åæ›´æ–°**: 2025-11-19  
**ç‰ˆæœ¬**: 1.0.0  
**æµ‹è¯•çŠ¶æ€**: âœ… æ‰€æœ‰æµ‹è¯•é€šè¿‡
