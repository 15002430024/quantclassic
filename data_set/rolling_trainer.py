"""
Rolling Window Trainer - æ»šåŠ¨çª—å£è®­ç»ƒå™¨

å®ç°æ»šåŠ¨çª—å£ï¼ˆWalk-Forwardï¼‰æ¨¡å‹è®­ç»ƒå’Œé¢„æµ‹
"""

import logging
from typing import List, Tuple, Optional, Dict, Any
import pandas as pd
import numpy as np
from pathlib import Path
import torch
from torch.utils.data import DataLoader

from .factory import TimeSeriesStockDataset, CrossSectionalBatchSampler


class RollingWindowTrainer:
    """
    æ»šåŠ¨çª—å£è®­ç»ƒå™¨
    
    å®ç° Walk-Forward éªŒè¯ç­–ç•¥ï¼š
    1. å°†æ—¶é—´åºåˆ—åˆ’åˆ†ä¸ºå¤šä¸ªæ»šåŠ¨çª—å£
    2. åœ¨æ¯ä¸ªçª—å£ä¸Šç‹¬ç«‹è®­ç»ƒæ¨¡å‹
    3. åœ¨ä¸‹ä¸€ä¸ªçª—å£ä¸Šæµ‹è¯•
    4. åˆå¹¶æ‰€æœ‰çª—å£çš„é¢„æµ‹ç»“æœ
    
    Features:
    - æ”¯æŒå®Œå…¨ç‹¬ç«‹çš„æ»šåŠ¨çª—å£è®­ç»ƒ
    - æ”¯æŒå¢é‡è®­ç»ƒï¼ˆä½¿ç”¨å‰ä¸€çª—å£æ¨¡å‹åˆå§‹åŒ–ï¼‰
    - è‡ªåŠ¨ç®¡ç†æ¨¡å‹ä¿å­˜å’ŒåŠ è½½
    - æä¾›è¯¦ç»†çš„è®­ç»ƒå’Œé¢„æµ‹æ—¥å¿—
    
    Example:
        >>> trainer = RollingWindowTrainer(
        ...     windows=rolling_windows,
        ...     config=data_config,
        ...     feature_cols=feature_cols
        ... )
        >>> results = trainer.train_all_windows(
        ...     model_class=GRUModel,
        ...     model_config=gru_config,
        ...     save_dir='output/rolling_models'
        ... )
        >>> predictions = trainer.predict_all_windows(results)
    """
    
    def __init__(
        self,
        windows: List[Tuple[pd.DataFrame, pd.DataFrame]],
        config: Any,
        feature_cols: List[str],
        logger: Optional[logging.Logger] = None,
        stock_universe: Optional[List[str]] = None  # ğŸ†• å…¨å±€è‚¡ç¥¨æ± 
    ):
        """
        åˆå§‹åŒ–æ»šåŠ¨çª—å£è®­ç»ƒå™¨
        
        Args:
            windows: æ»šåŠ¨çª—å£åˆ—è¡¨ [(train_df_1, test_df_1), ...]
            config: DataConfig é…ç½®å¯¹è±¡
            feature_cols: ç‰¹å¾åˆ—ååˆ—è¡¨
            logger: æ—¥å¿—è®°å½•å™¨ï¼ˆå¯é€‰ï¼‰
            stock_universe: å…¨å±€è‚¡ç¥¨ä»£ç åˆ—è¡¨ï¼ˆç”¨äºç»Ÿä¸€IDæ˜ å°„ï¼‰
        """
        self.windows = windows
        self.config = config
        self.feature_cols = feature_cols
        self.logger = logger or self._setup_logger()
        
        # ğŸ†• æ„å»ºå…¨å±€è‚¡ç¥¨æ˜ å°„
        if stock_universe:
            self.stock_map = {stock: i for i, stock in enumerate(sorted(stock_universe))}
            self.logger.info(f"  å…¨å±€è‚¡ç¥¨æ± : {len(stock_universe)} åª")
        else:
            self.stock_map = None
            self.logger.info(f"  æœªæä¾›å…¨å±€è‚¡ç¥¨æ± ï¼Œå°†ä½¿ç”¨å±€éƒ¨æ˜ å°„")
        
        self.n_windows = len(windows)
        self.window_results = []  # å­˜å‚¨æ¯ä¸ªçª—å£çš„è®­ç»ƒç»“æœ
        
        self.logger.info("=" * 80)
        self.logger.info("ğŸ”„ åˆå§‹åŒ–æ»šåŠ¨çª—å£è®­ç»ƒå™¨")
        self.logger.info("=" * 80)
        self.logger.info(f"  æ€»çª—å£æ•°: {self.n_windows}")
        self.logger.info(f"  ç‰¹å¾ç»´åº¦: {len(feature_cols)}")
        
        # ç»Ÿè®¡çª—å£ä¿¡æ¯
        train_sizes = [len(train_df) for train_df, _ in windows]
        test_sizes = [len(test_df) for _, test_df in windows]
        
        self.logger.info(f"  è®­ç»ƒé›†å¤§å°: {min(train_sizes):,} ~ {max(train_sizes):,} æ ·æœ¬")
        self.logger.info(f"  æµ‹è¯•é›†å¤§å°: {min(test_sizes):,} ~ {max(test_sizes):,} æ ·æœ¬")
    
    def _setup_logger(self) -> logging.Logger:
        """é…ç½®æ—¥å¿—"""
        logger = logging.getLogger('RollingWindowTrainer')
        logger.setLevel(logging.INFO)
        
        if not logger.handlers:
            handler = logging.StreamHandler()
            formatter = logging.Formatter(
                '%(asctime)s - %(name)s - %(levelname)s - %(message)s',
                datefmt='%Y-%m-%d %H:%M:%S'
            )
            handler.setFormatter(formatter)
            logger.addHandler(handler)
        
        return logger
    
    def create_datasets_for_window(
        self,
        window_idx: int,
        val_ratio: float = 0.2
    ) -> Tuple[Any, Any, Any]:
        """
        ä¸ºæŒ‡å®šçª—å£åˆ›å»ºæ•°æ®é›†
        
        Args:
            window_idx: çª—å£ç´¢å¼•
            val_ratio: ä»è®­ç»ƒé›†ä¸­åˆ’åˆ†éªŒè¯é›†çš„æ¯”ä¾‹
            
        Returns:
            (train_dataset, val_dataset, test_dataset)
        """
        from torch.utils.data import Subset
        import numpy as np
        
        train_df, test_df = self.windows[window_idx]
        
        # ğŸ†• ä¼ é€’çª—å£å˜æ¢é…ç½®
        enable_wt = getattr(self.config, 'enable_window_transform', False)
        price_log = getattr(self.config, 'window_price_log', False)
        vol_norm = getattr(self.config, 'window_volume_norm', False)
        price_cols = getattr(self.config, 'price_cols', ['open', 'high', 'low', 'close', 'vwap'])
        close_col = getattr(self.config, 'close_col', 'close')
        volume_cols = getattr(self.config, 'volume_cols', ['vol', 'amount'])
        
        # ğŸ†• æ ‡ç­¾çª—å£çº§æ’åæ ‡å‡†åŒ–é…ç½®
        label_rank_norm = getattr(self.config, 'label_rank_normalize', False)
        label_rank_range = getattr(self.config, 'label_rank_output_range', (-1, 1))

        # ğŸ†• æŒ‰æ—¶é—´åˆ’åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›† (Time-Series Split)
        # é¿å…ä½¿ç”¨ Subset å¯¼è‡´çš„æŒ‰è‚¡ç¥¨IDåˆ’åˆ†é—®é¢˜
        
        all_dates = sorted(train_df[self.config.time_col].unique())
        n_dates = len(all_dates)
        
        # ç¡®ä¿æ•°æ®è¶³å¤Ÿé•¿ä»¥è¿›è¡Œåˆ’åˆ†
        if n_dates > self.config.window_size + 20 and val_ratio > 0:
            # è®¡ç®—åˆ†å‰²ç‚¹
            split_idx = int(n_dates * (1 - val_ratio))
            split_date = all_dates[split_idx]
            
            # 1. æ„å»ºè®­ç»ƒé›† DataFrame (split_date ä¹‹å‰)
            train_split_df = train_df[train_df[self.config.time_col] < split_date].copy()
            
            # 2. æ„å»ºéªŒè¯é›† DataFrame (åŒ…å«è¶³å¤Ÿçš„å›çœ‹çª—å£)
            # æˆ‘ä»¬éœ€è¦ split_date å¼€å§‹çš„é¢„æµ‹ï¼Œæ‰€ä»¥éœ€è¦å¾€å‰æ¨ window_size å¤©çš„æ•°æ®ä½œä¸ºç‰¹å¾
            lookback_idx = max(0, split_idx - self.config.window_size)
            lookback_date = all_dates[lookback_idx]
            val_split_df = train_df[train_df[self.config.time_col] >= lookback_date].copy()
            
            self.logger.info(f"  æ•°æ®é›†åˆ’åˆ† (Time-Series Split):")
            self.logger.info(f"    è®­ç»ƒé›†æˆªæ­¢: {split_date} (ä¸å«)")
            self.logger.info(f"    éªŒè¯é›†å¼€å§‹: {split_date} (é¢„æµ‹æ—¥æœŸ)")
            
            # åˆ›å»ºè®­ç»ƒæ•°æ®é›†
            train_dataset = TimeSeriesStockDataset(
                df=train_split_df,
                feature_cols=self.feature_cols,
                label_col=self.config.label_col,
                window_size=self.config.window_size,
                stock_col=self.config.stock_col,
                time_col=self.config.time_col,
                return_stock_id=True,
                enable_window_transform=enable_wt,
                window_price_log=price_log,
                window_volume_norm=vol_norm,
                price_cols=price_cols,
                close_col=close_col,
                volume_cols=volume_cols,
                label_rank_normalize=label_rank_norm,
                label_rank_output_range=label_rank_range,
                stock_map=self.stock_map
            )
            
            # åˆ›å»ºéªŒè¯æ•°æ®é›†
            val_dataset = TimeSeriesStockDataset(
                df=val_split_df,
                feature_cols=self.feature_cols,
                label_col=self.config.label_col,
                window_size=self.config.window_size,
                stock_col=self.config.stock_col,
                time_col=self.config.time_col,
                return_stock_id=True,
                enable_window_transform=enable_wt,
                window_price_log=price_log,
                window_volume_norm=vol_norm,
                price_cols=price_cols,
                close_col=close_col,
                volume_cols=volume_cols,
                label_rank_normalize=label_rank_norm,
                label_rank_output_range=label_rank_range,
                stock_map=self.stock_map
            )
            
        else:
            # æ•°æ®å¤ªå°‘ï¼Œä¸åˆ’åˆ†éªŒè¯é›†
            self.logger.warning(f"  è­¦å‘Š: æ•°æ®ä¸è¶³ä»¥è¿›è¡Œæ—¶é—´åºåˆ—åˆ’åˆ†ï¼Œä½¿ç”¨å…¨é‡è®­ç»ƒ")
            train_dataset = TimeSeriesStockDataset(
                df=train_df,
                feature_cols=self.feature_cols,
                label_col=self.config.label_col,
                window_size=self.config.window_size,
                stock_col=self.config.stock_col,
                time_col=self.config.time_col,
                return_stock_id=True,
                enable_window_transform=enable_wt,
                window_price_log=price_log,
                window_volume_norm=vol_norm,
                price_cols=price_cols,
                close_col=close_col,
                volume_cols=volume_cols,
                label_rank_normalize=label_rank_norm,
                label_rank_output_range=label_rank_range,
                stock_map=self.stock_map
            )
            val_dataset = None
        
        # åˆ›å»ºæµ‹è¯•æ•°æ®é›†
        # ğŸ”´ ä¿®å¤: æµ‹è¯•é›†éœ€è¦åŒ…å« lookback window çš„å†å²æ•°æ®
        # å¦åˆ™ TimeSeriesStockDataset ä¼šå› ä¸ºæ•°æ®é•¿åº¦ä¸è¶³è€Œä¸¢å¼ƒæ ·æœ¬
        
        # 1. è·å–æµ‹è¯•é›†çš„å¼€å§‹æ—¥æœŸ
        if not test_df.empty:
            test_start_date = test_df[self.config.time_col].min()
            test_end_date = test_df[self.config.time_col].max()
            
            # 2. ä»åŸå§‹æ•°æ®ä¸­è·å–åŒ…å« lookback çš„æ•°æ®
            # æ³¨æ„ï¼šè¿™é‡Œæˆ‘ä»¬éœ€è¦è®¿é—®åŸå§‹æ•°æ®ï¼Œä½† self.windows åªåŒ…å«åˆ‡åˆ†åçš„æ•°æ®
            # å¹¸å¥½ train_df é€šå¸¸åŒ…å« test_df ä¹‹å‰çš„æ•°æ®ï¼ˆå¦‚æœæ˜¯ rolling ä¸”æ—  gapï¼‰
            # æˆ–è€…æˆ‘ä»¬å¯ä»¥å‡è®¾ train_df çš„æœ«å°¾å°±æ˜¯ test_df çš„å¼€å§‹å‰ä¸€å¤©
            
            # æ›´ç¨³å¥çš„æ–¹æ³•ï¼šæˆ‘ä»¬éœ€è¦è®¿é—® DataManager çš„åŸå§‹æ•°æ®ï¼Œä½†è¿™é‡Œæ²¡æœ‰å¼•ç”¨
            # æ›¿ä»£æ–¹æ¡ˆï¼šåˆ©ç”¨ train_df çš„æœ«å°¾æ•°æ®ä½œä¸º lookback
            
            train_end_date = train_df[self.config.time_col].max()
            
            # æ£€æŸ¥ train_df æ˜¯å¦ç´§é‚» test_df
            # å¦‚æœ train_end_date < test_start_dateï¼Œè¯´æ˜å¯èƒ½æœ‰ gapï¼Œæˆ–è€… train_df å°±æ˜¯å†å²æ•°æ®
            
            # æå– train_df ä¸­æœ€å window_size * 2 å¤©çš„æ•°æ®
            lookback_start = test_start_date - pd.Timedelta(days=self.config.window_size * 2)
            lookback_df = train_df[train_df[self.config.time_col] >= lookback_start].copy()
            
            if not lookback_df.empty:
                # åˆå¹¶ lookback å’Œ test_df
                extended_test_df = pd.concat([lookback_df, test_df], ignore_index=True)
                # å»é‡
                extended_test_df = extended_test_df.drop_duplicates(subset=[self.config.stock_col, self.config.time_col])
                # æ’åº
                extended_test_df = extended_test_df.sort_values([self.config.stock_col, self.config.time_col])
                
                # ä½¿ç”¨æ‰©å±•åçš„æ•°æ®åˆ›å»ºæµ‹è¯•é›†
                test_dataset_df = extended_test_df
                valid_label_start = pd.Timestamp(test_start_date)
            else:
                test_dataset_df = test_df
                valid_label_start = None
        else:
            test_dataset_df = test_df
            valid_label_start = None

        test_dataset = TimeSeriesStockDataset(
            df=test_dataset_df,
            feature_cols=self.feature_cols,
            label_col=self.config.label_col,
            window_size=self.config.window_size,
            stock_col=self.config.stock_col,
            time_col=self.config.time_col,
            # ğŸ†• å¯ç”¨è¿”å›è‚¡ç¥¨ID
            return_stock_id=True,
            # ğŸ†• ä¼ é€’çª—å£å˜æ¢å‚æ•°
            enable_window_transform=enable_wt,
            window_price_log=price_log,
            window_volume_norm=vol_norm,
            price_cols=price_cols,
            close_col=close_col,
            volume_cols=volume_cols,
            # ğŸ†• æ ‡ç­¾çª—å£çº§æ’åæ ‡å‡†åŒ–
            label_rank_normalize=label_rank_norm,
            label_rank_output_range=label_rank_range,
            # ğŸ†• ä¼ é€’å…¨å±€è‚¡ç¥¨æ˜ å°„
            stock_map=self.stock_map,
            # ğŸ†• ä¼ é€’æœ‰æ•ˆæ ‡ç­¾èµ·å§‹æ—¥æœŸ
            valid_label_start_date=valid_label_start
        )
        
        return train_dataset, val_dataset, test_dataset
    
    def train_window(
        self,
        window_idx: int,
        model_class: type,
        model_config: Any,
        save_path: Optional[str] = None,
        val_ratio: float = 0.2,
        init_model_path: Optional[str] = None,
        use_cross_sectional: bool = False
    ) -> Dict[str, Any]:
        """
        è®­ç»ƒå•ä¸ªçª—å£
        
        Args:
            window_idx: çª—å£ç´¢å¼•
            model_class: æ¨¡å‹ç±»ï¼ˆå¦‚ GRUModelï¼‰
            model_config: æ¨¡å‹é…ç½®å¯¹è±¡
            save_path: æ¨¡å‹ä¿å­˜è·¯å¾„ï¼ˆå¯é€‰ï¼‰
            val_ratio: éªŒè¯é›†æ¯”ä¾‹
            init_model_path: åˆå§‹åŒ–æ¨¡å‹è·¯å¾„ï¼ˆå¢é‡è®­ç»ƒï¼‰
            use_cross_sectional: æ˜¯å¦ä½¿ç”¨æˆªé¢æ‰¹é‡‡æ ·ï¼ˆæŒ‰æ—¥æœŸç»„ç»‡Batchï¼‰
            
        Returns:
            è®­ç»ƒç»“æœå­—å…¸ï¼ŒåŒ…å«ï¼š
            - window_idx: çª—å£ç´¢å¼•
            - model: è®­ç»ƒå¥½çš„æ¨¡å‹
            - train_loss: è®­ç»ƒæŸå¤±
            - val_loss: éªŒè¯æŸå¤±
            - best_epoch: æœ€ä½³è½®æ•°
            - save_path: æ¨¡å‹ä¿å­˜è·¯å¾„
        """
        self.logger.info("=" * 80)
        self.logger.info(f"ğŸ”„ è®­ç»ƒçª—å£ {window_idx + 1}/{self.n_windows}")
        self.logger.info("=" * 80)
        
        # åˆ›å»ºæ•°æ®é›†
        train_dataset, val_dataset, test_dataset = self.create_datasets_for_window(
            window_idx, val_ratio
        )
        
        self.logger.info(f"  è®­ç»ƒæ ·æœ¬: {len(train_dataset):,}")
        if val_dataset:
            self.logger.info(f"  éªŒè¯æ ·æœ¬: {len(val_dataset):,}")
        self.logger.info(f"  æµ‹è¯•æ ·æœ¬: {len(test_dataset):,}")
        
        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        if use_cross_sectional:
            self.logger.info("  é‡‡æ ·ç­–ç•¥: æˆªé¢æ‰¹é‡‡æ · (Cross-Sectional Batch)")
            self.logger.info("    - æ¯ä¸ª Batch åŒ…å«åŒä¸€äº¤æ˜“æ—¥çš„è‚¡ç¥¨")
            self.logger.info("    - æ—¥æœŸé¡ºåºéšæœºæ‰“ä¹±")
            
            # ä½¿ç”¨æˆªé¢æ‰¹é‡‡æ ·å™¨
            train_sampler = CrossSectionalBatchSampler(
                train_dataset, 
                batch_size=self.config.batch_size, 
                shuffle_dates=True
            )
            train_loader = DataLoader(
                train_dataset,
                batch_sampler=train_sampler,
                num_workers=0
            )
        else:
            self.logger.info("  é‡‡æ ·ç­–ç•¥: å…¨å±€éšæœºæ‰“ä¹± (Global Shuffle)")
            
            train_loader = DataLoader(
                train_dataset,
                batch_size=self.config.batch_size,
                shuffle=True,
                num_workers=0,
                drop_last=False
            )
        
        val_loader = None
        if val_dataset and len(val_dataset) > 0:
            val_loader = DataLoader(
                val_dataset,
                batch_size=self.config.batch_size,
                shuffle=False,
                num_workers=0,
                drop_last=False
            )
        
        # åˆ›å»ºæ¨¡å‹
        # ä¼˜å…ˆä½¿ç”¨ from_config ç±»æ–¹æ³•ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        if hasattr(model_class, 'from_config'):
            model = model_class.from_config(model_config, d_feat=len(self.feature_cols))
        else:
            # å…¼å®¹æ—§çš„åˆå§‹åŒ–æ–¹å¼
            model = model_class(
                d_feat=len(self.feature_cols),
                hidden_size=model_config.hidden_size,
                num_layers=model_config.num_layers,
                dropout=model_config.dropout,
                weight_decay=getattr(model_config, 'weight_decay', 0.0001),
                n_epochs=model_config.n_epochs,
                batch_size=model_config.batch_size,
                lr=model_config.learning_rate,
                early_stop=model_config.early_stop,
                optimizer=model_config.optimizer,
                device=model_config.device
            )
        
        # å¦‚æœæä¾›äº†åˆå§‹åŒ–æ¨¡å‹ï¼ŒåŠ è½½æƒé‡ï¼ˆå¢é‡è®­ç»ƒï¼‰
        if init_model_path and Path(init_model_path).exists():
            self.logger.info(f"  åŠ è½½åˆå§‹åŒ–æ¨¡å‹: {init_model_path}")
            model.load_model(init_model_path)
        
        # è®­ç»ƒæ¨¡å‹
        self.logger.info(f"  å¼€å§‹è®­ç»ƒ...")
        model.fit(train_loader, val_loader, save_path=save_path)
        
        result = {
            'window_idx': window_idx,
            'model': model,
            'train_loss': model.train_losses[-1] if model.train_losses else None,
            'val_loss': model.valid_losses[-1] if model.valid_losses else None,
            'best_epoch': model.best_epoch,
            'best_score': model.best_score,
            'save_path': save_path,
            'test_dataset': test_dataset
        }
        
        self.logger.info(f"âœ… çª—å£ {window_idx + 1} è®­ç»ƒå®Œæˆ")
        self.logger.info(f"  æœ€ä½³Epoch: {model.best_epoch + 1}")
        self.logger.info(f"  æœ€ä½³å¾—åˆ†: {model.best_score:.6f}")
        
        return result
    
    def train_all_windows(
        self,
        model_class: type,
        model_config: Any,
        save_dir: Optional[str] = None,
        val_ratio: float = 0.2,
        incremental: bool = False,
        # ğŸ†• åŠ¨æ€é‚»æ¥çŸ©é˜µå‚æ•°
        dynamic_adj: bool = False,
        adj_config: Optional[Dict] = None,
        # ğŸ†• é‡‡æ ·ç­–ç•¥å‚æ•°
        use_cross_sectional: bool = False
    ) -> List[Dict[str, Any]]:
        """
        è®­ç»ƒæ‰€æœ‰æ»šåŠ¨çª—å£
        
        Args:
            model_class: æ¨¡å‹ç±»
            model_config: æ¨¡å‹é…ç½®
            save_dir: æ¨¡å‹ä¿å­˜ç›®å½•ï¼ˆå¯é€‰ï¼‰
            val_ratio: éªŒè¯é›†æ¯”ä¾‹
            incremental: æ˜¯å¦ä½¿ç”¨å¢é‡è®­ç»ƒï¼ˆæ¯ä¸ªçª—å£ç”¨å‰ä¸€çª—å£æ¨¡å‹åˆå§‹åŒ–ï¼‰
            dynamic_adj: æ˜¯å¦ä¸ºæ¯ä¸ªçª—å£åŠ¨æ€æ„å»ºé‚»æ¥çŸ©é˜µ
            adj_config: é‚»æ¥çŸ©é˜µæ„å»ºé…ç½® (dict)
            use_cross_sectional: æ˜¯å¦ä½¿ç”¨æˆªé¢æ‰¹é‡‡æ ·ï¼ˆæŒ‰æ—¥æœŸç»„ç»‡Batchï¼‰
            
        Returns:
            æ‰€æœ‰çª—å£çš„è®­ç»ƒç»“æœåˆ—è¡¨
        """
        self.logger.info("\n" + "=" * 80)
        self.logger.info("ğŸš€ å¼€å§‹æ»šåŠ¨çª—å£è®­ç»ƒ")
        self.logger.info("=" * 80)
        self.logger.info(f"  è®­ç»ƒç­–ç•¥: {'å¢é‡è®­ç»ƒ' if incremental else 'ç‹¬ç«‹è®­ç»ƒ'}")
        self.logger.info(f"  é‡‡æ ·ç­–ç•¥: {'æˆªé¢æ‰¹é‡‡æ · (æŒ‰æ—¥æœŸ)' if use_cross_sectional else 'å…¨å±€éšæœºæ‰“ä¹±'}")
        self.logger.info(f"  æ€»çª—å£æ•°: {self.n_windows}")
        
        if dynamic_adj:
            self.logger.info(f"  åŠ¨æ€å›¾æ„å»º: å·²å¯ç”¨ (æ¯å¹´é‡æ–°è®¡ç®—é‚»æ¥çŸ©é˜µ)")
        
        if save_dir:
            save_dir = Path(save_dir)
            save_dir.mkdir(parents=True, exist_ok=True)
            self.logger.info(f"  ä¿å­˜ç›®å½•: {save_dir}")
        
        results = []
        prev_model_path = None
        
        for i in range(self.n_windows):
            # ç”Ÿæˆä¿å­˜è·¯å¾„
            save_path = None
            if save_dir:
                save_path = str(save_dir / f'window_{i+1}_model.pth')
            
            # ğŸ†• åŠ¨æ€æ„å»ºé‚»æ¥çŸ©é˜µ
            if dynamic_adj and adj_config:
                self.logger.info(f"  ğŸ”„ æ­£åœ¨ä¸ºçª—å£ {i+1} æ„å»ºåŠ¨æ€é‚»æ¥çŸ©é˜µ...")
                train_df, _ = self.windows[i]
                
                # æ„å»ºçŸ©é˜µ
                adj_matrix = self._build_adj_matrix(train_df, adj_config)
                
                # ä¿å­˜çŸ©é˜µ
                if save_dir:
                    adj_path = str(save_dir / f'adj_window_{i+1}.pt')
                    torch.save(adj_matrix, adj_path)
                    # æ›´æ–°é…ç½®
                    model_config.adj_matrix_path = adj_path
                    self.logger.info(f"     å·²ä¿å­˜å¹¶åº”ç”¨: {adj_path}")
            
            # è®­ç»ƒçª—å£
            result = self.train_window(
                window_idx=i,
                model_class=model_class,
                model_config=model_config,
                save_path=save_path,
                val_ratio=val_ratio,
                init_model_path=prev_model_path if incremental else None,
                use_cross_sectional=use_cross_sectional
            )
            
            results.append(result)
            
            # æ›´æ–°å‰ä¸€æ¨¡å‹è·¯å¾„ï¼ˆç”¨äºå¢é‡è®­ç»ƒï¼‰
            if incremental and save_path:
                prev_model_path = save_path
        
        self.window_results = results
        
        # æ±‡æ€»ç»Ÿè®¡
        self.logger.info("\n" + "=" * 80)
        self.logger.info("ğŸ“Š æ»šåŠ¨çª—å£è®­ç»ƒæ±‡æ€»")
        self.logger.info("=" * 80)
        
        train_losses = [r['train_loss'] for r in results if r['train_loss'] is not None]
        val_losses = [r['val_loss'] for r in results if r['val_loss'] is not None]
        best_epochs = [r['best_epoch'] for r in results]
        
        if train_losses:
            self.logger.info(f"  å¹³å‡è®­ç»ƒæŸå¤±: {np.mean(train_losses):.6f}")
        if val_losses:
            self.logger.info(f"  å¹³å‡éªŒè¯æŸå¤±: {np.mean(val_losses):.6f}")
        self.logger.info(f"  å¹³å‡æœ€ä½³Epoch: {np.mean(best_epochs):.1f}")
        
        self.logger.info("\nâœ… æ‰€æœ‰çª—å£è®­ç»ƒå®Œæˆï¼")
        
        return results

    def _build_adj_matrix(self, df: pd.DataFrame, config: Dict) -> torch.Tensor:
        """
        æ„å»ºé‚»æ¥çŸ©é˜µ (å†…éƒ¨æ–¹æ³•)
        
        å·²æ›´æ–°ï¼šä½¿ç”¨æ–°çš„ AdjMatrixUtilsï¼ˆæ¥è‡ª graph_builder.pyï¼‰ä»£æ›¿æ—§çš„ AdjMatrixBuilder
        """
        from quantclassic.data_processor.graph_builder import AdjMatrixUtils
        
        # 1. å‡†å¤‡æ”¶ç›Šç‡æ•°æ®
        # ä½¿ç”¨é…ç½®ä¸­çš„åˆ—åï¼Œé»˜è®¤ä¸º 'y_ret_10d'
        ret_col = config.get('return_col', 'y_ret_10d')
        
        # Pivot table
        returns_pivot = df.pivot_table(
            index=self.config.time_col,
            columns=self.config.stock_col,
            values=ret_col,
            aggfunc='first'
        )
        
        # å¡«å……ç¼ºå¤±å€¼
        returns_pivot = returns_pivot.ffill().bfill().fillna(0)
        
        # 2. å¦‚æœæœ‰å…¨å±€æ˜ å°„ï¼Œéœ€è¦å¯¹é½åˆ—
        if self.stock_map:
            # åˆ›å»ºä¸€ä¸ªåŒ…å«æ‰€æœ‰å…¨å±€è‚¡ç¥¨çš„DataFrame
            full_returns = pd.DataFrame(
                0.0, 
                index=returns_pivot.index, 
                columns=sorted(self.stock_map.keys()) # ç¡®ä¿æŒ‰å­—æ¯é¡ºåºæ’åºï¼Œä¸stock_mapä¸€è‡´
            )
            # æ›´æ–°æœ‰æ•°æ®çš„éƒ¨åˆ†
            common_cols = returns_pivot.columns.intersection(full_returns.columns)
            full_returns[common_cols] = returns_pivot[common_cols]
            returns_pivot = full_returns
        
        # 3. æ„å»ºçŸ©é˜µï¼ˆä½¿ç”¨æ–°çš„ AdjMatrixUtilsï¼‰
        adj = AdjMatrixUtils.build_correlation_adj(
            returns=returns_pivot,
            top_k=config.get('top_k', 10),
            method=config.get('method', 'pearson'),
            self_loop=True
        )
        
        return adj
    
    def predict_window(
        self,
        window_result: Dict[str, Any]
    ) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:
        """
        å¯¹å•ä¸ªçª—å£è¿›è¡Œé¢„æµ‹
        
        Args:
            window_result: çª—å£è®­ç»ƒç»“æœ
            
        Returns:
            (predictions, labels, stocks, dates)
        """
        model = window_result['model']
        test_dataset = window_result['test_dataset']
        
        # åˆ›å»ºæµ‹è¯•æ•°æ®åŠ è½½å™¨
        test_loader = DataLoader(
            test_dataset,
            batch_size=self.config.batch_size,
            shuffle=False,
            num_workers=0,
            drop_last=False
        )
        
        # é¢„æµ‹
        predictions = model.predict(test_loader, return_numpy=True)
        
        # ã€ä¿®å¤ã€‘å¤„ç†ç©ºæµ‹è¯•é›†çš„æƒ…å†µ
        if len(test_dataset) == 0:
            self.logger.warning(f"  è­¦å‘Š: æµ‹è¯•é›†ä¸ºç©ºï¼Œè·³è¿‡é¢„æµ‹")
            return (
                np.array([]),  # predictions
                np.array([]),  # labels
                np.array([]),  # stocks
                None           # dates
            )
        
        # ã€ä¿®å¤ã€‘ä»TimeSeriesStockDatasetä¸­æå–æ ‡ç­¾å’Œå…ƒæ•°æ®
        # TimeSeriesStockDatasetå°†æ•°æ®å­˜å‚¨åœ¨sample_indexå’Œstock_dataä¸­
        labels = []
        stocks = []
        dates = []
        
        for idx in range(len(test_dataset)):
            stock_idx, time_idx = test_dataset.sample_index[idx]
            stock_info = test_dataset.stock_data[stock_idx]
            
            # æ ‡ç­¾æ˜¯t+1æ—¶åˆ»çš„å€¼
            labels.append(stock_info['labels'][time_idx + 1])
            stocks.append(stock_info['ts_code'])
            
            # å¦‚æœæœ‰æ—¥æœŸä¿¡æ¯ï¼Œä¹Ÿæå–å‡ºæ¥
            if 'dates' in stock_info:
                dates.append(stock_info['dates'][time_idx + 1])
            else:
                dates.append(None)
        
        labels = np.array(labels)
        stocks = np.array(stocks)
        dates = np.array(dates) if dates and dates[0] is not None else None
        
        return predictions, labels, stocks, dates
    
    def predict_all_windows(
        self,
        window_results: Optional[List[Dict[str, Any]]] = None
    ) -> pd.DataFrame:
        """
        å¯¹æ‰€æœ‰çª—å£è¿›è¡Œé¢„æµ‹å¹¶åˆå¹¶ç»“æœ
        
        ğŸ†• æ”¯æŒå¤šå› å­è¾“å‡º (NÃ—F çŸ©é˜µ)
        
        Args:
            window_results: çª—å£è®­ç»ƒç»“æœï¼ˆå¯é€‰ï¼Œé»˜è®¤ä½¿ç”¨self.window_resultsï¼‰
            
        Returns:
            åˆå¹¶çš„é¢„æµ‹ç»“æœDataFrameï¼ŒåŒ…å«åˆ—ï¼š
            - stock_col: è‚¡ç¥¨ä»£ç 
            - time_col: æ—¥æœŸ
            - pred_alpha: é¢„æµ‹å€¼ (å•å› å­) æˆ– pred_factor_0, pred_factor_1, ... (å¤šå› å­)
            - label_col: çœŸå®æ ‡ç­¾
            - window_idx: çª—å£ç´¢å¼•
        """
        if window_results is None:
            window_results = self.window_results
        
        if not window_results:
            raise ValueError("æ²¡æœ‰å¯ç”¨çš„çª—å£ç»“æœï¼Œè¯·å…ˆè¿è¡Œ train_all_windows()")
        
        self.logger.info("\n" + "=" * 80)
        self.logger.info("ğŸ”® å¼€å§‹æ»šåŠ¨çª—å£é¢„æµ‹")
        self.logger.info("=" * 80)
        
        all_predictions = []
        
        for i, result in enumerate(window_results):
            self.logger.info(f"  é¢„æµ‹çª—å£ {i + 1}/{len(window_results)}...")
            
            predictions, labels, stocks, dates = self.predict_window(result)
            
            # ã€ä¿®å¤ã€‘è·³è¿‡ç©ºé¢„æµ‹çª—å£
            if len(predictions) == 0:
                self.logger.warning(f"    çª—å£ {i + 1} é¢„æµ‹ä¸ºç©ºï¼Œè·³è¿‡")
                continue
            
            # ğŸ†• å¤„ç†å¤šå› å­è¾“å‡º (NÃ—F çŸ©é˜µ)
            if predictions.ndim == 2 and predictions.shape[1] > 1:
                # å¤šå› å­è¾“å‡º
                n_factors = predictions.shape[1]
                self.logger.info(f"    æ£€æµ‹åˆ°å¤šå› å­è¾“å‡º: F = {n_factors}")
                
                # åˆ›å»ºDataFrameï¼Œæ¯ä¸ªå› å­ä¸€åˆ—
                window_df = pd.DataFrame({
                    self.config.stock_col: stocks,
                    self.config.time_col: dates,
                    self.config.label_col: labels,
                    'window_idx': i + 1
                })
                
                # æ·»åŠ æ¯ä¸ªå› å­åˆ—
                for f_idx in range(n_factors):
                    window_df[f'pred_factor_{f_idx}'] = predictions[:, f_idx]
                
                # åŒæ—¶æ·»åŠ ç®€å•å¹³å‡ä½œä¸ºé»˜è®¤é¢„æµ‹åˆ—
                window_df['pred_alpha'] = predictions.mean(axis=1)
                
            else:
                # å•å› å­è¾“å‡º
                window_df = pd.DataFrame({
                    self.config.stock_col: stocks,
                    self.config.time_col: dates,
                    'pred_alpha': predictions.flatten(),
                    self.config.label_col: labels,
                    'window_idx': i + 1
                })
            
            all_predictions.append(window_df)
            
            self.logger.info(f"    é¢„æµ‹æ ·æœ¬: {len(window_df):,}")
        
        # ã€ä¿®å¤ã€‘å¤„ç†æ— æœ‰æ•ˆé¢„æµ‹çš„æƒ…å†µ
        if not all_predictions:
            self.logger.warning("\nâš ï¸  æ‰€æœ‰çª—å£çš„é¢„æµ‹éƒ½ä¸ºç©ºï¼")
            self.logger.warning("  è¿™é€šå¸¸æ„å‘³ç€æµ‹è¯•æ•°æ®ä¸è¶³ä»¥åˆ›å»ºæœ‰æ•ˆæ ·æœ¬")
            # è¿”å›ç©ºDataFrameä½†ä¿æŒç»“æ„
            return pd.DataFrame(columns=[
                self.config.stock_col,
                self.config.time_col,
                'pred_alpha',
                self.config.label_col,
                'window_idx'
            ])
        
        # åˆå¹¶æ‰€æœ‰é¢„æµ‹
        combined_predictions = pd.concat(all_predictions, ignore_index=True)
        
        self.logger.info("\nâœ… é¢„æµ‹å®Œæˆï¼")
        self.logger.info(f"  æ€»é¢„æµ‹æ ·æœ¬: {len(combined_predictions):,}")
        self.logger.info(f"  æ—¶é—´èŒƒå›´: {combined_predictions[self.config.time_col].min()} ~ {combined_predictions[self.config.time_col].max()}")
        self.logger.info(f"  è‚¡ç¥¨æ•°é‡: {combined_predictions[self.config.stock_col].nunique()}")
        
        # ğŸ†• æ˜¾ç¤ºå› å­åˆ—ä¿¡æ¯
        factor_cols = [c for c in combined_predictions.columns if c.startswith('pred_factor_')]
        if factor_cols:
            self.logger.info(f"  å¤šå› å­è¾“å‡º: {len(factor_cols)} ä¸ªå› å­ ({factor_cols[0]}, ..., {factor_cols[-1]})")
        
        return combined_predictions
    
    def get_summary(self) -> Dict[str, Any]:
        """
        è·å–æ»šåŠ¨çª—å£è®­ç»ƒå’Œé¢„æµ‹çš„æ±‡æ€»ç»Ÿè®¡
        
        Returns:
            æ±‡æ€»ç»Ÿè®¡å­—å…¸
        """
        if not self.window_results:
            return {}
        
        train_losses = [r['train_loss'] for r in self.window_results if r['train_loss'] is not None]
        val_losses = [r['val_loss'] for r in self.window_results if r['val_loss'] is not None]
        best_epochs = [r['best_epoch'] for r in self.window_results]
        best_scores = [r['best_score'] for r in self.window_results if r['best_score'] is not None]
        
        summary = {
            'n_windows': self.n_windows,
            'avg_train_loss': float(np.mean(train_losses)) if train_losses else None,
            'avg_val_loss': float(np.mean(val_losses)) if val_losses else None,
            'avg_best_epoch': float(np.mean(best_epochs)),
            'avg_best_score': float(np.mean(best_scores)) if best_scores else None,
            'std_train_loss': float(np.std(train_losses)) if train_losses else None,
            'std_val_loss': float(np.std(val_losses)) if val_losses else None,
        }
        
        return summary


if __name__ == '__main__':
    print("=" * 80)
    print("Rolling Window Trainer æµ‹è¯•")
    print("=" * 80)
    
    print("\nâœ… RollingWindowTrainer å®šä¹‰å®Œæˆ")
    print("\nåŠŸèƒ½:")
    print("  - æ”¯æŒå®Œå…¨ç‹¬ç«‹çš„æ»šåŠ¨çª—å£è®­ç»ƒ")
    print("  - æ”¯æŒå¢é‡è®­ç»ƒï¼ˆä½¿ç”¨å‰ä¸€çª—å£æ¨¡å‹åˆå§‹åŒ–ï¼‰")
    print("  - è‡ªåŠ¨ç®¡ç†æ¨¡å‹ä¿å­˜å’ŒåŠ è½½")
    print("  - æä¾›è¯¦ç»†çš„è®­ç»ƒå’Œé¢„æµ‹æ—¥å¿—")
    print("  - åˆå¹¶æ‰€æœ‰çª—å£çš„é¢„æµ‹ç»“æœ")
    
    print("\næ»šåŠ¨çª—å£è®­ç»ƒå™¨å·²å‡†å¤‡å°±ç»ªï¼")
