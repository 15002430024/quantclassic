"""
DataLoader - æ•°æ®åŠ è½½å¼•æ“

æ”¯æŒå¤šç§æ•°æ®æ ¼å¼çš„åŠ è½½ã€éªŒè¯å’Œå†…å­˜ä¼˜åŒ–
"""

import os
import pandas as pd
import numpy as np
from typing import Optional, Dict, Any, List
from pathlib import Path
import logging
from .config import DataConfig


class DataLoaderEngine:
    """æ•°æ®åŠ è½½å¼•æ“ - æ”¯æŒå¤šç§æ ¼å¼å’Œä¼˜åŒ–ç­–ç•¥"""
    
    def __init__(self, config: DataConfig):
        """
        Args:
            config: DataConfigé…ç½®å¯¹è±¡
        """
        self.config = config
        self.logger = self._setup_logger()
        self._data_cache: Optional[pd.DataFrame] = None
    
    def _setup_logger(self) -> logging.Logger:
        """é…ç½®æ—¥å¿—"""
        logger = logging.getLogger('DataLoader')
        logger.setLevel(getattr(logging, self.config.log_level))
        
        if not logger.handlers:
            handler = logging.StreamHandler()
            formatter = logging.Formatter(
                '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
            )
            handler.setFormatter(formatter)
            logger.addHandler(handler)
        
        return logger
    
    def load_data(self, file_path: Optional[str] = None, 
                  use_cache: bool = True) -> pd.DataFrame:
        """
        åŠ è½½æ•°æ®ä¸»æ–¹æ³•
        
        Args:
            file_path: æ•°æ®æ–‡ä»¶è·¯å¾„ï¼ˆNoneåˆ™ä½¿ç”¨configä¸­çš„è·¯å¾„ï¼‰
            use_cache: æ˜¯å¦ä½¿ç”¨ç¼“å­˜
            
        Returns:
            åŠ è½½çš„DataFrame
        """
        # ä½¿ç”¨ç¼“å­˜
        if use_cache and self._data_cache is not None:
            self.logger.info("ğŸ“¦ ä½¿ç”¨ç¼“å­˜æ•°æ®")
            return self._data_cache.copy()
        
        # ç¡®å®šæ–‡ä»¶è·¯å¾„
        if file_path is None:
            file_path = self.config.data_path
        
        if not os.path.exists(file_path):
            raise FileNotFoundError(f"æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
        
        self.logger.info(f"ğŸ“ åŠ è½½æ•°æ®: {file_path}")
        
        # æ ¹æ®æ ¼å¼é€‰æ‹©åŠ è½½æ–¹æ³•
        file_ext = Path(file_path).suffix.lower()
        
        if file_ext == '.parquet':
            df = self._load_parquet(file_path)
        elif file_ext == '.csv':
            df = self._load_csv(file_path)
        elif file_ext in ['.h5', '.hdf5']:
            df = self._load_hdf5(file_path)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {file_ext}")
        
        # æ•°æ®ç±»å‹ä¼˜åŒ–
        if self.config.use_dtype_optimization:
            df = self._optimize_dtypes(df)
        
        # åŸºç¡€éªŒè¯
        self._validate_data(df)
        
        # ç¼“å­˜æ•°æ®
        if use_cache:
            self._data_cache = df.copy()
        
        self.logger.info(f"âœ… æ•°æ®åŠ è½½å®Œæˆ: {len(df):,} è¡Œ, {len(df.columns)} åˆ—")
        
        return df
    
    def _load_parquet(self, file_path: str) -> pd.DataFrame:
        """åŠ è½½Parquetæ–‡ä»¶"""
        if self.config.chunk_size:
            # åˆ†å—åŠ è½½
            chunks = []
            for chunk in pd.read_parquet(file_path, chunksize=self.config.chunk_size):
                chunks.append(chunk)
            df = pd.concat(chunks, ignore_index=True)
        else:
            df = pd.read_parquet(file_path)
        
        return df
    
    def _load_csv(self, file_path: str) -> pd.DataFrame:
        """åŠ è½½CSVæ–‡ä»¶"""
        if self.config.chunk_size:
            chunks = []
            for chunk in pd.read_csv(file_path, chunksize=self.config.chunk_size):
                chunks.append(chunk)
            df = pd.concat(chunks, ignore_index=True)
        else:
            df = pd.read_csv(file_path)
        
        return df
    
    def _load_hdf5(self, file_path: str) -> pd.DataFrame:
        """åŠ è½½HDF5æ–‡ä»¶"""
        df = pd.read_hdf(file_path)
        return df
    
    def _optimize_dtypes(self, df: pd.DataFrame) -> pd.DataFrame:
        """ä¼˜åŒ–æ•°æ®ç±»å‹ä»¥èŠ‚çœå†…å­˜"""
        self.logger.info("ğŸ”§ ä¼˜åŒ–æ•°æ®ç±»å‹...")
        
        original_memory = df.memory_usage(deep=True).sum() / 1024**2
        
        for col in df.columns:
            col_type = df[col].dtype
            
            # æ•°å€¼ç±»å‹è½¬æ¢ä¸ºfloat32
            if col_type == 'float64':
                df[col] = df[col].astype('float32')
            
            # æ•´æ•°ç±»å‹ä¼˜åŒ–
            elif col_type == 'int64':
                col_min = df[col].min()
                col_max = df[col].max()
                
                if col_min >= 0:
                    if col_max < 255:
                        df[col] = df[col].astype('uint8')
                    elif col_max < 65535:
                        df[col] = df[col].astype('uint16')
                    elif col_max < 4294967295:
                        df[col] = df[col].astype('uint32')
                else:
                    if col_min > np.iinfo(np.int8).min and col_max < np.iinfo(np.int8).max:
                        df[col] = df[col].astype('int8')
                    elif col_min > np.iinfo(np.int16).min and col_max < np.iinfo(np.int16).max:
                        df[col] = df[col].astype('int16')
                    elif col_min > np.iinfo(np.int32).min and col_max < np.iinfo(np.int32).max:
                        df[col] = df[col].astype('int32')
            
            # å­—ç¬¦ä¸²ç±»å‹è½¬æ¢ä¸ºcategory
            elif col_type == 'object':
                num_unique = df[col].nunique()
                num_total = len(df[col])
                if num_unique / num_total < 0.5:  # å¦‚æœå”¯ä¸€å€¼å°‘äº50%
                    df[col] = df[col].astype('category')
        
        optimized_memory = df.memory_usage(deep=True).sum() / 1024**2
        reduction = (1 - optimized_memory / original_memory) * 100
        
        self.logger.info(f"   å†…å­˜ä¼˜åŒ–: {original_memory:.2f}MB â†’ {optimized_memory:.2f}MB "
                        f"(å‡å°‘ {reduction:.1f}%)")
        
        return df
    
    def _validate_data(self, df: pd.DataFrame):
        """åŸºç¡€æ•°æ®éªŒè¯"""
        # æ£€æŸ¥æ˜¯å¦ä¸ºç©º
        if df.empty:
            raise ValueError("æ•°æ®ä¸ºç©º")
        
        # æ£€æŸ¥å¿…éœ€åˆ—
        required_cols = [self.config.stock_col, self.config.time_col, self.config.label_col]
        missing_cols = [col for col in required_cols if col not in df.columns]
        
        if missing_cols:
            raise ValueError(f"ç¼ºå°‘å¿…éœ€åˆ—: {missing_cols}")
        
        # æ£€æŸ¥ç¼ºå¤±å€¼
        na_counts = df.isnull().sum()
        na_ratios = na_counts / len(df)
        
        high_na_cols = na_ratios[na_ratios > self.config.max_na_ratio].index.tolist()
        if high_na_cols:
            self.logger.warning(f"âš ï¸  ä»¥ä¸‹åˆ—ç¼ºå¤±å€¼è¶…è¿‡{self.config.max_na_ratio*100}%: {high_na_cols}")
    
    def get_data_info(self, df: pd.DataFrame) -> Dict[str, Any]:
        """è·å–æ•°æ®ä¿¡æ¯æ‘˜è¦"""
        info = {
            'shape': df.shape,
            'memory_usage_mb': df.memory_usage(deep=True).sum() / 1024**2,
            'num_stocks': df[self.config.stock_col].nunique() if self.config.stock_col in df.columns else 0,
            'date_range': (
                df[self.config.time_col].min(), 
                df[self.config.time_col].max()
            ) if self.config.time_col in df.columns else None,
            'missing_values': df.isnull().sum().to_dict(),
            'dtypes': df.dtypes.value_counts().to_dict(),
        }
        
        return info
    
    def print_data_summary(self, df: pd.DataFrame):
        """æ‰“å°æ•°æ®æ‘˜è¦"""
        info = self.get_data_info(df)
        
        print("\n" + "=" * 80)
        print("ğŸ“Š æ•°æ®æ‘˜è¦")
        print("=" * 80)
        print(f"å½¢çŠ¶: {info['shape'][0]:,} è¡Œ Ã— {info['shape'][1]} åˆ—")
        print(f"å†…å­˜å ç”¨: {info['memory_usage_mb']:.2f} MB")
        
        if info['num_stocks'] > 0:
            print(f"è‚¡ç¥¨æ•°é‡: {info['num_stocks']:,}")
        
        if info['date_range']:
            print(f"æ—¶é—´èŒƒå›´: {info['date_range'][0]} ~ {info['date_range'][1]}")
        
        print(f"\næ•°æ®ç±»å‹åˆ†å¸ƒ:")
        for dtype, count in info['dtypes'].items():
            print(f"  {dtype}: {count} åˆ—")
        
        # ç¼ºå¤±å€¼ç»Ÿè®¡
        missing = {k: v for k, v in info['missing_values'].items() if v > 0}
        if missing:
            print(f"\nç¼ºå¤±å€¼ (å‰10åˆ—):")
            for col, count in list(missing.items())[:10]:
                ratio = count / info['shape'][0] * 100
                print(f"  {col}: {count} ({ratio:.2f}%)")
        
        print("=" * 80)
    
    def clear_cache(self):
        """æ¸…é™¤ç¼“å­˜"""
        self._data_cache = None
        self.logger.info("ğŸ—‘ï¸  ç¼“å­˜å·²æ¸…é™¤")


if __name__ == '__main__':
    # æµ‹è¯•æ•°æ®åŠ è½½å™¨
    from config import DataConfig
    
    print("=" * 80)
    print("DataLoader æµ‹è¯•")
    print("=" * 80)
    
    # åˆ›å»ºé…ç½®
    config = DataConfig(
        base_dir='rq_data_parquet',
        data_file='train_data_final.parquet',
        use_dtype_optimization=True
    )
    
    # åˆ›å»ºåŠ è½½å™¨
    loader = DataLoaderEngine(config)
    
    # åŠ è½½æ•°æ®
    try:
        df = loader.load_data()
        loader.print_data_summary(df)
        
        print("\nâœ… æ•°æ®åŠ è½½å™¨æµ‹è¯•å®Œæˆ")
    except FileNotFoundError:
        print("\nâš ï¸  æµ‹è¯•æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè·³è¿‡å®é™…åŠ è½½æµ‹è¯•")
        print("âœ… æ•°æ®åŠ è½½å™¨ç±»å®šä¹‰å®Œæˆ")
