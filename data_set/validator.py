"""
DataValidator - æ•°æ®éªŒè¯å™¨

æä¾›æ•°æ®è´¨é‡ç›‘æ§å’ŒéªŒè¯åŠŸèƒ½
"""

import pandas as pd
import numpy as np
from typing import Dict, List, Any, Optional
import logging
from dataclasses import dataclass
from .config import DataConfig


@dataclass
class ValidationReport:
    """æ•°æ®éªŒè¯æŠ¥å‘Š"""
    is_valid: bool
    warnings: List[str]
    errors: List[str]
    stats: Dict[str, Any]
    
    def print_report(self):
        """æ‰“å°éªŒè¯æŠ¥å‘Š"""
        print("\n" + "=" * 80)
        print("ğŸ“‹ æ•°æ®éªŒè¯æŠ¥å‘Š")
        print("=" * 80)
        
        print(f"\nçŠ¶æ€: {'âœ… é€šè¿‡' if self.is_valid else 'âŒ å¤±è´¥'}")
        
        if self.errors:
            print(f"\né”™è¯¯ ({len(self.errors)}):")
            for i, error in enumerate(self.errors, 1):
                print(f"  {i}. âŒ {error}")
        
        if self.warnings:
            print(f"\nè­¦å‘Š ({len(self.warnings)}):")
            for i, warning in enumerate(self.warnings, 1):
                print(f"  {i}. âš ï¸  {warning}")
        
        if self.stats:
            print(f"\nç»Ÿè®¡ä¿¡æ¯:")
            for key, value in self.stats.items():
                print(f"  {key}: {value}")
        
        print("=" * 80)


class DataValidator:
    """æ•°æ®éªŒè¯å™¨ - æ•°æ®è´¨é‡ç›‘æ§"""
    
    def __init__(self, config: DataConfig):
        """
        Args:
            config: DataConfigé…ç½®å¯¹è±¡
        """
        self.config = config
        self.logger = self._setup_logger()
    
    def _setup_logger(self) -> logging.Logger:
        """é…ç½®æ—¥å¿—"""
        logger = logging.getLogger('DataValidator')
        logger.setLevel(getattr(logging, self.config.log_level))
        
        if not logger.handlers:
            handler = logging.StreamHandler()
            formatter = logging.Formatter(
                '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
            )
            handler.setFormatter(formatter)
            logger.addHandler(handler)
        
        return logger
    
    def validate(self, df: pd.DataFrame, 
                feature_cols: Optional[List[str]] = None) -> ValidationReport:
        """
        å…¨é¢éªŒè¯æ•°æ®è´¨é‡
        
        Args:
            df: å¾…éªŒè¯çš„æ•°æ®
            feature_cols: ç‰¹å¾åˆ—åˆ—è¡¨
            
        Returns:
            ValidationReportå¯¹è±¡
        """
        self.logger.info("ğŸ” å¼€å§‹æ•°æ®éªŒè¯...")
        
        errors = []
        warnings = []
        stats = {}
        
        # 1. åŸºç¡€éªŒè¯
        errors.extend(self._validate_basic(df))
        
        # 2. ç¼ºå¤±å€¼æ£€æŸ¥
        missing_warnings = self._check_missing_values(df, feature_cols)
        warnings.extend(missing_warnings)
        
        # 3. æ—¶åºè¿ç»­æ€§æ£€æŸ¥
        continuity_warnings = self._check_time_continuity(df)
        warnings.extend(continuity_warnings)
        
        # 4. å¼‚å¸¸å€¼æ£€æµ‹
        if self.config.detect_outliers:
            outlier_stats = self._detect_outliers(df, feature_cols)
            stats['outliers'] = outlier_stats
        
        # 5. è‚¡ç¥¨æ ·æœ¬æ•°æ£€æŸ¥
        stock_warnings = self._check_stock_samples(df)
        warnings.extend(stock_warnings)
        
        # 6. æ•°æ®æ³„æ¼æ£€æµ‹
        leakage_warnings = self._check_data_leakage(df)
        warnings.extend(leakage_warnings)
        
        # ç”Ÿæˆç»Ÿè®¡
        stats.update(self._compute_validation_stats(df, feature_cols))
        
        # åˆ¤æ–­æ˜¯å¦é€šè¿‡éªŒè¯
        is_valid = len(errors) == 0
        
        self.logger.info(f"âœ… éªŒè¯å®Œæˆ: {len(errors)} é”™è¯¯, {len(warnings)} è­¦å‘Š")
        
        return ValidationReport(
            is_valid=is_valid,
            warnings=warnings,
            errors=errors,
            stats=stats
        )
    
    def _validate_basic(self, df: pd.DataFrame) -> List[str]:
        """åŸºç¡€éªŒè¯"""
        errors = []
        
        # æ£€æŸ¥æ˜¯å¦ä¸ºç©º
        if df.empty:
            errors.append("æ•°æ®ä¸ºç©º")
            return errors
        
        # æ£€æŸ¥å¿…éœ€åˆ—
        required_cols = [
            self.config.stock_col,
            self.config.time_col,
            self.config.label_col
        ]
        
        for col in required_cols:
            if col not in df.columns:
                errors.append(f"ç¼ºå°‘å¿…éœ€åˆ—: {col}")
        
        return errors
    
    def _check_missing_values(self, df: pd.DataFrame, 
                             feature_cols: Optional[List[str]]) -> List[str]:
        """æ£€æŸ¥ç¼ºå¤±å€¼"""
        warnings = []
        
        if feature_cols is None:
            check_cols = df.columns
        else:
            check_cols = feature_cols
        
        for col in check_cols:
            if col not in df.columns:
                continue
            
            na_count = df[col].isnull().sum()
            na_ratio = na_count / len(df)
            
            if na_ratio > self.config.max_na_ratio:
                warnings.append(
                    f"åˆ— '{col}' ç¼ºå¤±å€¼è¿‡é«˜: {na_ratio*100:.2f}% ({na_count}/{len(df)})"
                )
        
        return warnings
    
    def _check_time_continuity(self, df: pd.DataFrame) -> List[str]:
        """æ£€æŸ¥æ—¶åºè¿ç»­æ€§"""
        warnings = []
        
        if self.config.time_col not in df.columns:
            return warnings
        
        # æŒ‰è‚¡ç¥¨åˆ†ç»„æ£€æŸ¥
        for stock_code, stock_df in df.groupby(self.config.stock_col):
            dates = pd.to_datetime(stock_df[self.config.time_col]).sort_values()
            
            # æ£€æŸ¥æ˜¯å¦æœ‰é‡å¤æ—¥æœŸ
            duplicates = dates.duplicated().sum()
            if duplicates > 0:
                warnings.append(
                    f"è‚¡ç¥¨ {stock_code} å­˜åœ¨ {duplicates} ä¸ªé‡å¤æ—¥æœŸ"
                )
            
            # æ£€æŸ¥æ—¶é—´é—´éš”ï¼ˆç®€å•æ£€æŸ¥æ˜¯å¦å•è°ƒé€’å¢ï¼‰
            if not dates.is_monotonic_increasing:
                warnings.append(
                    f"è‚¡ç¥¨ {stock_code} æ—¶é—´åºåˆ—ä¸æ˜¯å•è°ƒé€’å¢"
                )
        
        return warnings
    
    def _detect_outliers(self, df: pd.DataFrame, 
                        feature_cols: Optional[List[str]]) -> Dict[str, int]:
        """æ£€æµ‹å¼‚å¸¸å€¼"""
        outlier_stats = {}
        
        if feature_cols is None:
            return outlier_stats
        
        threshold = self.config.outlier_std_threshold
        
        for col in feature_cols:
            if col not in df.columns:
                continue
            
            if not pd.api.types.is_numeric_dtype(df[col]):
                continue
            
            mean = df[col].mean()
            std = df[col].std()
            
            outliers = np.abs((df[col] - mean) / std) > threshold
            outlier_count = outliers.sum()
            
            if outlier_count > 0:
                outlier_stats[col] = int(outlier_count)
        
        return outlier_stats
    
    def _check_stock_samples(self, df: pd.DataFrame) -> List[str]:
        """æ£€æŸ¥æ¯åªè‚¡ç¥¨çš„æ ·æœ¬æ•°"""
        warnings = []
        
        if self.config.stock_col not in df.columns:
            return warnings
        
        stock_counts = df[self.config.stock_col].value_counts()
        
        low_sample_stocks = stock_counts[
            stock_counts < self.config.min_samples_per_stock
        ]
        
        if len(low_sample_stocks) > 0:
            warnings.append(
                f"{len(low_sample_stocks)} åªè‚¡ç¥¨æ ·æœ¬æ•°å°‘äº {self.config.min_samples_per_stock}"
            )
        
        return warnings
    
    def _check_data_leakage(self, df: pd.DataFrame) -> List[str]:
        """æ£€æŸ¥æ•°æ®æ³„æ¼ï¼ˆç®€å•æ£€æŸ¥ï¼‰"""
        warnings = []
        
        # æ£€æŸ¥æ˜¯å¦æœ‰æœªæ¥æ•°æ®ï¼ˆæ—¶é—´æˆ³ï¼‰
        # è¿™é‡Œåªåšç®€å•ç¤ºä¾‹ï¼Œå®é™…åº”ç”¨ä¸­éœ€è¦æ›´å¤æ‚çš„æ£€æŸ¥
        
        return warnings
    
    def _compute_validation_stats(self, df: pd.DataFrame, 
                                  feature_cols: Optional[List[str]]) -> Dict[str, Any]:
        """è®¡ç®—éªŒè¯ç»Ÿè®¡ä¿¡æ¯"""
        stats = {
            'total_rows': len(df),
            'total_columns': len(df.columns),
        }
        
        if self.config.stock_col in df.columns:
            stats['num_stocks'] = df[self.config.stock_col].nunique()
        
        if self.config.time_col in df.columns:
            stats['date_range'] = (
                str(df[self.config.time_col].min()),
                str(df[self.config.time_col].max())
            )
        
        if feature_cols:
            stats['num_features'] = len(feature_cols)
            
            # ç‰¹å¾ç±»å‹åˆ†å¸ƒ
            numeric_features = sum(
                1 for col in feature_cols 
                if col in df.columns and pd.api.types.is_numeric_dtype(df[col])
            )
            stats['numeric_features'] = numeric_features
        
        return stats


if __name__ == '__main__':
    # æµ‹è¯•æ•°æ®éªŒè¯å™¨
    from config import DataConfig
    
    print("=" * 80)
    print("DataValidator æµ‹è¯•")
    print("=" * 80)
    
    # åˆ›å»ºé…ç½®
    config = DataConfig(
        max_na_ratio=0.3,
        min_samples_per_stock=50,
        detect_outliers=True,
        outlier_std_threshold=5.0
    )
    
    # åˆ›å»ºéªŒè¯å™¨
    validator = DataValidator(config)
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    np.random.seed(42)
    df = pd.DataFrame({
        'ts_code': ['000001.SZ'] * 100 + ['000002.SZ'] * 100,
        'trade_date': pd.date_range('2020-01-01', periods=100).tolist() * 2,
        'y_processed': np.random.randn(200),
        'feature1': np.random.randn(200),
        'feature2': [np.nan] * 50 + list(np.random.randn(150)),  # 25% ç¼ºå¤±
        'feature3': np.concatenate([np.random.randn(190), [100, 100, 100, 100, 100, 100, 100, 100, 100, 100]]),  # å¼‚å¸¸å€¼
    })
    
    # æ‰§è¡ŒéªŒè¯
    report = validator.validate(df, feature_cols=['feature1', 'feature2', 'feature3'])
    report.print_report()
    
    print("\nâœ… æ•°æ®éªŒè¯å™¨æµ‹è¯•å®Œæˆ")
