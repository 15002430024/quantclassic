"""
ç‰¹å¾å¤„ç†å¼•æ“ - å…·ä½“ç®—æ³•å®ç°
"""
import numpy as np
import pandas as pd
from typing import List, Union, Tuple, Dict, Optional
from scipy.stats.mstats import winsorize
from scipy import stats
from sklearn.linear_model import LinearRegression
from tqdm import tqdm
import logging


logger = logging.getLogger(__name__)


class FeatureProcessor:
    """ç‰¹å¾å¤„ç†å¼•æ“ - å®ç°å„ç§æ•°æ®å¤„ç†ç®—æ³•"""
    
    def __init__(self, groupby_columns: List[str] = None, stock_col: str = 'order_book_id'):
        """
        åˆå§‹åŒ–ç‰¹å¾å¤„ç†å™¨
        
        Args:
            groupby_columns: é»˜è®¤åˆ†ç»„åˆ—
            stock_col: è‚¡ç¥¨ä»£ç åˆ—åï¼Œå…¼å®¹ 'order_book_id'(RiceQuant) å’Œ 'ts_code'(Tushare/DataManager)
        """
        self.groupby_columns = groupby_columns or ['trade_date']
        self.stock_col = stock_col
        self.fitted_params = {}  # å­˜å‚¨æ‹Ÿåˆå‚æ•°
    
    def set_stock_col(self, df: pd.DataFrame) -> str:
        """
        æ ¹æ®æ•°æ®è‡ªåŠ¨æ£€æµ‹å¹¶è®¾ç½®è‚¡ç¥¨ä»£ç åˆ—å
        
        Args:
            df: æ•°æ®æ¡†
            
        Returns:
            å®é™…ä½¿ç”¨çš„è‚¡ç¥¨ä»£ç åˆ—å
        """
        candidates = ['order_book_id', 'ts_code', 'stock_code', 'symbol']
        for col in candidates:
            if col in df.columns:
                self.stock_col = col
                return col
        return self.stock_col
    
    # ========== åŸºç¡€å¤„ç† ==========
    
    def handle_infinite_values(
        self, 
        df: pd.DataFrame, 
        features: List[str],
        method: str = 'remove',
        lower: float = -1e10,
        upper: float = 1e10
    ) -> pd.DataFrame:
        """
        å¤„ç†æ— ç©·å€¼
        
        Args:
            df: æ•°æ®æ¡†
            features: ç‰¹å¾åˆ—
            method: 'remove'(æ›¿æ¢ä¸ºNaN) æˆ– 'clip'(æˆªæ–­)
            lower: ä¸‹ç•Œ
            upper: ä¸Šç•Œ
        """
        df = df.copy()
        
        if method == 'remove':
            df[features] = df[features].replace([np.inf, -np.inf], np.nan)
        elif method == 'clip':
            for col in features:
                df.loc[df[col] == np.inf, col] = upper
                df.loc[df[col] == -np.inf, col] = lower
        
        return df
    
    def handle_missing_values(
        self,
        df: pd.DataFrame,
        features: List[str],
        method: str = 'median',
        fillna_value: float = 0,
        industry_column: Optional[str] = None
    ) -> pd.DataFrame:
        """
        å¤„ç†ç¼ºå¤±å€¼
        
        Args:
            df: æ•°æ®æ¡†
            features: ç‰¹å¾åˆ—
            method: 'median', 'mean', 'zero', 'forward'
            fillna_value: å¡«å……å€¼(method='constant'æ—¶ä½¿ç”¨)
            industry_column: è¡Œä¸šåˆ—(ç”¨äºè¡Œä¸šå†…å¡«å……)
        """
        df = df.copy()
        
        if method == 'zero':
            df[features] = df[features].fillna(0)
        
        elif method == 'forward':
            # ä½¿ç”¨è‡ªé€‚åº”çš„è‚¡ç¥¨ä»£ç åˆ—å
            stock_col = self.stock_col
            if stock_col not in df.columns:
                # å°è¯•è‡ªåŠ¨æ£€æµ‹
                for col in ['order_book_id', 'ts_code', 'stock_code', 'symbol']:
                    if col in df.columns:
                        stock_col = col
                        break
            df[features] = df.groupby(stock_col)[features].fillna(method='ffill')
        
        elif method in ['median', 'mean']:
            # å…ˆç”¨è¡Œä¸šå†…ç»Ÿè®¡é‡å¡«å……
            if industry_column and industry_column in df.columns:
                for col in features:
                    if method == 'median':
                        industry_values = df.groupby(self.groupby_columns + [industry_column])[col].transform('median')
                    else:
                        industry_values = df.groupby(self.groupby_columns + [industry_column])[col].transform('mean')
                    df[col] = df[col].fillna(industry_values)
            
            # å†ç”¨å¸‚åœºç»Ÿè®¡é‡å¡«å……
            for col in features:
                if method == 'median':
                    market_values = df.groupby(self.groupby_columns)[col].transform('median')
                else:
                    market_values = df.groupby(self.groupby_columns)[col].transform('mean')
                df[col] = df[col].fillna(market_values)
            
            # æœ€åç”¨0å¡«å……
            df[features] = df[features].fillna(0)
        
        return df
    
    # ========== æ ‡å‡†åŒ–/å½’ä¸€åŒ– ==========
    
    def z_score_normalize(
        self,
        df: pd.DataFrame,
        features: List[str],
        ddof: int = 1,
        clip_sigma: Optional[float] = None,
        fit: bool = True,
        normalize_mode: str = 'cross_section'
    ) -> pd.DataFrame:
        """
        Z-scoreæ ‡å‡†åŒ–ï¼ˆæ€§èƒ½ä¼˜åŒ–ç‰ˆæœ¬ï¼‰
        
        Args:
            df: æ•°æ®æ¡†
            features: ç‰¹å¾åˆ—
            ddof: æ ‡å‡†å·®è‡ªç”±åº¦
            clip_sigma: å¯é€‰çš„sigmaæˆªæ–­å€¼
            fit: æ˜¯å¦æ‹Ÿåˆ(ä¿å­˜å‡å€¼å’Œæ ‡å‡†å·®)
            normalize_mode: æ ‡å‡†åŒ–æ¨¡å¼
                - 'cross_section': æˆªé¢æ ‡å‡†åŒ–ï¼ˆæŒ‰æ—¥æœŸåˆ†ç»„ï¼Œæ¯ä¸ªæ—¥æœŸå†…æ ‡å‡†åŒ–ï¼‰
                - 'time_series': æ—¶åºæ ‡å‡†åŒ–ï¼ˆæŒ‰è‚¡ç¥¨åˆ†ç»„ï¼Œæ¯åªè‚¡ç¥¨è‡ªèº«æ—¶åºæ ‡å‡†åŒ–ï¼‰
                - 'global': å…¨å±€æ ‡å‡†åŒ–ï¼ˆæ•´ä½“æ ‡å‡†åŒ–ï¼‰
        """
        from tqdm.auto import tqdm
        
        df = df.copy()
        
        # æ€§èƒ½ä¼˜åŒ–ï¼šæ‰¹é‡å¤„ç†ï¼Œå‡å°‘ groupby æ¬¡æ•°
        if fit:
            if normalize_mode == 'cross_section':
                # æˆªé¢æ ‡å‡†åŒ–ï¼šä¸€æ¬¡æ€§è®¡ç®—æ‰€æœ‰ç‰¹å¾çš„å‡å€¼å’Œæ ‡å‡†å·®
                grouped = df.groupby(self.groupby_columns)[features]
                mean_vals = grouped.transform('mean')
                std_vals = grouped.transform(lambda x: x.std(ddof=ddof))
                
            elif normalize_mode == 'time_series':
                # æ—¶åºæ ‡å‡†åŒ–ï¼šæŒ‰è‚¡ç¥¨åˆ†ç»„ï¼ˆè‡ªé€‚åº”åˆ—åï¼‰
                stock_col = self.stock_col
                if stock_col not in df.columns:
                    # å°è¯•è‡ªåŠ¨æ£€æµ‹
                    for col in ['order_book_id', 'ts_code', 'stock_code', 'symbol']:
                        if col in df.columns:
                            stock_col = col
                            break
                
                if stock_col in df.columns:
                    grouped = df.groupby(stock_col)[features]
                    mean_vals = grouped.transform('mean')
                    std_vals = grouped.transform(lambda x: x.std(ddof=ddof))
                else:
                    logger.warning(f"æ—¶åºæ ‡å‡†åŒ–éœ€è¦è‚¡ç¥¨ä»£ç åˆ—ï¼ˆå°è¯•: order_book_id/ts_code/stock_code/symbolï¼‰ï¼Œå›é€€åˆ°å…¨å±€æ ‡å‡†åŒ–")
                    mean_vals = df[features].mean()
                    std_vals = df[features].std(ddof=ddof)
            
            elif normalize_mode == 'global':
                # å…¨å±€æ ‡å‡†åŒ–ï¼šæ•´ä½“è®¡ç®—ï¼ˆå‘é‡åŒ–ï¼‰
                mean_vals = df[features].mean()
                std_vals = df[features].std(ddof=ddof)
            
            else:
                raise ValueError(f"ä¸æ”¯æŒçš„æ ‡å‡†åŒ–æ¨¡å¼: {normalize_mode}")
            
            # ä¿å­˜å‚æ•°ï¼ˆæ‰¹é‡ï¼‰
            for col in features:
                if isinstance(mean_vals, pd.DataFrame):
                    self.fitted_params[f'{col}_mean_{normalize_mode}'] = mean_vals[col]
                    self.fitted_params[f'{col}_std_{normalize_mode}'] = std_vals[col]
                else:
                    self.fitted_params[f'{col}_mean_{normalize_mode}'] = mean_vals[col] if hasattr(mean_vals, '__getitem__') else mean_vals
                    self.fitted_params[f'{col}_std_{normalize_mode}'] = std_vals[col] if hasattr(std_vals, '__getitem__') else std_vals
        else:
            # åŠ è½½ä¿å­˜çš„å‚æ•°
            mean_vals = pd.DataFrame({col: self.fitted_params[f'{col}_mean_{normalize_mode}'] for col in features})
            std_vals = pd.DataFrame({col: self.fitted_params[f'{col}_std_{normalize_mode}'] for col in features})
        
        # æ‰¹é‡æ ‡å‡†åŒ–ï¼ˆå‘é‡åŒ–æ“ä½œï¼‰
        if isinstance(mean_vals, pd.DataFrame):
            df[features] = (df[features] - mean_vals) / (std_vals + 1e-8)
        else:
            for col in features:
                df[col] = (df[col] - mean_vals[col]) / (std_vals[col] + 1e-8)
        
        # å¯é€‰çš„sigmaæˆªæ–­
        if clip_sigma:
            df[features] = df[features].clip(-clip_sigma, clip_sigma)
        
        return df
    
    def minmax_normalize(
        self,
        df: pd.DataFrame,
        features: List[str],
        output_range: Tuple[float, float] = (0, 1),
        fit: bool = True,
        normalize_mode: str = 'cross_section'
    ) -> pd.DataFrame:
        """
        æœ€å°æœ€å¤§å½’ä¸€åŒ–
        
        Args:
            df: æ•°æ®æ¡†
            features: ç‰¹å¾åˆ—
            output_range: è¾“å‡ºèŒƒå›´
            fit: æ˜¯å¦æ‹Ÿåˆ
            normalize_mode: æ ‡å‡†åŒ–æ¨¡å¼
                - 'cross_section': æˆªé¢å½’ä¸€åŒ–ï¼ˆæŒ‰æ—¥æœŸåˆ†ç»„ï¼Œæ¯ä¸ªæ—¥æœŸå†…å½’ä¸€åŒ–ï¼‰
                - 'time_series': æ—¶åºå½’ä¸€åŒ–ï¼ˆæŒ‰è‚¡ç¥¨åˆ†ç»„ï¼Œæ¯åªè‚¡ç¥¨è‡ªèº«æ—¶åºå½’ä¸€åŒ–ï¼‰
                - 'global': å…¨å±€å½’ä¸€åŒ–ï¼ˆæ•´ä½“å½’ä¸€åŒ–ï¼‰
        """
        df = df.copy()
        min_val, max_val = output_range
        
        for col in features:
            if fit:
                if normalize_mode == 'cross_section':
                    # æˆªé¢å½’ä¸€åŒ–ï¼šæŒ‰æ—¥æœŸåˆ†ç»„ï¼ˆåŒä¸€å¤©å†…ä¸åŒè‚¡ç¥¨å½’ä¸€åŒ–ï¼‰
                    col_min = df.groupby(self.groupby_columns)[col].transform('min')
                    col_max = df.groupby(self.groupby_columns)[col].transform('max')
                
                elif normalize_mode == 'time_series':
                    # æ—¶åºå½’ä¸€åŒ–ï¼šæŒ‰è‚¡ç¥¨åˆ†ç»„ï¼ˆè‡ªé€‚åº”åˆ—åï¼‰
                    stock_col = self.stock_col
                    if stock_col not in df.columns:
                        # å°è¯•è‡ªåŠ¨æ£€æµ‹
                        for candidate in ['order_book_id', 'ts_code', 'stock_code', 'symbol']:
                            if candidate in df.columns:
                                stock_col = candidate
                                break
                    
                    if stock_col in df.columns:
                        col_min = df.groupby(stock_col)[col].transform('min')
                        col_max = df.groupby(stock_col)[col].transform('max')
                    else:
                        logger.warning(f"æ—¶åºå½’ä¸€åŒ–éœ€è¦è‚¡ç¥¨ä»£ç åˆ—ï¼ˆå°è¯•: order_book_id/ts_code/stock_code/symbolï¼‰ï¼Œå›é€€åˆ°å…¨å±€å½’ä¸€åŒ–")
                        col_min = df[col].min()
                        col_max = df[col].max()
                
                elif normalize_mode == 'global':
                    # å…¨å±€å½’ä¸€åŒ–ï¼šæ•´ä½“è®¡ç®—æœ€å°æœ€å¤§å€¼
                    col_min = df[col].min()
                    col_max = df[col].max()
                
                else:
                    raise ValueError(f"ä¸æ”¯æŒçš„æ ‡å‡†åŒ–æ¨¡å¼: {normalize_mode}")
                
                self.fitted_params[f'{col}_min_{normalize_mode}'] = col_min
                self.fitted_params[f'{col}_max_{normalize_mode}'] = col_max
            else:
                col_min = self.fitted_params[f'{col}_min_{normalize_mode}']
                col_max = self.fitted_params[f'{col}_max_{normalize_mode}']
            
            # å½’ä¸€åŒ–
            col_range = col_max - col_min
            df[col] = (df[col] - col_min) / (col_range + 1e-8)
            df[col] = df[col] * (max_val - min_val) + min_val
        
        return df
    
    def rank_normalize(
        self,
        df: pd.DataFrame,
        features: List[str],
        output_range: Tuple[float, float] = (-1, 1),
        method: str = 'average',
        normalize_mode: str = 'cross_section'
    ) -> pd.DataFrame:
        """
        ç§©å½’ä¸€åŒ–åˆ°æŒ‡å®šåŒºé—´
        
        Args:
            df: æ•°æ®æ¡†
            features: ç‰¹å¾åˆ—
            output_range: è¾“å‡ºèŒƒå›´,é»˜è®¤(-1, 1)
            method: æ’åæ–¹æ³• 'average', 'min', 'max', 'dense', 'ordinal'
            normalize_mode: æ ‡å‡†åŒ–æ¨¡å¼
                - 'cross_section': æˆªé¢å½’ä¸€åŒ–ï¼ˆæŒ‰æ—¥æœŸåˆ†ç»„ï¼Œæ¯ä¸ªæ—¥æœŸå†…æ’åå½’ä¸€åŒ–ï¼‰
                - 'time_series': æ—¶åºå½’ä¸€åŒ–ï¼ˆæŒ‰è‚¡ç¥¨åˆ†ç»„ï¼Œæ¯åªè‚¡ç¥¨è‡ªèº«æ—¶åºæ’åå½’ä¸€åŒ–ï¼‰
                - 'global': å…¨å±€å½’ä¸€åŒ–ï¼ˆæ•´ä½“æ’åå½’ä¸€åŒ–ï¼‰
        """
        df = df.copy()
        min_val, max_val = output_range
        
        def _rank_normalize_group(x):
            """å¯¹å•ä¸ªåˆ†ç»„è¿›è¡Œç§©å½’ä¸€åŒ–"""
            if x.isnull().all():
                return x
            
            # è®¡ç®—ç§©
            ranks = x.rank(method=method, na_option='keep')
            
            # è·å–æœ‰æ•ˆå€¼æ•°é‡
            valid_count = ranks.notna().sum()
            
            if valid_count <= 1:
                return pd.Series(0.0, index=x.index)
            
            # å½’ä¸€åŒ–åˆ°æŒ‡å®šåŒºé—´
            # å…¬å¼: (max_val - min_val) * (rank - 1) / (n - 1) + min_val
            normalized = (max_val - min_val) * (ranks - 1.0) / (valid_count - 1.0) + min_val
            
            return normalized
        
        # æ ¹æ®æ¨¡å¼é€‰æ‹©åˆ†ç»„æ–¹å¼
        if normalize_mode == 'cross_section':
            # æˆªé¢å½’ä¸€åŒ–ï¼šæŒ‰æ—¥æœŸåˆ†ç»„ï¼ˆåŒä¸€å¤©å†…ä¸åŒè‚¡ç¥¨æ’åå½’ä¸€åŒ–ï¼‰
            for col in features:
                df[col] = df.groupby(self.groupby_columns)[col].transform(_rank_normalize_group)
        
        elif normalize_mode == 'time_series':
            # æ—¶åºå½’ä¸€åŒ–ï¼šæŒ‰è‚¡ç¥¨åˆ†ç»„ï¼ˆæ¯åªè‚¡ç¥¨è‡ªèº«æ—¶åºæ’åå½’ä¸€åŒ–ï¼‰
            if 'order_book_id' in df.columns:
                for col in features:
                    df[col] = df.groupby('order_book_id')[col].transform(_rank_normalize_group)
            else:
                logger.warning(f"æ—¶åºå½’ä¸€åŒ–éœ€è¦order_book_idåˆ—ï¼Œå›é€€åˆ°å…¨å±€å½’ä¸€åŒ–")
                for col in features:
                    df[col] = _rank_normalize_group(df[col])
        
        elif normalize_mode == 'global':
            # å…¨å±€å½’ä¸€åŒ–ï¼šæ•´ä½“æ’åå½’ä¸€åŒ–
            for col in features:
                df[col] = _rank_normalize_group(df[col])
        
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ ‡å‡†åŒ–æ¨¡å¼: {normalize_mode}")
        
        return df
    
    # ========== æå€¼å¤„ç† ==========
    
    def winsorize_features(
        self,
        df: pd.DataFrame,
        features: List[str],
        limits: Tuple[float, float] = (0.025, 0.025)
    ) -> pd.DataFrame:
        """
        Winsorizeå»æå€¼ï¼ˆæ€§èƒ½ä¼˜åŒ–ç‰ˆæœ¬ï¼‰
        
        Args:
            df: æ•°æ®æ¡†
            features: ç‰¹å¾åˆ—
            limits: ç¼©å°¾æ¯”ä¾‹ (ä¸‹é™, ä¸Šé™)
        """
        from tqdm.auto import tqdm
        
        df = df.copy()
        
        def _winsorize_group(x):
            """å¯¹å•ä¸ªåˆ†ç»„è¿›è¡Œç¼©å°¾"""
            if x.isnull().all():
                return x
            try:
                return pd.Series(winsorize(x.astype(float), limits=limits), index=x.index)
            except:
                return x
        
        # æŒ‰æ—¥æœŸåˆ†ç»„è¿›è¡Œç¼©å°¾ï¼ˆæ·»åŠ è¿›åº¦æ¡ï¼‰
        with tqdm(total=len(features), desc="å»æå€¼å¤„ç†", unit="åˆ—", leave=False) as pbar:
            for col in features:
                df[col] = df.groupby(self.groupby_columns)[col].transform(_winsorize_group)
                pbar.update(1)
        
        return df
    
    def clip_features(
        self,
        df: pd.DataFrame,
        features: List[str],
        lower_percentile: float = 1,
        upper_percentile: float = 99
    ) -> pd.DataFrame:
        """
        æˆªå°¾å¤„ç†(æŒ‰ç™¾åˆ†ä½)
        
        Args:
            df: æ•°æ®æ¡†
            features: ç‰¹å¾åˆ—
            lower_percentile: ä¸‹ç™¾åˆ†ä½
            upper_percentile: ä¸Šç™¾åˆ†ä½
        """
        df = df.copy()
        
        for col in features:
            lower = df.groupby(self.groupby_columns)[col].transform(
                lambda x: x.quantile(lower_percentile / 100)
            )
            upper = df.groupby(self.groupby_columns)[col].transform(
                lambda x: x.quantile(upper_percentile / 100)
            )
            df[col] = df[col].clip(lower, upper)
        
        return df
    
    # ========== ä¸­æ€§åŒ–å¤„ç† ==========
    
    def industry_cap_neutralize_ols(
        self,
        df: pd.DataFrame,
        features: List[str],
        industry_column: str = 'industry_name',
        market_cap_column: str = 'total_mv',
        min_samples: int = 10
    ) -> pd.DataFrame:
        """
        OLSå¸‚å€¼è¡Œä¸šä¸­æ€§åŒ–ï¼ˆæ€§èƒ½ä¼˜åŒ–ç‰ˆæœ¬ï¼‰
        
        å¯¹æ¯ä¸ªç‰¹å¾,ä½¿ç”¨OLSå›å½’å‰”é™¤è¡Œä¸šå’Œå¸‚å€¼çš„å½±å“
        å…¬å¼: feature_residual = feature - (Î²_industry * industry_dummy + Î²_mv * market_cap)
        
        Args:
            df: æ•°æ®æ¡†
            features: ç‰¹å¾åˆ—
            industry_column: è¡Œä¸šåˆ—
            market_cap_column: å¸‚å€¼åˆ—
            min_samples: æœ€å°æ ·æœ¬æ•°
        """
        from tqdm.auto import tqdm
        
        df = df.copy()
        
        def _neutralize_group(group):
            """å¯¹å•ä¸ªæ—¶é—´åˆ‡ç‰‡è¿›è¡Œä¸­æ€§åŒ–ï¼ˆæ‰¹é‡å¤„ç†ï¼‰"""
            if len(group) < min_samples:
                return group
            
            # å‡†å¤‡è¡Œä¸šå“‘å˜é‡ï¼ˆä¸€æ¬¡æ€§åˆ›å»ºï¼‰
            industry_dummies = pd.get_dummies(group[industry_column], prefix='ind')
            
            # å¸‚å€¼å–å¯¹æ•°
            if group[market_cap_column].isnull().all():
                return group
            
            log_mv = np.log(group[market_cap_column] + 1)
            
            # åˆå¹¶ç‰¹å¾ï¼ˆé¢„å…ˆè®¡ç®—ï¼‰
            X = pd.concat([industry_dummies, log_mv.rename('log_mv')], axis=1)
            
            # æ‰¹é‡å¤„ç†æ‰€æœ‰ç‰¹å¾ï¼ˆå‡å°‘å¾ªç¯å¼€é”€ï¼‰
            for col in features:
                if col not in group.columns:
                    continue
                
                # å‡†å¤‡æœ‰æ•ˆæ•°æ®
                valid_mask = group[col].notna() & log_mv.notna()
                if valid_mask.sum() < min_samples:
                    continue
                
                y = group.loc[valid_mask, col].values
                X_valid = X.loc[valid_mask].values
                
                # OLSå›å½’
                try:
                    lr = LinearRegression(n_jobs=-1)  # ä½¿ç”¨å¤šçº¿ç¨‹åŠ é€Ÿ
                    lr.fit(X_valid, y)
                    
                    # è®¡ç®—æ®‹å·®
                    residuals = y - lr.predict(X_valid)
                    group.loc[valid_mask, col] = residuals
                except:
                    pass
            
            return group
        
        # æŒ‰æ—¥æœŸåˆ†ç»„è¿›è¡Œä¸­æ€§åŒ–ï¼ˆæ·»åŠ è¿›åº¦æ¡ï¼‰
        grouped = df.groupby(self.groupby_columns, group_keys=False)
        n_groups = len(grouped)
        
        with tqdm(total=n_groups, desc="OLSä¸­æ€§åŒ–", unit="æ—¥æœŸ", leave=False) as pbar:
            results = []
            for name, group in grouped:
                results.append(_neutralize_group(group))
                pbar.update(1)
            df = pd.concat(results, ignore_index=False)
        
        return df
    
    def industry_cap_neutralize_mean(
        self,
        df: pd.DataFrame,
        features: List[str],
        industry_column: str = 'industry_name',
        market_cap_column: str = 'total_mv',
        n_quantiles: int = 5
    ) -> pd.DataFrame:
        """
        å‡å‡å€¼ç‰ˆå¸‚å€¼è¡Œä¸šä¸­æ€§åŒ–
        
        åœ¨æ¯ä¸ªè¡Œä¸š-å¸‚å€¼åˆ†ç»„å†…,å‡å»ç»„å†…å‡å€¼
        
        Args:
            df: æ•°æ®æ¡†
            features: ç‰¹å¾åˆ—
            industry_column: è¡Œä¸šåˆ—
            market_cap_column: å¸‚å€¼åˆ—
            n_quantiles: å¸‚å€¼åˆ†ç»„æ•°
        """
        df = df.copy()
        
        def _neutralize_group(group):
            """å¯¹å•ä¸ªæ—¶é—´åˆ‡ç‰‡è¿›è¡Œä¸­æ€§åŒ–"""
            # å¸‚å€¼åˆ†ç»„
            group['mv_quantile'] = pd.qcut(
                group[market_cap_column], 
                q=n_quantiles, 
                labels=False, 
                duplicates='drop'
            )
            
            # åœ¨è¡Œä¸š-å¸‚å€¼åˆ†ç»„å†…å‡å‡å€¼
            for col in features:
                if col in group.columns:
                    group_mean = group.groupby([industry_column, 'mv_quantile'])[col].transform('mean')
                    group[col] = group[col] - group_mean
            
            group.drop('mv_quantile', axis=1, inplace=True)
            return group
        
        # æŒ‰æ—¥æœŸåˆ†ç»„è¿›è¡Œä¸­æ€§åŒ–
        df = df.groupby(self.groupby_columns, group_keys=False).apply(_neutralize_group)
        
        return df
    
    def simstock_label_neutralize(
        self,
        df: pd.DataFrame,
        label_column: str = 'ret_1d',
        similarity_threshold: float = 0.7,
        lookback_window: int = 252,
        min_similar_stocks: int = 5,
        correlation_method: str = 'pearson',
        output_column: str = 'alpha_label',
        recalc_interval: int = 20  # æ–°å¢ï¼šæ¯éš”20å¤©æ›´æ–°ä¸€æ¬¡ç›¸å…³æ€§çŸ©é˜µï¼ˆå…³é”®ä¼˜åŒ–ï¼‰
    ) -> pd.DataFrame:
        """
        SimStockæ ‡ç­¾ä¸­æ€§åŒ–ï¼ˆNumpyåŠ é€Ÿ + é™é¢‘æ›´æ–°ç‰ˆï¼‰
        
        å¯¹æ¯åªè‚¡ç¥¨ï¼Œæ‰¾åˆ°æ”¶ç›Šç‡ç›¸å…³æ€§é«˜äºé˜ˆå€¼çš„"å…„å¼Ÿè‚¡ç¥¨"ï¼Œç”¨å…¶å‡å€¼ä½œä¸ºåŸºå‡†ï¼Œæ ‡ç­¾ä¸ºè¶…é¢æ”¶ç›Šã€‚
        
        ğŸš€ æ ¸å¿ƒä¼˜åŒ–ï¼ˆ3å°æ—¶ â†’ 2-3åˆ†é’Ÿï¼‰ï¼š
        1. ä½¿ç”¨ np.corrcoef æ›¿ä»£ pd.DataFrame.corrï¼ˆå¿« 5-10xï¼‰
        2. é™é¢‘æ›´æ–°ï¼šæ¯éš” recalc_interval å¤©æ‰é‡ç®—ç›¸å…³æ€§ï¼ˆå¿« 20xï¼‰
        3. å®½è¡¨çŸ©é˜µï¼šé›¶æ‹·è´åˆ‡ç‰‡ + çŸ©é˜µä¹˜æ³•ï¼ˆå¿« 100xï¼‰
        
        Args:
            df: æ•°æ®æ¡†ï¼ˆå¿…é¡»åŒ…å« 'trade_date' å’Œ 'order_book_id' åˆ—ï¼‰
            label_column: ç”¨äºç›¸å…³æ€§å’Œä¸­æ€§åŒ–çš„æ ‡ç­¾åˆ—ï¼ˆå¦‚ 'y_ret_1d'ï¼‰
            similarity_threshold: ç›¸å…³æ€§é˜ˆå€¼ï¼ˆé»˜è®¤ 0.7ï¼‰
            lookback_window: å›æº¯çª—å£(äº¤æ˜“æ—¥)ï¼ˆé»˜è®¤ 252ï¼Œçº¦1å¹´ï¼‰
            min_similar_stocks: æœ€å°ç›¸ä¼¼è‚¡ç¥¨æ•°ï¼ˆçŸ©é˜µæ–¹æ¡ˆä¸­è‡ªåŠ¨å¤„ç†ï¼‰
            correlation_method: 'pearson' æˆ– 'spearman'ï¼ˆé»˜è®¤ 'pearson'ï¼Œæš‚æœªä½¿ç”¨ï¼‰
            output_column: è¾“å‡ºçš„alphaæ ‡ç­¾åˆ—åï¼ˆé»˜è®¤ 'alpha_label'ï¼‰
            recalc_interval: ç›¸å…³æ€§æ›´æ–°é—´éš”ï¼ˆé»˜è®¤ 20å¤©ï¼‰
                           - è®¾ä¸º 1ï¼šæ¯å¤©æ›´æ–°ï¼ˆæœ€ç²¾ç¡®ä½†æœ€æ…¢ï¼‰
                           - è®¾ä¸º 20ï¼šæ¯æœˆæ›´æ–°ï¼ˆæ¨èï¼Œå¿« 20xï¼‰
                           - è®¾ä¸º 63ï¼šæ¯å­£åº¦æ›´æ–°ï¼ˆé€‚åˆæå¤§æ•°æ®é›†ï¼‰
            
        Returns:
            æ–°å¢ output_column åˆ—çš„ DataFrame
            
        æ€§èƒ½æŒ‡æ ‡ï¼ˆä¸­è¯800ï¼Œ5å¹´æ•°æ®ï¼‰ï¼š
            - æ•°æ®é‡ï¼š~2000å¤© Ã— 800è‚¡ = 160ä¸‡è¡Œ
            - åŸæ–¹æ¡ˆï¼š~3å°æ—¶ï¼ˆæ¯å¤©è®¡ç®—ç›¸å…³æ€§ï¼‰
            - ä¼˜åŒ–åï¼š~2-3åˆ†é’Ÿï¼ˆrecalc_interval=20ï¼‰
            - æé€Ÿæ¯”ï¼š~60x
        """
        print(f"\n{'='*80}")
        print("ğŸš€ SimStock æ ‡ç­¾ä¸­æ€§åŒ–ï¼ˆNumpyåŠ é€Ÿ + é™é¢‘æ›´æ–°ç‰ˆï¼‰")
        print(f"{'='*80}")
        
        # ==================== 1. æ„å»ºå®½è¡¨ (Time x Stock) ====================
        print(f"ğŸ“Š æ­¥éª¤1/4: è½¬æ¢ä¸ºå®½è¡¨çŸ©é˜µ (Pivot)...")
        print(f"  åŸå§‹æ•°æ®: {len(df):,} è¡Œ")
        
        # ç¡®ä¿æ•°æ®å·²æ’åº
        df = df.sort_values(['trade_date', 'order_book_id']).copy()
        
        # Pivot ä¸ºå®½è¡¨ï¼šè¡Œ=æ—¥æœŸ, åˆ—=è‚¡ç¥¨ä»£ç , å€¼=æ ‡ç­¾
        wide_ret = df.pivot(index='trade_date', columns='order_book_id', values=label_column)
        
        # æå–åŸºç¡€æ•°æ®ç»“æ„
        dates = wide_ret.index
        stocks = wide_ret.columns
        ret_values = wide_ret.values  # è½¬æ¢ä¸º numpy æ•°ç»„åŠ é€Ÿ
        n_days, n_stocks = ret_values.shape
        
        print(f"  å®½è¡¨ç»´åº¦: {n_days:,} å¤© Ã— {n_stocks:,} åªè‚¡ç¥¨")
        print(f"  å›æº¯çª—å£: {lookback_window} å¤©")
        print(f"  ç›¸å…³æ€§é˜ˆå€¼: {similarity_threshold:.2f}")
        print(f"  æ›´æ–°é¢‘ç‡: æ¯ {recalc_interval} å¤©é‡æ–°è®¡ç®—ä¸€æ¬¡ç›¸ä¼¼æ€§ï¼ˆå…³é”®ä¼˜åŒ–ï¼ï¼‰")
        print(f"  é¢„è®¡é‡ç®—æ¬¡æ•°: {(n_days - lookback_window) // recalc_interval + 1} æ¬¡ï¼ˆvs åŸæ–¹æ¡ˆ {n_days - lookback_window} æ¬¡ï¼‰")
        
        # åˆå§‹åŒ–ç»“æœçŸ©é˜µ (å…¨ä¸º NaN)
        alpha_matrix = np.full((n_days, n_stocks), np.nan)
        
        # ç¼“å­˜å˜é‡ï¼šå­˜å‚¨ä¸Šä¸€æ¬¡è®¡ç®—çš„ç›¸ä¼¼æ€§æ©ç 
        cached_sim_mask = None
        
        # ==================== 2. æ»šåŠ¨è®¡ç®— Alpha ====================
        print(f"\nğŸ“ˆ æ­¥éª¤2/4: æ»šåŠ¨è®¡ç®— Alpha ({n_days - lookback_window:,} ä¸ªæœ‰æ•ˆæ—¥æœŸ)...")
        
        # ä» lookback_window å¼€å§‹éå†
        for i in tqdm(range(lookback_window, n_days), desc="è®¡ç®—Alpha", unit="å¤©", mininterval=0.5):
            
            # --- A. è·å–å½“æ—¥æ•°æ® ---
            # å½“æ—¥æ”¶ç›Šç‡å‘é‡ shape: (n_stocks, )
            curr_ret = ret_values[i, :]
            
            # æ£€æŸ¥å½“æ—¥æ˜¯å¦æœ‰æœ‰æ•ˆæ•°æ®ï¼ˆå¦‚æœå½“å¤©å…¨åœç‰Œï¼Œç›´æ¥è·³è¿‡ï¼‰
            valid_mask = ~np.isnan(curr_ret)
            if not np.any(valid_mask):
                continue
            
            # --- B. æ™ºèƒ½æ›´æ–°ç›¸å…³æ€§çŸ©é˜µï¼ˆé™é¢‘æ›´æ–°é€»è¾‘ï¼‰---
            # åªæœ‰åœ¨ä»¥ä¸‹æƒ…å†µæ‰é‡æ–°è®¡ç®—ç›¸å…³æ€§ï¼š
            # 1. ç¬¬ä¸€æ¬¡è®¡ç®— (cached_sim_mask is None)
            # 2. è¾¾åˆ°äº†æ›´æ–°é—´éš” ((i - lookback_window) % recalc_interval == 0)
            should_recalc = (cached_sim_mask is None) or ((i - lookback_window) % recalc_interval == 0)
            
            if should_recalc:
                # è·å–å†å²çª—å£æ•°æ® shape: (window, n_stocks)
                hist_slice = ret_values[i - lookback_window : i, :]
                
                # ã€æ ¸å¿ƒä¼˜åŒ–ã€‘ä½¿ç”¨ Numpy è®¡ç®—ç›¸å…³æ€§
                # np.corrcoef ä¸æ”¯æŒ NaNï¼Œå¿…é¡»å…ˆå¡«å……
                # å¯¹äºæ”¶ç›Šç‡ç›¸å…³æ€§ï¼Œç¼ºå¤±å€¼å¡« 0ï¼ˆä»£è¡¨æ— æ³¢åŠ¨/æ— ç›¸å…³ï¼‰æ˜¯ä¸šç•Œå¸¸ç”¨åšæ³•
                hist_slice_filled = np.nan_to_num(hist_slice, nan=0.0)
                
                # è®¡ç®—ç›¸å…³æ€§çŸ©é˜µ (n_stocks, n_stocks)
                # rowvar=False è¡¨ç¤ºæ¯ä¸€åˆ—æ˜¯ä¸€ä¸ªå˜é‡ï¼ˆè‚¡ç¥¨ï¼‰
                # è¿™ä¸€æ­¥æ¯” pandas.corr å¿« 5-10 å€ï¼
                with np.errstate(invalid='ignore'):
                    corr_matrix = np.corrcoef(hist_slice_filled, rowvar=False)
                
                # ç”Ÿæˆç›¸ä¼¼æ€§æ©ç  (0/1 çŸ©é˜µ)
                with np.errstate(invalid='ignore'):
                    cached_sim_mask = (corr_matrix >= similarity_threshold).astype(np.float32)
                
                # æ’é™¤è‡ªèº«ï¼ˆå¯¹è§’çº¿ç½® 0ï¼‰
                np.fill_diagonal(cached_sim_mask, 0)
            
            # --- C. çŸ©é˜µåŒ–è®¡ç®— SimStock Benchmarkï¼ˆå¤ç”¨ cached_sim_maskï¼‰---
            
            # 1. å¤„ç†å½“æ—¥åœç‰Œå¯¹é½é—®é¢˜
            # å¦‚æœæŸåªè‚¡ç¥¨ä»Šå¤©æ²¡äº¤æ˜“ï¼ˆNaNï¼‰ï¼Œå®ƒä¸èƒ½ä½œä¸ºåˆ«äººçš„åŸºå‡†
            # ä¸´æ—¶æ©ç  = ç¼“å­˜çš„é•¿æœŸç›¸ä¼¼å…³ç³» Ã— å½“å¤©å®é™…äº¤æ˜“çŠ¶æ€
            valid_mask_2d = valid_mask.reshape(1, -1)
            current_step_mask = cached_sim_mask * valid_mask_2d
            
            # 2. è®¡ç®—åˆ†å­ï¼šç›¸ä¼¼è‚¡ç¥¨æ”¶ç›Šç‡ä¹‹å’Œ
            # (N, N) @ (N, ) -> (N, )
            # å°† curr_ret ä¸­çš„ NaN æ¢æˆ 0 é¿å…æ±¡æŸ“çŸ©é˜µä¹˜æ³•
            curr_ret_safe = np.nan_to_num(curr_ret, 0.0)
            sum_ret = current_step_mask @ curr_ret_safe
            
            # 3. è®¡ç®—åˆ†æ¯ï¼šç›¸ä¼¼è‚¡ç¥¨æ•°é‡
            # (N, N) æŒ‰è¡Œæ±‚å’Œ -> (N, )
            count_sim = current_step_mask.sum(axis=1)
            
            # 4. è®¡ç®—åŸºå‡†å€¼ Benchmark
            # é»˜è®¤åŸºå‡†ï¼šå…¨å¸‚åœºå‡å€¼ï¼ˆé™çº§ç­–ç•¥ï¼‰
            market_mean = np.nanmean(curr_ret)
            
            # è®¡ç®—å¹³å‡ç›¸ä¼¼è‚¡ç¥¨æ”¶ç›Šç‡
            with np.errstate(divide='ignore', invalid='ignore'):
                benchmark = sum_ret / count_sim
            
            # 5. å¡«å……æ— æ•ˆå€¼
            # å¦‚æœ count_sim == 0ï¼ˆæ²¡æœ‰ç›¸ä¼¼è‚¡ç¥¨ï¼‰ï¼Œä½¿ç”¨ market_mean
            benchmark = np.where(count_sim == 0, market_mean, benchmark)
            # å¦‚æœ benchmark æ˜¯ NaNï¼ˆæ¯”å¦‚ç›¸ä¼¼è‚¡ç¥¨éƒ½åœç‰Œï¼‰ï¼Œä¹Ÿç”¨ market_mean
            benchmark = np.where(np.isnan(benchmark), market_mean, benchmark)
            
            # --- D. è®¡ç®— Alpha å¹¶å­˜å…¥çŸ©é˜µ ---
            # Alpha = åŸå§‹æ”¶ç›Š - åŸºå‡†
            alpha = curr_ret - benchmark
            
            # å­˜å…¥ç»“æœçŸ©é˜µçš„ç¬¬ i è¡Œ
            alpha_matrix[i, :] = alpha
        
        # ==================== 3. è¿˜åŸä¸ºé•¿è¡¨ ====================
        print(f"\nğŸ“¦ æ­¥éª¤3/4: å †å ç»“æœå›é•¿è¡¨ (Stack)...")
        
        # å°† alpha_matrix è½¬å› DataFrame
        alpha_df = pd.DataFrame(alpha_matrix, index=dates, columns=stocks)
        
        # Stack ä¸º Series: index=(trade_date, order_book_id)
        alpha_series = alpha_df.stack(dropna=False)
        alpha_series.name = output_column
        
        # ==================== 4. åˆå¹¶å›åŸæ•°æ® ====================
        print(f"ğŸ”— æ­¥éª¤4/4: åˆå¹¶å›åŸå§‹æ•°æ®...")
        
        # ç¡®ä¿åŸ df æœ‰æ­£ç¡®çš„ç´¢å¼•
        df_indexed = df.set_index(['trade_date', 'order_book_id'])
        
        # åˆå¹¶ï¼ˆä½¿ç”¨ left join ä¿ç•™åŸæ•°æ®çš„æ‰€æœ‰è¡Œï¼‰
        result_df = df_indexed.join(alpha_series, how='left')
        
        # æ¢å¤ç´¢å¼•
        result_df = result_df.reset_index()
        
        # ç»Ÿè®¡ä¿¡æ¯
        valid_alpha = result_df[output_column].count()
        total_rows = len(result_df)
        print(f"\nâœ… SimStock ä¸­æ€§åŒ–å®Œæˆ!")
        print(f"  è¾“å‡ºåˆ—: {output_column}")
        print(f"  æœ‰æ•ˆæ ·æœ¬: {valid_alpha:,} / {total_rows:,} ({100*valid_alpha/total_rows:.1f}%)")
        print(f"  Alphaå‡å€¼: {result_df[output_column].mean():.6f}")
        print(f"  Alphaæ ‡å‡†å·®: {result_df[output_column].std():.6f}")
        print(f"{'='*80}\n")
        
        return result_df
    
    # åˆ«åæ–¹æ³•ï¼ˆå‘åå…¼å®¹ï¼‰
    def simstock_neutralize(self, *args, **kwargs):
        """simstock_label_neutralize çš„åˆ«åæ–¹æ³•ï¼ˆå‘åå…¼å®¹ï¼‰"""
        return self.simstock_label_neutralize(*args, **kwargs)
