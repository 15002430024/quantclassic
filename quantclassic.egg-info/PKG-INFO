Metadata-Version: 2.4
Name: quantclassic
Version: 1.0.0
Summary: QuantClassic - é‡åŒ–å› å­ä¸æ¨¡å‹è®­ç»ƒæ¡†æ¶
Author: QuantClassic Team
License: MIT
Keywords: quantitative,finance,machine-learning,deep-learning
Classifier: Development Status :: 4 - Beta
Classifier: Intended Audience :: Financial and Insurance Industry
Classifier: Programming Language :: Python :: 3
Classifier: Programming Language :: Python :: 3.8
Classifier: Programming Language :: Python :: 3.9
Classifier: Programming Language :: Python :: 3.10
Classifier: Programming Language :: Python :: 3.11
Requires-Python: >=3.8
Description-Content-Type: text/markdown
Requires-Dist: numpy>=1.20.0
Requires-Dist: pandas>=1.3.0
Requires-Dist: pyyaml>=5.4
Requires-Dist: dataclasses-json>=0.5.0
Provides-Extra: torch
Requires-Dist: torch>=2.0.0; extra == "torch"
Provides-Extra: dev
Requires-Dist: pytest>=7.0.0; extra == "dev"
Requires-Dist: pytest-cov>=4.0.0; extra == "dev"
Provides-Extra: all
Requires-Dist: torch>=2.0.0; extra == "all"
Requires-Dist: pytest>=7.0.0; extra == "all"
Requires-Dist: pytest-cov>=4.0.0; extra == "all"



# ğŸ“Š QuantClassic æ¶æ„æ–‡æ¡£

### å•ä¸€æ•°æ®æº Â· é…ç½®é©±åŠ¨çš„ç«¯åˆ°ç«¯é‡åŒ–ç ”ç©¶ä¸å›æµ‹æµæ°´çº¿

[![Status](https://img.shields.io/badge/status-active-success.svg)]()
[![Documentation](https://img.shields.io/badge/docs-up--to--date-blue.svg)]()
[![Python](https://img.shields.io/badge/python-3.8%2B-blue.svg)]()

</div>

---

## ğŸ“– æ–‡æ¡£å¯¼è¯»

> ğŸ’¡ **æ ¸å¿ƒç›®æ ‡**ï¼šè®©æ–°äººå¿«é€Ÿç†è§£ç«¯åˆ°ç«¯é“¾è·¯ï¼Œé¿å…ç ´åæ•°æ®/é…ç½® Schemaï¼Œå¹¶ä¾¿äºæ’æŸ¥é—®é¢˜

| æ¿å— | è¯´æ˜ |
|------|------|
| ğŸ¯ **é€‚ç”¨åœºæ™¯** | æ–°å¢æˆ–è°ƒæ•´æ•°æ®æºã€é¢„å¤„ç†ã€æ•°æ®é›†ã€æ¨¡å‹ã€å›æµ‹ã€å®éªŒè¿½è¸ªç­‰å…¨é“¾è·¯å¼€å‘ |
| ğŸ—ºï¸ **å¿«é€Ÿå¯¼èˆª** | [ç³»ç»Ÿæ¶æ„](#1-ç³»ç»Ÿæ¶æ„) Â· [æ•°æ®æµ](#2-æ•°æ®æµ) Â· [æ¨¡å—è¯¦æƒ…](#3-æ¨¡å—è¯¦æƒ…) Â· [ä¿®æ”¹æŒ‡å—](#4-ä¿®æ”¹æŒ‡å—) Â· [ç‰ˆæœ¬å†å²](#5-ç‰ˆæœ¬å†å²) |
| âš ï¸ **æ ¸å¿ƒçº¦æŸ** | å•ä¸€æ•°æ®æºã€é…ç½®é©±åŠ¨(YAML/CLI)ï¼Œè®­ç»ƒ/å›æµ‹å…¨ç¨‹é€šè¿‡ workflow.R è®°å½•äº§ç‰© |
| ğŸ“¦ **å…³é”®äº§ç‰©** | `features_raw.*` Â· `feature_columns.txt` Â· `preprocessor.pkl` Â· `output/experiments/*` |

---

## ğŸ—ï¸ 1. ç³»ç»Ÿæ¶æ„

```mermaid
flowchart LR
    subgraph Orchestration["ç¼–æ’å±‚"]
        cfg["config/\nConfigLoader + TaskRunner + CLI(qcrun)"]
        wf["workflow/\nQCRecorder (R) + ExpManager"]
    end

    subgraph Data["æ•°æ®å±‚"]
        fetch["data_fetch/\nQuantDataPipeline\n(ConfigManager, DataFetcher,\nDataProcessor, DataValidator, ResumeManager)"]
        monitor["data_monitor/\né™æ€/åŠ¨æ€æ³„æ¼æ£€æµ‹"]
    end

    subgraph Feature["ç‰¹å¾ä¸æ•°æ®é›†"]
        prep["data_processor/\nDataPreprocessor + FeatureProcessor"]
        dm["data_set/\nDataManager + Loader/Factory + Splitter"]
    end

    subgraph Modeling["å»ºæ¨¡ä¸è¯„ä¼°"]
        mdl["model/\nModelFactory + PyTorchModel ç³»åˆ— + Trainers"]
        fh["factor_hub/\nFactorRegistry + BaseFactor"]
        bt["backtest/\nFactorBacktestSystem\n(Generator/Processor/IC/Portfolio/Perf/Visualizer)"]
    end

    out["output/ & cache/\nç‰¹å¾ã€æ¨¡å‹ã€å›æµ‹ã€å®éªŒè®°å½•"]

    cfg --> fetch
    fetch --> prep
    prep --> dm
    dm --> mdl
    monitor --> prep
    monitor --> dm
    mdl --> bt
    fh --> bt
    cfg -. log params/metrics .-> wf
    mdl -. train metrics .-> wf
    bt -. bt metrics .-> wf
    wf --> out
    bt --> out
    dm --> out
```

---

## ğŸ”„ 2. æ•°æ®æµ

```mermaid
flowchart LR
    raw[(å¤–éƒ¨è¡Œæƒ…/åŸºæœ¬é¢æº\nClickHouse/ç¼“å­˜æ–‡ä»¶)]
    basic[åŸºç¡€æ•°æ®è·å–\nè‚¡ç¥¨åˆ—è¡¨/äº¤æ˜“æ—¥å†/è¡Œä¸š]
    daily[æ—¥é¢‘æ•°æ®è·å–\nä»·æ ¼/ä¼°å€¼/è‚¡æœ¬]
    merge[æ•°æ®åˆå¹¶\nåŸºç¡€å­—æ®µè®¡ç®—]
    feat[ç‰¹å¾å·¥ç¨‹\nfactor & ç»Ÿè®¡ç‰¹å¾]
    quality[æ•°æ®éªŒè¯\nç¼ºå¤±/å¼‚å¸¸/ä¸€è‡´æ€§]
    store[è½åœ° features_raw.*\nfeature_columns.txt]
    preprocess[é¢„å¤„ç†ä¸ä¸­æ€§åŒ–\nwinsorize/z-score/rank]
    split[æ—¶é—´/æ»šåŠ¨åˆ’åˆ†\ntrain/val/test]
    loaders[DataLoader æ„å»º\nå¸¸è§„/æ»šåŠ¨/æ—¥æ‰¹æ¬¡/å›¾æ•°æ®]
    train[æ¨¡å‹è®­ç»ƒ\næ—©åœ/æ»šåŠ¨/åŠ¨æ€å›¾]
    predict[é¢„æµ‹/æ½œåœ¨å› å­è¾“å‡º]
    backtest[å›æµ‹ä¸ICåˆ†æ\nç»„åˆæ„å»º/ç»©æ•ˆè¯„ä¼°/å¯è§†åŒ–]
    record[å®éªŒè®°å½•\nworkflow.R ä¿å­˜å‚æ•°/æŒ‡æ ‡/å¯¹è±¡]
    reports[è¾“å‡ºäº§ç‰©\næ¨¡å‹ã€æŒ‡æ ‡ã€å›¾è¡¨ã€æŠ¥å‘Šã€ç¼“å­˜]

    raw --> basic --> daily --> merge --> feat --> quality --> store
    store --> preprocess --> split --> loaders --> train --> predict --> backtest --> record --> reports
```

## 3. æ¨¡å—è¯¦æƒ…

| æ¨¡å— | èŒè´£ | å…³é”®ç»„ä»¶/ç±» | è¾“å…¥ | è¾“å‡º |
|------|------|-------------|------|------|
| `config/` | YAML/CLI é©±åŠ¨ç«¯åˆ°ç«¯ä»»åŠ¡ï¼Œç»„è£…æ•°æ®ã€æ¨¡å‹ã€å›æµ‹å¹¶å¯¹æ¥ workflow | `ConfigLoader`, `TaskRunner`, `cli.py (qcrun)`, `TaskConfig` | `task` é…ç½®ã€æ¨¡æ¿ | åˆå§‹åŒ–çš„ `DataManager`/æ¨¡å‹ã€è®­ç»ƒ/å›æµ‹ç»“æœï¼Œè‡ªåŠ¨è®°å½•åˆ° `workflow` |
| `data_fetch/` | æŠ“å–å¹¶ç¼“å­˜è¡Œæƒ…/åŸºæœ¬é¢ï¼Œæ„å»ºåŸºç¡€ç‰¹å¾çŸ©é˜µ | `ConfigManager`, `QuantDataPipeline`, `DataFetcher`, `DataProcessor`, `DataValidator`, `ResumeManager` | æ—¶é—´æ®µã€è‚¡ç¥¨æ± ã€å­—æ®µä¸å­˜å‚¨é…ç½® | `features_raw.*`ã€`feature_columns.txt`ã€æ•°æ®è´¨é‡æŠ¥å‘Š |
| `data_processor/` | å› å­/ç‰¹å¾é¢„å¤„ç†ä¸ä¸­æ€§åŒ– | `PreprocessConfig`, `ProcessingStep`, `DataPreprocessor`, `FeatureProcessor` | åŸå§‹æˆ–æŠ“å–åçš„ç‰¹å¾åˆ— | å½’ä¸€åŒ–/å»æå€¼/ä¸­æ€§åŒ–åçš„ç‰¹å¾ï¼ŒæŒä¹…åŒ–çš„é¢„å¤„ç†å™¨çŠ¶æ€ |
| `data_set/` | è®­ç»ƒæ•°æ®ç®¡ç†ä¸åˆ‡åˆ†ï¼Œæä¾› DataLoader | `DataManager`, `FeatureEngineer`, `DataLoaderEngine`, `create_splitter`, `DatasetFactory`, `LoaderCollection` | é¢„å¤„ç†ç‰¹å¾ã€æ ‡ç­¾åˆ—ã€åˆ’åˆ†ç­–ç•¥ | è®­ç»ƒ/éªŒè¯/æµ‹è¯• DataFrameï¼Œå¸¸è§„/æ»šåŠ¨/æ—¥æ‰¹æ¬¡/å›¾ DataLoaderï¼Œç‰¹å¾ç»Ÿè®¡ |
| `data_monitor/` | æ•°æ®æ³„æ¼æ£€æµ‹ä¸è´¨é‡ç›‘æ§ | `StaticLeakageDetector`, `DynamicLeakageDetector`, `leakage_detection_config.py` | æ•°æ®å¸§ã€æ—¶é—´/åˆ†ç»„å­—æ®µ | æ³„æ¼ä¸è´¨é‡æŠ¥å‘Šï¼Œè­¦å‘Šæ—¥å¿— |
| `factor_hub/` | å› å­åè®®ä¸æ³¨å†Œä¸­å¿ƒ | `StandardDataProtocol`, `FactorRegistry`, `BaseFactor`, providers/writers | æ ‡å‡†åŒ–è¡Œæƒ…æ•°æ® | å¯è°ƒç”¨çš„å› å­é›†åˆã€å¯¼å‡ºå†™å…¥å™¨äº§ç‰© |
| `model/` | æ¨¡å‹å®šä¹‰ä¸è®­ç»ƒå·¥å…·é“¾ | `ModelFactory`, `PyTorchModel` ç³»åˆ—ã€`rolling_daily_trainer.py`, `dynamic_graph_trainer.py`, `model_config.py` | DataLoader/å›¾æ•°æ®ã€æ¨¡å‹é…ç½® | è®­ç»ƒå¥½çš„æ¨¡å‹ã€é¢„æµ‹å€¼/æ½œåœ¨å› å­ã€æœ€ä½³æŒ‡æ ‡ã€æ¨¡å‹æ–‡ä»¶ |
| `backtest/` | å› å­/ç­–ç•¥å›æµ‹ä¸å¯è§†åŒ– | `FactorBacktestSystem`, `FactorGenerator`, `FactorProcessor`, `ICAnalyzer`, `PortfolioBuilder`, `PerformanceEvaluator`, `ResultVisualizer` | å› å­/é¢„æµ‹å€¼ã€ä»·æ ¼æ•°æ®ã€å›æµ‹é…ç½® | ç»©æ•ˆæŒ‡æ ‡ã€ICç»Ÿè®¡ã€å¤šç©ºç»„åˆæ”¶ç›Šã€å›¾è¡¨ |
| `workflow/` | å®éªŒè¿½è¸ªä¸å¯¹è±¡ç®¡ç† | `QCRecorder (R)`, `ExpManager`, `Experiment`, `Recorder`, `recorder.py` | è®­ç»ƒ/å›æµ‹æœŸé—´çš„å‚æ•°ã€æŒ‡æ ‡ã€å¯¹è±¡ | `output/experiments` ä¸‹çš„ params/metrics/objects ç´¢å¼•ä¸å­˜æ¡£ |
| `output/` `cache/` | ç»“æœä¸ç¼“å­˜è½åœ° | æ–‡ä»¶ç³»ç»Ÿç›®å½• | å„é˜¶æ®µäº§ç‰© | å¤ç°æ‰€éœ€çš„æ¨¡å‹ã€æ•°æ®ã€æŠ¥å‘Šã€ç¼“å­˜ |


### 3.0 å¸¸ç”¨è°ƒç”¨è·¯å¾„

- é…ç½®ï¼š`config/ConfigLoader` è§£æ YAML/CLI ç”Ÿæˆ `TaskConfig`
- æ•°æ®ï¼š`data_fetch.QuantDataPipeline.run_full_pipeline` äº§å‡º features_raw.* ä¸ feature_columns.txt
- é¢„å¤„ç†ï¼š`data_processor.DataPreprocessor.fit_transform` ç”Ÿæˆé¢„å¤„ç†æ•°æ®ä¸ `preprocessor.pkl`
- æ•°æ®é›†ï¼š`data_set.DataManager.create_datasets` + `get_dataloaders` æ„å»º Dataset/DataLoaderï¼ˆå«æ»šåŠ¨/åŠ¨æ€å›¾ï¼‰
- è®­ç»ƒä¸é¢„æµ‹ï¼š`ModelFactory` åˆ›å»ºæ¨¡å‹ï¼Œ`PyTorchModel`/`RollingDailyTrainer`/`DynamicGraphTrainer` è®­ç»ƒå¹¶è¾“å‡ºé¢„æµ‹/æ½œåœ¨å› å­
- å›æµ‹ä¸è®°å½•ï¼šå¯é€‰ `backtest.run_backtest`ï¼Œæœ€ç»ˆé€šè¿‡ `workflow.R` å†™å…¥ `output/experiments`

---

### 3.1 config/TaskRunnerï¼ˆç¼–æ’å…¥å£ï¼‰

| å±æ€§ | å€¼ |
|------|-----|
| è·¯å¾„ | `config/runner.py`, `config/cli.py` |
| èŒè´£ | è§£æ YAML/TaskConfigï¼Œåˆå§‹åŒ–æ•°æ®ã€æ¨¡å‹ï¼Œå¯é€‰å›æµ‹ï¼Œå¹¶é€šè¿‡ `workflow.R` è®°å½•å®éªŒ |
| ä¾èµ– | `data_set`, `model`, `backtest`, `workflow` |

**è¾“å…¥Schema**:
```python
TaskConfig | Dict{
    task: {
        model: {class: str, module_path: str, kwargs: dict},
        dataset: {class: str, kwargs: {config: DataConfig|dict}},
        backtest?: dict,
        trainer_class?: Literal["RollingDailyTrainer","DynamicGraphTrainer"],
        trainer_kwargs?: dict
    },
    experiment_name?: str,
    recorder_name?: str
}
```

**è¾“å‡ºSchema**:
```python
Dict[
    model: Model,
    dataset: LoaderCollection|Any,
    data_manager: DataManager|None,
    rolling_loaders: Optional[RollingDailyLoaderCollection],
    daily_loaders: Optional[DailyLoaderCollection],
    train_results: Dict[str, Any],
    backtest_results: Dict[str, Any],
    experiment_name: str
]
```

**æ ¸å¿ƒé€»è¾‘**:
```python
config_dict = _adapt_task_config_to_legacy(config)
dataset, dm = self._init_dataset(config_dict["task"]["dataset"])
model = self._init_model(config_dict["task"]["model"])
train_results = self._train_model(model, dataset, config_dict["task"])
backtest_results = self._run_backtest(model, dataset, config_dict["task"].get("backtest", {}))
R.log_params(...); R.log_metrics(...); R.save_objects(model=model, results=results)
```

**ä¸»è¦å‡½æ•°**:
- `run(config: Union[Dict, TaskConfig], experiment_name: str, recorder_name: str=None) -> Dict`
- `_init_dataset(dataset_config: Dict|BaseConfig) -> Tuple[Any, DataManager|None]`
- `_init_model(model_config: Dict|BaseConfig) -> Any`
- `_train_model(model, dataset, task_cfg: Dict) -> Dict`
- `_train_rolling(model, rolling_loaders, trainer_kwargs: Dict) -> Dict`
- `_train_dynamic_graph(model, daily_loaders, trainer_kwargs: Dict) -> Dict`
- `_run_backtest(model, dataset, backtest_cfg: Dict) -> Dict`

---

### 3.2 data_fetch/QuantDataPipelineï¼ˆæ•°æ®è·å–ï¼‰

| å±æ€§ | å€¼ |
|------|-----|
| è·¯å¾„ | `data_fetch/pipeline.py` |
| èŒè´£ | æŒ‰é…ç½®æŠ“å–åŸºç¡€/æ—¥é¢‘æ•°æ®ï¼Œåˆå¹¶è¡Œä¸šä¸ä¼°å€¼ï¼Œæ„å»ºç‰¹å¾å¹¶æ ¡éªŒï¼Œè½åœ° `features_raw.*` |
| ä¾èµ– | `DataFetcher`, `DataProcessor`, `DataValidator`, `ResumeManager`, `ConfigManager` |

**è¾“å…¥Schema**:
```python
ConfigManager(
    time.start_date: str,
    time.end_date: str,
    universe.universe_type: Literal["index","custom","all"],
    universe.custom_stocks?: List[str],
    fields.price: List[str], fields.valuation: List[str], fields.share: List[str],
    storage.save_dir: str, storage.file_format: Literal["parquet","csv","hdf5"]
)
```

**è¾“å‡ºSchema**:
```python
DataFrame[
    order_book_id: str,
    trade_date: datetime64,
    open: float, high: float, low: float, close: float,
    vol: float, amount: float, vwap?: float,
    pct_chg: float, pre_close: float, amplitude: float,
    turnover_rate?: float, volume_ratio: float,
    ret_{1,5,10,20}d: float, vol_{20}d: float,
    ma_close_{5,20,60}d: float, ma_vol_{5,20,60}d: float,
    close_lag_{1,5,10,20}: float, ret_lag_{1,5,10,20}: float,
    close_to_ma{5,20}_lag_1: float, momentum_lag_1_{5,10}: float,
    price_position_20d: float, amount_ratio: float,
    industry?: str, limit_up?: float, limit_down?: float, is_limit_up?: int, is_limit_down?: int
]
```

**æ ¸å¿ƒé€»è¾‘**:
```python
def run_full_pipeline(steps=None, save_intermediate=True, validate=True):
    _fetch_basic_data()   # è‚¡ç¥¨åˆ—è¡¨/äº¤æ˜“æ—¥å†/è¡Œä¸š
    _fetch_daily_data()   # ä»·æ ¼/ä¼°å€¼/è‚¡æœ¬ (æ–­ç‚¹ç»­ä¼ )
    _merge_data()         # åˆå¹¶å¹¶ç®— pct_chg/pre_close/turnover_rate/volume_ratio
    _build_features()     # æŠ€æœ¯æŒ‡æ ‡/æ»å/è¡ç”Ÿ
    _validate_data()      # DataValidator.run_full_validation
    _save_final_data()    # features_raw.{fmt} + feature_columns.txt
```

**ä¸»è¦å‡½æ•°**:
- `run_full_pipeline(steps: List[str]=None, save_intermediate: bool=True, validate: bool=True) -> pd.DataFrame`
- `run_incremental_update(update_date: str) -> None`
- `run_custom_universe(custom_stocks: List[str]) -> None`
- `_merge_data(price, valuation, share) -> pd.DataFrame`
- `_build_features(df) -> pd.DataFrame`

---

### 3.3 data_processor/DataPreprocessorï¼ˆé¢„å¤„ç†ä¸ä¸­æ€§åŒ–ï¼‰

| å±æ€§ | å€¼ |
|------|-----|
| è·¯å¾„ | `data_processor/data_preprocessor.py`, `data_processor/preprocess_config.py`, `data_processor/feature_processor.py`, `data_processor/label_generator.py` |
| èŒè´£ | æŒ‰é…ç½®ä¸²è¡Œæ‰§è¡Œå»æå€¼ã€ç¼ºå¤±å€¼å¡«å……ã€æ ‡å‡†åŒ–/å½’ä¸€åŒ–ã€è¡Œä¸š/å¸‚å€¼ä¸­æ€§åŒ–ã€æ ‡ç­¾ç”Ÿæˆä¸ SimStock æ ‡ç­¾ä¸­æ€§åŒ–ï¼›æ”¯æŒè®­ç»ƒ/æ¨ç†åˆ†ç¦»å’ŒçŠ¶æ€æŒä¹…åŒ– |
| ä¾èµ– | `PreprocessConfig`, `ProcessingStep`, `FeatureProcessor`, `LabelGenerator`, `BaseConfig` |

**æ¨¡å—ç»“æ„**:
```mermaid
flowchart LR
    cfg[PreprocessConfig\n(pipeline_steps/id/groupby/column_mapping\nlabel_config/neutralize_config)] --> dp[DataPreprocessor\nfit/transform/save/load]
    dp --> fp[FeatureProcessor\nz-score/minmax/rank\nwinsorize/clip\nfillna/neutralize]
    dp --> lg[LabelGenerator\nmulti-period labels]
    dp --> state[preprocessor.pkl\nconfig + fitted_params + feature_columns]
```

**è¾“å…¥Schema**:
```python
DataFrame[
    trade_date: datetime64,
    order_book_id: str,
    ts_code?: str,
    industry_name?: str,
    total_mv?: float,
    feature_*: float,          # è‡ªåŠ¨æ¨æ–­: æ•°å€¼åˆ—ï¼Œæ’é™¤ id/industry/label å‰ç¼€
    y_ret_*?: float,           # ç”Ÿæˆ/ä½¿ç”¨çš„æ ‡ç­¾
    target_column?: float      # SimStock ä¸­æ€§åŒ–æ‰€éœ€
]
PreprocessConfig(
    pipeline_steps: List[ProcessingStep],
    id_columns: List[str]=['order_book_id','trade_date'],
    groupby_columns: List[str]=['trade_date'],
    column_mapping: Dict[str,str],
    label_config: LabelGeneratorConfig,
    neutralize_config: NeutralizeConfig
)
```

**è¾“å‡ºSchema**:
```python
DataFrame[
    trade_date: datetime64,
    order_book_id: str,
    feature_*: float,          # ç» winsorize/fillna/neutralize/normalize å
    y_ret_*?: float,           # ç”Ÿæˆçš„æ ‡ç­¾
    alpha_label?: float        # SimStock æ ‡ç­¾ä¸­æ€§åŒ–è¾“å‡º
]
preprocessor.pkl  # ä¿å­˜ config + FeatureProcessor.fitted_params + feature_columns + id_columns
```

**æ ¸å¿ƒé€»è¾‘**:
```python
def fit_transform(df, feature_columns=None, target_column=None):
    features = feature_columns or infer_numeric_features(df, exclude=id/industry/label patterns)
    id_cols = [c for c in config.id_columns if c in df] or ['order_book_id','trade_date']
    for step in enabled(config.pipeline_steps):
        required = _get_required_columns(step.method, step.params)
        if missing(required, df): continue
        process_features = _get_process_features(step, features, df)
        if not process_features and step.method not in {GENERATE_LABELS, SIMSTOCK_LABEL_NEUTRALIZE}: continue
        df = feature_processor.handle_infinite_values(df, process_features)
        dispatch(step.method):
            GENERATE_LABELS -> LabelGenerator(label_config|params).generate_labels(...)
            WINSORIZE/CLIP -> FeatureProcessor.winsorize_features|clip_features
            FILLNA_* -> FeatureProcessor.handle_missing_values(industry-aware)
            Z_SCORE/MINMAX/RANK -> FeatureProcessor.(z_score|minmax|rank)_normalize(save params when fit)
            OLS_NEUTRALIZE/MEAN_NEUTRALIZE -> FeatureProcessor.industry_cap_neutralize_ols|mean
            SIMSTOCK_LABEL_NEUTRALIZE -> FeatureProcessor.simstock_label_neutralize(target_column required)
    is_fitted = True
```

**ä¸»è¦å‡½æ•°**:
- `DataPreprocessor.fit_transform(df: pd.DataFrame, feature_columns: List[str]=None, target_column: str=None) -> pd.DataFrame`
- `DataPreprocessor.transform(df: pd.DataFrame, target_column: str=None) -> pd.DataFrame`ï¼ˆå¤ç”¨å·²ä¿å­˜çš„ç»Ÿè®¡é‡ï¼‰
- `DataPreprocessor.save(path: str) -> None`, `DataPreprocessor.load(path: str) -> DataPreprocessor`
- `DataPreprocessor.get_pipeline_summary() -> pd.DataFrame`, `validate_data(df: pd.DataFrame) -> Dict`
- `PreprocessConfig.add_step(name: str, method: Union[str, ProcessMethod], features: Union[str,List[str],None]=None, enabled: bool=True, **params) -> PreprocessConfig`
- é¢„è®¾æ¨¡æ¿ï¼š`PreprocessTemplates.basic_pipeline() | advanced_pipeline() | alpha_pipeline()`

---

### 3.4 data_set/DataManagerï¼ˆæ•°æ®é›†ä¸åŠ è½½å™¨ï¼‰

| å±æ€§ | å€¼ |
|------|-----|
| è·¯å¾„ | `data_set/manager.py` |
| èŒè´£ | åŠ è½½é¢„å¤„ç†åæ•°æ®ã€ç‰¹å¾é€‰æ‹©ã€æ•°æ®åˆ’åˆ†ã€æ„å»º Dataset/DataLoaderï¼ˆå«æ»šåŠ¨ä¸åŠ¨æ€å›¾æ¨¡å¼ï¼‰ |
| ä¾èµ– | `DataConfig`, `DataLoaderEngine`, `FeatureEngineer`, `DatasetFactory`, `splitter` |

**è¾“å…¥Schema**:
```python
DataFrame[
    trade_date: datetime64,
    ts_code: str,                    # stock_col
    y_processed: float,              # label_col
    feature_*: float,
    industry_name?: str,
    total_mv?: float
]
DataConfig(
    window_size: int,
    split_strategy: Literal["time_series","rolling","random","stratified"],
    train_ratio/val_ratio/test_ratio: float,
    batch_size: int,
    graph_builder_config?: Dict
)
```

**è¾“å‡ºSchema**:
```python
DatasetCollection(
    train: TimeSeriesStockDataset,
    val: TimeSeriesStockDataset,
    test: TimeSeriesStockDataset|DailyBatchDataset
)
LoaderCollection(
    train: DataLoader[Tensor(batch, window, features), Tensor(batch)],
    val: DataLoader,
    test: DataLoader
)
rolling_windows?: List[Tuple[pd.DataFrame, pd.DataFrame]]
```

**æ ¸å¿ƒé€»è¾‘**:
```python
raw = load_raw_data()
feature_cols = preprocess_features(raw)
train_df, val_df, test_df = splitter.split(df[label_non_na])
datasets = DatasetFactory.create_datasets(train_df, val_df, test_df, feature_cols,
    test_valid_label_start_date=calc_start_date_for_rolling())
loaders = datasets.get_loaders(batch_size=config.batch_size, num_workers=config.num_workers)
```

**ä¸»è¦å‡½æ•°**:
- `run_full_pipeline(file_path: str=None) -> LoaderCollection`
- `create_datasets(df: pd.DataFrame=None, feature_cols: List[str]=None, split_strategy: str=None) -> DatasetCollection`
- `get_dataloaders(batch_size: int=None, num_workers: int=None, shuffle_train: bool=None) -> LoaderCollection`
- `create_daily_loaders(graph_builder_config: Dict=None, shuffle_dates: bool=None, device: str='cuda')`
- `create_rolling_daily_loaders(graph_builder_config: Dict=None)`
- `save_state(path: str) / load_state(path: str)`

---

### 3.5 model/ModelFactory + Trainersï¼ˆæ¨¡å‹ä¸è®­ç»ƒï¼‰

| å±æ€§ | å€¼ |
|------|-----|
| è·¯å¾„ | `model/model_factory.py`, `model/pytorch_models.py`, `model/rolling_daily_trainer.py`, `model/dynamic_graph_trainer.py` |
| èŒè´£ | åˆ›å»ºå¹¶è®­ç»ƒ LSTM/GRU/Transformer/VAE ç­‰æ¨¡å‹ï¼Œæ”¯æŒæ»šåŠ¨çª—å£ä¸åŠ¨æ€å›¾è®­ç»ƒï¼Œç®¡ç†æ—©åœä¸æ¨¡å‹ä¿å­˜ |
| ä¾èµ– | `torch`, `DataManager` loaders, `workflow.R` |

**è¾“å…¥Schema**:
```python
DataLoader[
    X: FloatTensor[batch, window, n_features],
    y: FloatTensor[batch]
]
Model config dict:
{
  class: "LSTM"|"GRU"|"Transformer"|"VAE"|custom,
  kwargs: {d_feat:int, hidden_size:int, num_layers:int, dropout:float, n_epochs:int, lr:float, ...}
}
Rolling/Dynamic trainer kwargs (optional): {n_epochs:int, save_dir:str, warm_start:bool, ...}
```

**è¾“å‡ºSchema**:
```python
Dict[
    metrics: Dict[str, float],           # best_metrics if available
    predictions: np.ndarray|pd.DataFrame,
    model: TrainedModel,
    trainer?: RollingDailyTrainer|DynamicGraphTrainer,
    latent_features?: np.ndarray         # VAE
]
model_state.pth                         # ä¿å­˜çš„æœ€ä½³æ¨¡å‹
```

**æ ¸å¿ƒé€»è¾‘**:
```python
model = ModelFactory.create(model_cfg)
model.fit(train_loader, val_loader)            # æ—©åœ/æ¢¯åº¦è£å‰ª/è°ƒåº¦
if rolling: RollingDailyTrainer.fit(rolling_loaders, n_epochs, save_dir)
if dynamic: DynamicGraphTrainer.fit(daily_loaders.train, daily_loaders.val, n_epochs)
pred = model.predict(test_loader)
```

**ä¸»è¦å‡½æ•°**:
- `ModelFactory.create_model(config: Dict) -> Model`
- `PyTorchModel.fit(train_loader, valid_loader=None, save_path=None) -> Dict`
- `PyTorchModel.predict(loader, return_numpy=True) -> np.ndarray`
- `RollingDailyTrainer.fit(loaders, n_epochs: int, save_dir: str) -> Dict`
- `DynamicGraphTrainer.fit(train_loader, val_loader, n_epochs: int, save_path: str) -> Dict`

---

### 3.6 backtest/FactorBacktestSystemï¼ˆå›æµ‹ä¸è¯„ä¼°ï¼‰

| å±æ€§ | å€¼ |
|------|-----|
| è·¯å¾„ | `backtest/backtest_system.py` åŠå­æ¨¡å— |
| èŒè´£ | åŸºäºå› å­æˆ–æ¨¡å‹é¢„æµ‹æ„å»ºç»„åˆï¼Œè®¡ç®— IC/æ”¶ç›Š/é£é™©æŒ‡æ ‡å¹¶ç”Ÿæˆå¯è§†åŒ– |
| ä¾èµ– | `FactorGenerator`, `FactorProcessor`, `PortfolioBuilder`, `ICAnalyzer`, `PerformanceEvaluator`, `ResultVisualizer` |

**è¾“å…¥Schema**:
```python
DataFrame[
    trade_date: datetime64,
    order_book_id: str,
    factor|pred: float,            # å•å› å­æˆ–å¤šåˆ—å› å­
    label?: float,                 # å¯é€‰ï¼Œç”¨äºå¯¹ç…§
    price_fields?: float           # ä»·æ ¼/æƒé‡è®¡ç®—æ‰€éœ€
]
BacktestConfig(n_groups: int=10, rebalance_freq: str='monthly', weight_method: str='equal',
               industry_neutral: bool=False, consider_cost: bool=False, save_plots: bool=True)
```

**è¾“å‡ºSchema**:
```python
Dict[
    metrics: {
        ic_stats: {ic_mean: float, ic_ir: float, win_rate: float},
        performance_metrics: {
            long_short: {annual_return: float, sharpe_ratio: float, max_drawdown: float, vol: float},
            group_returns: Dict[int, float]
        }
    },
    plots: List[pathlib.Path]
]
```

**æ ¸å¿ƒé€»è¾‘**:
```python
factor_df = factor_generator.generate_factors(df)
processed = factor_processor.process(factor_df)
ic_df = ic_analyzer.calculate_ic(processed)
ports = portfolio_builder.build_portfolios(processed, n_groups=config.n_groups)
perf = performance_evaluator.evaluate_portfolio(ports["long_short"], cost=config.cost)
result_visualizer.save_all(ports, ic_df, perf, save_dir=config.output_dir)
```

**ä¸»è¦å‡½æ•°**:
- `run_backtest(predictions: pd.DataFrame=None, **cfg) -> Dict`
- `FactorGenerator.generate_factors(df: pd.DataFrame) -> pd.DataFrame`
- `ICAnalyzer.calculate_ic(df: pd.DataFrame) -> pd.DataFrame`
- `PerformanceEvaluator.evaluate_portfolio(df: pd.DataFrame, cost: float=0.0) -> Dict`

---

### 3.7 workflow/QCRecorderï¼ˆå®éªŒè¿½è¸ªï¼‰

| å±æ€§ | å€¼ |
|------|-----|
| è·¯å¾„ | `workflow/qc_recorder.py`, `workflow/recorder.py` |
| èŒè´£ | ç»Ÿä¸€çš„å®éªŒä¸Šä¸‹æ–‡ç®¡ç†ï¼Œè®°å½•å‚æ•°/æŒ‡æ ‡/å¯¹è±¡åˆ° `output/experiments` |
| ä¾èµ– | `ExpManager`, `Experiment`, `Recorder` |

**è¾“å…¥Schema**:
```python
experiment_name: str,
recorder_name?: str,
params: Dict[str, Any],
metrics: Dict[str, Union[int, float, Sequence]],
objects: Dict[str, Any]              # æ¨¡å‹/é…ç½®/é¢„æµ‹/å›¾è¡¨è·¯å¾„ç­‰
```

**è¾“å‡ºSchema**:
```python
output/experiments/{experiment}/
    index.json
    recorder_*/metadata.json
    recorder_*/params.json
    recorder_*/metrics.json
    recorder_*/objects/{model.pkl, config.pkl, plots/...}
```

**æ ¸å¿ƒé€»è¾‘**:
```python
with R.start(experiment_name, recorder_name, resume=False, **params):
    R.log_params(...)
    R.log_metrics(step=epoch, loss=loss, ic=ic)
    R.save_objects(model=model, config=config, predictions=pred_df)
```

**ä¸»è¦å‡½æ•°**:
- `R.start(experiment_name: str, recorder_name: str=None, resume: bool=False, **params)`
- `R.log_params(**params)`, `R.log_metrics(step: int=None, **metrics)`
- `R.save_objects(**objects)`, `R.load_object(experiment_name, recorder_id, object_name)`
- `R.list_experiments()`, `R.list_recorders(experiment_name)`, `R.search_recorders(...)`

## 4. ä¿®æ”¹æŒ‡å—
- **æ–°å¢/ä¿®æ”¹æ•°æ®æº**ï¼šä»…æ”¹ `data_fetch`ï¼Œä¿æŒè¾“å‡º Schemaï¼ˆä»·æ ¼/ä¼°å€¼/è‚¡æœ¬/è¡Œä¸šå­—æ®µï¼‰ç¨³å®šï¼›è‹¥å­—æ®µå˜æ›´ï¼ŒåŒæ­¥æ›´æ–° 3.2 è¾“å…¥/è¾“å‡º Schema ä¸ä¸‹æ¸¸ `data_processor`/`data_set`ã€‚
- **è°ƒæ•´é¢„å¤„ç†/ä¸­æ€§åŒ–**ï¼šä¿®æ”¹ `data_processor` ç®¡çº¿æˆ– `PreprocessConfig`ï¼›æ›´æ–° 3.3 æ ¸å¿ƒé€»è¾‘ä»£ç å—ä¸è¾“å‡º Schemaã€‚
- **æ•°æ®åˆ’åˆ†æˆ–çª—å£ç­–ç•¥**ï¼šåœ¨ `data_set` é‡Œå˜æ›´ `split_strategy`/`window_size`/`graph_builder_config`ï¼›åŒæ­¥ 3.4 è¾“å…¥/è¾“å‡º Schema ä¸å‡½æ•°ç­¾åã€‚
- **æ–°å¢æ¨¡å‹æˆ–è®­ç»ƒæ–¹å¼**: åœ¨ `model` æ³¨å†Œæ–°ç±»æˆ– trainerï¼›æ›´æ–° 3.5 ä¸»è¦å‡½æ•°/è¾“å‡ºè¯´æ˜ã€‚
- **å›æµ‹æŒ‡æ ‡æˆ–ç»„åˆé€»è¾‘**ï¼šä¿®æ”¹ `backtest` ç»„ä»¶ï¼›æ›´æ–° 3.6 æ ¸å¿ƒé€»è¾‘ä¸è¾“å‡º Schemaã€‚
- **å®éªŒè®°å½•è¦æ±‚**ï¼šä»»ä½•è®­ç»ƒ/å›æµ‹éœ€é€šè¿‡ `workflow.R`ï¼Œä¿è¯ params/metrics/objects è½åœ°ï¼›è‹¥ç›®å½•ç»“æ„è°ƒæ•´ï¼Œæ›´æ–° 3.7 è¾“å‡º Schemaã€‚

## 5. ç‰ˆæœ¬å†å²

| æ—¥æœŸ | å˜æ›´ | å½±å“æ¨¡å— |
|------|------|----------|
| 2026-01-05 | åˆå§‹åŒ–ç³»ç»Ÿçº§æ¶æ„æ–‡æ¡£ï¼Œæ¢³ç†æ•°æ®è·å–â†’é¢„å¤„ç†â†’æ•°æ®é›†â†’å»ºæ¨¡â†’å›æµ‹â†’å®éªŒè¿½è¸ªå…¨é“¾è·¯ | config/, data_fetch/, data_processor/, data_set/, model/, workflow/, backtest/, factor_hub/, data_monitor/ |
| 2026-01-05 | æŒ‰æœ€æ–° architecture-sync skill è¡¥å……æ ¸å¿ƒæ¨¡å— Schemaã€æ ¸å¿ƒé€»è¾‘ä»£ç å—ã€å‡½æ•°ç­¾åä¸ä¿®æ”¹æŒ‡å— | ARCHITECTURE.md |
| 2026-01-05 | æ›´æ–° `data_processor/` æ¶æ„ï¼šè¡¥å……æ¨¡å—ç»“æ„å›¾ã€è¾“å…¥/è¾“å‡º Schemaã€æ ¸å¿ƒé€»è¾‘ä¸ä¸»è¦å‡½æ•°ï¼Œä¿æŒä¸å®ç°åŒæ­¥ | data_processor/ |
