"""
multi_factor_backtest.py - å¤šå› å­å›æµ‹ç³»ç»Ÿ

ä¸“ä¸º RollingWindowTrainer å¤šå› å­è¾“å‡ºè®¾è®¡çš„å›æµ‹æµæ°´çº¿ã€‚
æ•´åˆ PredictionAdapterã€FactorProcessorã€ICAnalyzerã€PortfolioBuilderã€PerformanceEvaluatorã€‚

Usage:
    from quantclassic.backtest.multi_factor_backtest import MultiFactorBacktest
    
    backtest = MultiFactorBacktest(config)
    results = backtest.run(
        rolling_predictions,
        stock_col='order_book_id',
        time_col='trade_date',
        label_col='y_ret_10d'
    )
"""

import pandas as pd
import numpy as np
from typing import Dict, List, Optional, Any, Tuple
from pathlib import Path
import logging
import json
from datetime import datetime

from .backtest_config import BacktestConfig
from .prediction_adapter import PredictionAdapter
from .factor_processor import FactorProcessor
from .ic_analyzer import ICAnalyzer
from .portfolio_builder import PortfolioBuilder
from .performance_evaluator import PerformanceEvaluator
from .result_visualizer import ResultVisualizer
from .benchmark_manager import BenchmarkManager


class MultiFactorBacktest:
    """
    å¤šå› å­å›æµ‹ç³»ç»Ÿ
    
    ä¸€ç«™å¼å› å­å›æµ‹æµæ°´çº¿ï¼Œä¸“ä¸º RollingWindowTrainer çš„å¤šå› å­è¾“å‡ºè®¾è®¡ã€‚
    
    Pipeline:
        1. PredictionAdapter: é€‚é…é¢„æµ‹ç»“æœï¼Œå¤šå› å­é›†æˆ
        2. FactorProcessor: å› å­æ ‡å‡†åŒ–ï¼ˆå»æå€¼ã€Z-scoreã€ä¸­æ€§åŒ–ï¼‰
        3. ICAnalyzer: IC/ICIR åˆ†æ
        4. PortfolioBuilder: å¤šç©ºç»„åˆæ„å»º
        5. PerformanceEvaluator: ç»©æ•ˆè¯„ä¼°
        6. ResultVisualizer: å¯è§†åŒ–æŠ¥å‘Šç”Ÿæˆ
    
    Features:
        - æ”¯æŒå¤šå› å­é›†æˆï¼ˆmean, ic_weighted, bestï¼‰
        - è‡ªåŠ¨å› å­é¢„å¤„ç†
        - å®Œæ•´çš„ IC åˆ†æï¼ˆæ—¥åº¦ã€æœˆåº¦ã€è¡°å‡ï¼‰
        - å¤šç©ºç»„åˆç»©æ•ˆè¯„ä¼°
        - è‡ªåŠ¨åŒ–æŠ¥å‘Šç”Ÿæˆ
    """
    
    def __init__(self, config: Optional[BacktestConfig] = None):
        """
        åˆå§‹åŒ–å¤šå› å­å›æµ‹ç³»ç»Ÿ
        
        Args:
            config: å›æµ‹é…ç½®
        """
        self.config = config or BacktestConfig()
        self.logger = self._setup_logger()
        
        # åˆå§‹åŒ–å„ç»„ä»¶
        self.adapter = PredictionAdapter(self.config)
        self.processor = FactorProcessor(self.config)
        self.ic_analyzer = ICAnalyzer(self.config)
        self.portfolio_builder = PortfolioBuilder(self.config)
        self.evaluator = PerformanceEvaluator(self.config)
        self.visualizer = ResultVisualizer(self.config)
        self.benchmark_manager = BenchmarkManager()
        
        # ç»“æœç¼“å­˜
        self._results: Dict[str, Any] = {}
        
        self.logger.info("=" * 60)
        self.logger.info("ğŸ“Š å¤šå› å­å›æµ‹ç³»ç»Ÿåˆå§‹åŒ–")
        self.logger.info("=" * 60)
        self.logger.info(f"  è¾“å‡ºç›®å½•: {self.config.output_dir}")
        self.logger.info(f"  åˆ†ç»„æ•°é‡: {self.config.n_groups}")
        self.logger.info(f"  è°ƒä»“é¢‘ç‡: {self.config.rebalance_freq}")
        self.logger.info(f"  åŸºå‡†æŒ‡æ•°: {self.config.benchmark_index or 'æœªè®¾ç½®'}")
    
    def _setup_logger(self) -> logging.Logger:
        """é…ç½®æ—¥å¿—"""
        logger = logging.getLogger(__name__)
        logger.setLevel(getattr(logging, self.config.log_level))
        
        if not logger.handlers and self.config.console_log:
            handler = logging.StreamHandler()
            formatter = logging.Formatter(
                '%(asctime)s - %(levelname)s - %(message)s',
                datefmt='%H:%M:%S'
            )
            handler.setFormatter(formatter)
            logger.addHandler(handler)
        
        return logger
    
    def run(
        self,
        predictions_df: pd.DataFrame,
        stock_col: str = 'order_book_id',
        time_col: str = 'trade_date',
        label_col: str = 'y_ret_10d',
        ensemble_method: str = 'mean',
        custom_weights: Optional[Dict[str, float]] = None,
        save_results: bool = True
    ) -> Dict[str, Any]:
        """
        è¿è¡Œå®Œæ•´å›æµ‹æµç¨‹
        
        Args:
            predictions_df: RollingWindowTrainer çš„é¢„æµ‹ç»“æœ
            stock_col: è‚¡ç¥¨åˆ—å
            time_col: æ—¶é—´åˆ—å
            label_col: æ ‡ç­¾åˆ—å
            ensemble_method: å¤šå› å­é›†æˆæ–¹æ³• ('mean', 'ic_weighted', 'best', 'custom')
            custom_weights: è‡ªå®šä¹‰æƒé‡ï¼ˆä»…å½“ ensemble_method='custom' æ—¶ä½¿ç”¨ï¼‰
            save_results: æ˜¯å¦ä¿å­˜ç»“æœ
            
        Returns:
            å›æµ‹ç»“æœå­—å…¸ï¼ŒåŒ…å«ï¼š
            - adapted_df: é€‚é…åçš„æ•°æ®
            - processed_df: å¤„ç†åçš„å› å­æ•°æ®
            - ic_df: IC æ—¶é—´åºåˆ—
            - ic_stats: IC ç»Ÿè®¡æŒ‡æ ‡
            - portfolios: ç»„åˆæ”¶ç›Šæ•°æ®
            - metrics: ç»©æ•ˆæŒ‡æ ‡
            - factor_ics: å„å› å­ IC
            - ensemble_weights: é›†æˆæƒé‡
        """
        self.logger.info("\n" + "=" * 60)
        self.logger.info("ğŸš€ å¼€å§‹å¤šå› å­å›æµ‹")
        self.logger.info("=" * 60)
        
        results = {}
        
        # ========== Step 1: é€‚é…é¢„æµ‹ç»“æœ ==========
        self.logger.info("\nã€1/6ã€‘é€‚é…é¢„æµ‹ç»“æœ")
        adapted_df = self.adapter.adapt(
            predictions_df,
            stock_col=stock_col,
            time_col=time_col,
            label_col=label_col,
            ensemble_method=ensemble_method,
            custom_weights=custom_weights,
            output_factor_col='factor_raw'
        )
        results['adapted_df'] = adapted_df
        results['factor_ics'] = dict(self.adapter.get_factor_ics())  # å¤åˆ¶ä»¥é¿å…å¼•ç”¨é—®é¢˜
        results['ensemble_weights'] = dict(self.adapter.get_ensemble_weights())  # å¤åˆ¶ä»¥é¿å…å¼•ç”¨é—®é¢˜
        
        # ========== Step 2: å› å­å¤„ç† ==========
        self.logger.info("\nã€2/6ã€‘å› å­é¢„å¤„ç†")
        processed_df = self.processor.process(adapted_df, factor_cols=['factor_raw'])
        results['processed_df'] = processed_df
        
        # ç¡®å®šä½¿ç”¨çš„å› å­åˆ—ï¼ˆå¤„ç†åï¼‰
        factor_col = 'factor_raw_std'
        return_col = 'y_true' if 'y_true' in processed_df.columns else 'y_processed'
        
        # ========== Step 3: IC åˆ†æ ==========
        self.logger.info("\nã€3/6ã€‘IC åˆ†æ")
        ic_df = self.ic_analyzer.calculate_ic(processed_df, factor_col, return_col)
        ic_stats = self.ic_analyzer.analyze_ic_statistics(ic_df)
        
        results['ic_df'] = ic_df
        results['ic_stats'] = ic_stats
        
        self._print_ic_stats(ic_stats)
        
        # ========== Step 4: æ„å»ºç»„åˆ ==========
        self.logger.info("\nã€4/6ã€‘æ„å»ºå¤šç©ºç»„åˆ")
        portfolios = self.portfolio_builder.build_portfolios(
            processed_df, factor_col, return_col
        )
        
        # ========== Step 4.5: è·å–åŸºå‡†æ”¶ç›Šï¼ˆæ–°å¢ï¼‰ ==========
        benchmark_returns = None
        if self.config.benchmark_index:
            self.logger.info(f"\nã€4.5ã€‘è·å–åŸºå‡†æ”¶ç›Š: {self.config.benchmark_index}")
            try:
                # è·å–æ—¥æœŸèŒƒå›´
                if 'long_short' in portfolios and 'trade_date' in portfolios['long_short'].columns:
                    ls_df = portfolios['long_short']
                    start_date = pd.to_datetime(ls_df['trade_date'].min()).strftime('%Y-%m-%d')
                    end_date = pd.to_datetime(ls_df['trade_date'].max()).strftime('%Y-%m-%d')
                    
                    # ä» BenchmarkManager è·å–åŸºå‡†æ”¶ç›Š
                    benchmark_returns = self.benchmark_manager.get_benchmark_returns(
                        self.config.benchmark_index,
                        start_date=start_date,
                        end_date=end_date
                    )
                    
                    self.logger.info(f"     âœ“ æˆåŠŸè·å–åŸºå‡†æ•°æ®: {len(benchmark_returns)} æ¡")
                    self.logger.info(f"     æ—¥æœŸèŒƒå›´: {start_date} ~ {end_date}")
                    
                    # å°†åŸºå‡†æ”¶ç›Šåˆå¹¶åˆ°å„ç»„åˆ DataFrame
                    benchmark_df = pd.DataFrame({
                        'trade_date': benchmark_returns.index,
                        'benchmark_return': benchmark_returns.values
                    })
                    benchmark_df['trade_date'] = pd.to_datetime(benchmark_df['trade_date'])
                    
                    for name in portfolios:
                        if 'trade_date' in portfolios[name].columns:
                            portfolios[name]['trade_date'] = pd.to_datetime(portfolios[name]['trade_date'])
                            portfolios[name] = pd.merge(
                                portfolios[name], 
                                benchmark_df, 
                                on='trade_date', 
                                how='left'
                            )
                            portfolios[name]['benchmark_return'] = portfolios[name]['benchmark_return'].fillna(0)
                            
                            # è®¡ç®—åŸºå‡†ç´¯è®¡æ”¶ç›Š
                            if 'benchmark_return' in portfolios[name].columns:
                                portfolios[name]['benchmark_cumret'] = (1 + portfolios[name]['benchmark_return']).cumprod() - 1
                    
                    self.logger.info(f"     âœ“ å·²å°†åŸºå‡†æ”¶ç›Šåˆå¹¶åˆ°ç»„åˆæ•°æ®")
                else:
                    self.logger.warning("     âš ï¸ æ— æ³•è·å–ç»„åˆæ—¥æœŸèŒƒå›´ï¼Œè·³è¿‡åŸºå‡†è·å–")
                    
            except Exception as e:
                self.logger.warning(f"     âš ï¸ è·å–åŸºå‡†æ”¶ç›Šå¤±è´¥: {e}")
                benchmark_returns = None
        
        results['portfolios'] = portfolios
        results['benchmark_returns'] = benchmark_returns
        
        # ========== Step 5: ç»©æ•ˆè¯„ä¼° ==========
        self.logger.info("\nã€5/6ã€‘ç»©æ•ˆè¯„ä¼°")
        metrics = {}
        
        # ç¡®å®šæ˜¯å¦æœ‰åŸºå‡†åˆ—
        benchmark_col = 'benchmark_return' if benchmark_returns is not None else None
        
        for name, portfolio_df in portfolios.items():
            if 'portfolio_return' in portfolio_df.columns:
                m = self.evaluator.evaluate_portfolio(
                    portfolio_df, 
                    benchmark_col=benchmark_col
                )
                metrics[name] = m
        
        results['metrics'] = metrics
        self._print_metrics_summary(metrics, has_benchmark=(benchmark_col is not None))
        
        # ========== Step 6: å¯è§†åŒ–ä¸ä¿å­˜ ==========
        self.logger.info("\nã€6/6ã€‘ç”ŸæˆæŠ¥å‘Š")
        
        if save_results:
            self._save_results(results)
        
        if self.config.save_plots:
            plot_dir = Path(self.config.output_dir) / 'plots'
            plot_dir.mkdir(parents=True, exist_ok=True)
            self.visualizer.create_comprehensive_report(
                portfolios, ic_df, metrics, str(plot_dir),
                benchmark_name=self.config.benchmark_index
            )
        
        # æ±‡æ€»
        self.logger.info("\n" + "=" * 60)
        self.logger.info("âœ… å¤šå› å­å›æµ‹å®Œæˆ!")
        self.logger.info("=" * 60)
        
        self._results = results
        return results
    
    def run_multi_ensemble(
        self,
        predictions_df: pd.DataFrame,
        stock_col: str = 'order_book_id',
        time_col: str = 'trade_date',
        label_col: str = 'y_ret_10d',
        methods: List[str] = ['mean', 'ic_weighted', 'best']
    ) -> Dict[str, Dict[str, Any]]:
        """
        å¯¹æ¯”å¤šç§é›†æˆæ–¹æ³•
        
        Args:
            predictions_df: é¢„æµ‹ç»“æœ
            stock_col: è‚¡ç¥¨åˆ—å
            time_col: æ—¶é—´åˆ—å
            label_col: æ ‡ç­¾åˆ—å
            methods: è¦å¯¹æ¯”çš„é›†æˆæ–¹æ³•åˆ—è¡¨
            
        Returns:
            å„æ–¹æ³•çš„å›æµ‹ç»“æœå­—å…¸
        """
        self.logger.info("\n" + "=" * 60)
        self.logger.info("ğŸ“Š å¤šé›†æˆæ–¹æ³•å¯¹æ¯”")
        self.logger.info("=" * 60)
        
        all_results = {}
        
        for method in methods:
            self.logger.info(f"\n>>> æµ‹è¯•é›†æˆæ–¹æ³•: {method}")
            
            # ä¿®æ”¹è¾“å‡ºç›®å½•
            original_output = self.config.output_dir
            self.config.output_dir = f"{original_output}_{method}"
            
            results = self.run(
                predictions_df,
                stock_col=stock_col,
                time_col=time_col,
                label_col=label_col,
                ensemble_method=method,
                save_results=True
            )
            
            all_results[method] = results
            
            # æ¢å¤è¾“å‡ºç›®å½•
            self.config.output_dir = original_output
        
        # æ‰“å°å¯¹æ¯”ç»“æœ
        self._print_comparison(all_results)
        
        return all_results
    
    def _print_ic_stats(self, stats: Dict[str, Any]):
        """æ‰“å° IC ç»Ÿè®¡"""
        self.logger.info(f"\n  ğŸ“Š IC ç»Ÿè®¡:")
        self.logger.info(f"     IC å‡å€¼:   {stats['ic_mean']:+.4f}")
        self.logger.info(f"     IC æ ‡å‡†å·®: {stats['ic_std']:.4f}")
        self.logger.info(f"     ICIR:      {stats['icir']:.4f}")
        self.logger.info(f"     IC èƒœç‡:   {stats['ic_win_rate']:.2%}")
        self.logger.info(f"     t ç»Ÿè®¡é‡:  {stats['t_stat']:.4f}")
    
    def _print_metrics_summary(self, metrics: Dict[str, Dict[str, float]], has_benchmark: bool = False):
        """æ‰“å°ç»©æ•ˆæ±‡æ€»"""
        self.logger.info(f"\n  ğŸ“ˆ ç»©æ•ˆæ±‡æ€»:")
        
        for name in ['long', 'short', 'long_short']:
            if name in metrics:
                m = metrics[name]
                base_info = (
                    f"     {name:12s}: "
                    f"å¹´åŒ–={m['annual_return']:+.2%}, "
                    f"æ³¢åŠ¨={m['annual_volatility']:.2%}, "
                    f"å¤æ™®={m['sharpe_ratio']:.2f}, "
                    f"å›æ’¤={m['max_drawdown']:.2%}"
                )
                self.logger.info(base_info)
                
                # å¦‚æœæœ‰åŸºå‡†ï¼Œæ‰“å°ç›¸å¯¹æŒ‡æ ‡
                if has_benchmark and 'excess_return' in m:
                    relative_info = (
                        f"                   "
                        f"è¶…é¢={m.get('excess_return', 0):+.2%}, "
                        f"IR={m.get('information_ratio', 0):.2f}, "
                        f"Alpha={m.get('alpha', 0):+.2%}, "
                        f"Beta={m.get('beta', 0):.2f}"
                    )
                    self.logger.info(relative_info)
    
    def _print_comparison(self, all_results: Dict[str, Dict[str, Any]]):
        """æ‰“å°å¤šæ–¹æ³•å¯¹æ¯”"""
        self.logger.info("\n" + "=" * 60)
        self.logger.info("ğŸ“Š é›†æˆæ–¹æ³•å¯¹æ¯”æ±‡æ€»")
        self.logger.info("=" * 60)
        
        comparison = []
        for method, results in all_results.items():
            ic_stats = results.get('ic_stats', {})
            metrics = results.get('metrics', {})
            ls_metrics = metrics.get('long_short', {})
            
            comparison.append({
                'Method': method,
                'IC_Mean': ic_stats.get('ic_mean', 0),
                'ICIR': ic_stats.get('icir', 0),
                'Annual_Return': ls_metrics.get('annual_return', 0),
                'Sharpe': ls_metrics.get('sharpe_ratio', 0),
                'Max_Drawdown': ls_metrics.get('max_drawdown', 0)
            })
        
        comparison_df = pd.DataFrame(comparison)
        print(comparison_df.to_string(index=False))
    
    def _save_results(self, results: Dict[str, Any]):
        """ä¿å­˜ç»“æœ"""
        output_dir = Path(self.config.output_dir)
        output_dir.mkdir(parents=True, exist_ok=True)
        
        # ä¿å­˜é€‚é…åæ•°æ®
        if 'adapted_df' in results:
            results['adapted_df'].to_parquet(output_dir / 'adapted_predictions.parquet')
        
        # ä¿å­˜ IC
        if 'ic_df' in results:
            results['ic_df'].to_csv(output_dir / 'ic_analysis.csv', index=False)
        
        # ä¿å­˜ IC ç»Ÿè®¡
        if 'ic_stats' in results:
            with open(output_dir / 'ic_stats.json', 'w') as f:
                json.dump(results['ic_stats'], f, indent=2, default=str)
        
        # ä¿å­˜ç»„åˆ
        if 'portfolios' in results:
            for name, df in results['portfolios'].items():
                df.to_csv(output_dir / f'portfolio_{name}.csv', index=False)
        
        # ä¿å­˜ç»©æ•ˆæŒ‡æ ‡
        if 'metrics' in results:
            metrics_df = pd.DataFrame(results['metrics']).T
            metrics_df.to_csv(output_dir / 'performance_metrics.csv')
            
            if self.config.generate_excel:
                metrics_df.to_excel(output_dir / 'performance_metrics.xlsx')
        
        # ä¿å­˜å› å­ IC
        if 'factor_ics' in results and results['factor_ics']:
            with open(output_dir / 'factor_ics.json', 'w') as f:
                json.dump(results['factor_ics'], f, indent=2)
        
        self.logger.info(f"\n  ğŸ’¾ ç»“æœå·²ä¿å­˜è‡³: {output_dir}")
    
    def get_results(self) -> Dict[str, Any]:
        """è·å–æœ€è¿‘ä¸€æ¬¡å›æµ‹ç»“æœ"""
        return self._results


# ==================== ä¾¿æ·å‡½æ•° ====================

def run_factor_backtest(
    predictions_df: pd.DataFrame,
    stock_col: str = 'order_book_id',
    time_col: str = 'trade_date',
    label_col: str = 'y_ret_10d',
    ensemble_method: str = 'mean',
    output_dir: str = 'output/backtest',
    n_groups: int = 10,
    rebalance_freq: str = 'biweekly'
) -> Dict[str, Any]:
    """
    ä¾¿æ·å‡½æ•°ï¼šä¸€é”®è¿è¡Œå› å­å›æµ‹
    
    Example:
        from quantclassic.backtest.multi_factor_backtest import run_factor_backtest
        
        results = run_factor_backtest(
            rolling_predictions,
            stock_col='order_book_id',
            time_col='trade_date',
            label_col='y_ret_10d',
            ensemble_method='ic_weighted'
        )
    """
    config = BacktestConfig(
        output_dir=output_dir,
        n_groups=n_groups,
        rebalance_freq=rebalance_freq,
        save_plots=True,
        generate_excel=True
    )
    
    backtest = MultiFactorBacktest(config)
    return backtest.run(
        predictions_df,
        stock_col=stock_col,
        time_col=time_col,
        label_col=label_col,
        ensemble_method=ensemble_method
    )
