"""
æ¨¡å—åŒ–é…ç½®ä½¿ç”¨ç¤ºä¾‹

å±•ç¤ºå¦‚ä½•ä½¿ç”¨æ–°çš„æ¨¡å—åŒ–é…ç½®ç³»ç»Ÿæ‰‹åŠ¨æ­å»ºä¸åŒçš„æ¨¡å‹å˜ä½“ã€‚
"""

import sys
from pathlib import Path

# æ·»åŠ è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent))

from model.modular_config import (
    TemporalModuleConfig,
    GraphModuleConfig,
    FusionModuleConfig,
    CompositeModelConfig,
    ModelConfigBuilder,
    ConfigTemplates
)


def example_1_basic_usage():
    """
    ç¤ºä¾‹ 1: åŸºç¡€ä½¿ç”¨ - æ‰‹åŠ¨ç»„åˆæ¨¡å—
    """
    print("=" * 80)
    print("ç¤ºä¾‹ 1: åŸºç¡€ä½¿ç”¨ - æ‰‹åŠ¨ç»„åˆæ¨¡å—")
    print("=" * 80)
    
    # æ­¥éª¤ 1: ç‹¬ç«‹é…ç½®æ¯ä¸ªæ¨¡å—
    temporal_config = TemporalModuleConfig(
        rnn_type='lstm',
        hidden_size=64,
        num_layers=2,
        use_attention=True,
        attention_type='self',
        dropout=0.3
    )
    
    graph_config = GraphModuleConfig(
        gat_type='correlation',
        hidden_dim=32,
        heads=4,
        top_k_neighbors=10,
        dropout=0.3
    )
    
    fusion_config = FusionModuleConfig(
        hidden_sizes=[64],
        activation='relu',
        dropout=0.3,
        output_dim=1
    )
    
    # æ­¥éª¤ 2: ç»„åˆæˆå®Œæ•´æ¨¡å‹é…ç½®
    model_config = CompositeModelConfig(
        temporal=temporal_config,
        graph=graph_config,
        fusion=fusion_config,
        d_feat=20,
        n_epochs=100,
        batch_size=256,
        learning_rate=0.001
    )
    
    # æ­¥éª¤ 3: éªŒè¯å¹¶æŸ¥çœ‹æ‘˜è¦
    model_config.validate()
    print(model_config.summary())
    
    return model_config


def example_2_builder_pattern():
    """
    ç¤ºä¾‹ 2: ä½¿ç”¨æ„å»ºå™¨æ¨¡å¼ (æ¨è)
    """
    print("\n" + "=" * 80)
    print("ç¤ºä¾‹ 2: ä½¿ç”¨æ„å»ºå™¨æ¨¡å¼ (æ¨è)")
    print("=" * 80)
    
    # ä¸€æ¬¡æ€§æ„å»ºå®Œæ•´é…ç½®
    config = ModelConfigBuilder() \
        .set_input(d_feat=20, funda_dim=None) \
        .add_temporal(
            rnn_type='gru',
            hidden_size=128,
            num_layers=2,
            use_attention=True,
            attention_type='multi_head',
            attention_heads=8
        ) \
        .add_graph(
            gat_type='standard',
            hidden_dim=64,
            heads=4,
            adj_matrix_path='./adj_matrix.pt'
        ) \
        .add_fusion(
            hidden_sizes=[128, 64],
            activation='gelu',
            use_batch_norm=True
        ) \
        .set_training(
            device='cuda',
            n_epochs=150,
            batch_size=512,
            learning_rate=0.0005,
            optimizer='adamw'
        ) \
        .build()
    
    print(config.summary())
    
    return config


def example_3_pure_temporal():
    """
    ç¤ºä¾‹ 3: çº¯æ—¶åºæ¨¡å‹ (ä¸ä½¿ç”¨å›¾)
    """
    print("\n" + "=" * 80)
    print("ç¤ºä¾‹ 3: çº¯æ—¶åºæ¨¡å‹ (ä¸ä½¿ç”¨å›¾)")
    print("=" * 80)
    
    config = ModelConfigBuilder() \
        .set_input(d_feat=20) \
        .add_temporal(
            rnn_type='lstm',
            hidden_size=64,
            num_layers=2,
            bidirectional=True,  # ä½¿ç”¨åŒå‘LSTM
            use_attention=True
        ) \
        .add_fusion(hidden_sizes=[64]) \
        .build()
    
    print(config.summary())
    print(f"\næ³¨æ„: graph æ¨¡å—ä¸º Noneï¼Œæ¨¡å‹å°†è·³è¿‡å›¾ç¥ç»ç½‘ç»œéƒ¨åˆ†")
    
    return config


def example_4_graph_variants():
    """
    ç¤ºä¾‹ 4: ä¸åŒçš„å›¾ç¥ç»ç½‘ç»œå˜ä½“
    """
    print("\n" + "=" * 80)
    print("ç¤ºä¾‹ 4: ä¸åŒçš„å›¾ç¥ç»ç½‘ç»œå˜ä½“")
    print("=" * 80)
    
    # å˜ä½“ A: åŸºäºè¡Œä¸šå…³ç³»çš„æ ‡å‡†GAT
    print("\nã€å˜ä½“ A: è¡Œä¸šå…³ç³»GATã€‘")
    config_a = ModelConfigBuilder() \
        .add_temporal(rnn_type='lstm', hidden_size=64) \
        .add_graph(
            gat_type='standard',
            hidden_dim=32,
            heads=4
        ) \
        .add_fusion(hidden_sizes=[64]) \
        .build(d_feat=20)
    
    print(f"  GATç±»å‹: {config_a.graph.gat_type}")
    print(f"  è¾“å‡ºç»´åº¦: {config_a.graph.output_dim}")
    
    # å˜ä½“ B: åŸºäºç›¸å…³æ€§çš„åŠ¨æ€GAT
    print("\nã€å˜ä½“ B: ç›¸å…³æ€§GATã€‘")
    config_b = ModelConfigBuilder() \
        .add_temporal(rnn_type='gru', hidden_size=64) \
        .add_graph(
            gat_type='correlation',
            hidden_dim=32,
            heads=4,
            top_k_neighbors=15  # æ¯åªè‚¡ç¥¨è¿æ¥15ä¸ªæœ€ç›¸å…³çš„é‚»å±…
        ) \
        .add_fusion(hidden_sizes=[64]) \
        .build(d_feat=20)
    
    print(f"  GATç±»å‹: {config_b.graph.gat_type}")
    print(f"  Kè¿‘é‚»æ•°: {config_b.graph.top_k_neighbors}")
    
    # å˜ä½“ C: å®Œå…¨åŠ¨æ€å­¦ä¹ å›¾ç»“æ„
    print("\nã€å˜ä½“ C: åŠ¨æ€å›¾ç»“æ„ã€‘")
    config_c = ModelConfigBuilder() \
        .add_temporal(rnn_type='lstm', hidden_size=64) \
        .add_graph(
            gat_type='dynamic',
            hidden_dim=64,
            heads=8,
            use_edge_features=True  # ä½¿ç”¨è¾¹ç‰¹å¾
        ) \
        .add_fusion(hidden_sizes=[128, 64]) \
        .build(d_feat=20)
    
    print(f"  GATç±»å‹: {config_c.graph.gat_type}")
    print(f"  è¾¹ç‰¹å¾: {config_c.graph.use_edge_features}")
    
    return config_a, config_b, config_c


def example_5_attention_variants():
    """
    ç¤ºä¾‹ 5: ä¸åŒçš„æ³¨æ„åŠ›æœºåˆ¶å˜ä½“
    """
    print("\n" + "=" * 80)
    print("ç¤ºä¾‹ 5: ä¸åŒçš„æ³¨æ„åŠ›æœºåˆ¶å˜ä½“")
    print("=" * 80)
    
    # å˜ä½“ A: Self-Attention
    print("\nã€å˜ä½“ A: Self-Attentionã€‘")
    config_a = ModelConfigBuilder() \
        .add_temporal(
            rnn_type='lstm',
            hidden_size=64,
            use_attention=True,
            attention_type='self'
        ) \
        .add_fusion(hidden_sizes=[64]) \
        .build(d_feat=20)
    
    print(f"  æ³¨æ„åŠ›ç±»å‹: {config_a.temporal.attention_type}")
    
    # å˜ä½“ B: Multi-Head Attention
    print("\nã€å˜ä½“ B: Multi-Head Attentionã€‘")
    config_b = ModelConfigBuilder() \
        .add_temporal(
            rnn_type='gru',
            hidden_size=64,
            use_attention=True,
            attention_type='multi_head',
            attention_heads=8
        ) \
        .add_fusion(hidden_sizes=[64]) \
        .build(d_feat=20)
    
    print(f"  æ³¨æ„åŠ›ç±»å‹: {config_b.temporal.attention_type}")
    print(f"  æ³¨æ„åŠ›å¤´æ•°: {config_b.temporal.attention_heads}")
    
    # å˜ä½“ C: Additive Attention
    print("\nã€å˜ä½“ C: Additive Attentionã€‘")
    config_c = ModelConfigBuilder() \
        .add_temporal(
            rnn_type='lstm',
            hidden_size=64,
            use_attention=True,
            attention_type='additive'
        ) \
        .add_fusion(hidden_sizes=[64]) \
        .build(d_feat=20)
    
    print(f"  æ³¨æ„åŠ›ç±»å‹: {config_c.temporal.attention_type}")
    
    return config_a, config_b, config_c


def example_6_fusion_variants():
    """
    ç¤ºä¾‹ 6: ä¸åŒçš„èåˆç­–ç•¥
    """
    print("\n" + "=" * 80)
    print("ç¤ºä¾‹ 6: ä¸åŒçš„èåˆç­–ç•¥")
    print("=" * 80)
    
    # å˜ä½“ A: ç®€å•æ‹¼æ¥ (é»˜è®¤)
    print("\nã€å˜ä½“ A: ç®€å•æ‹¼æ¥ã€‘")
    config_a = ModelConfigBuilder() \
        .add_temporal(rnn_type='lstm', hidden_size=64) \
        .add_graph(gat_type='standard', hidden_dim=32) \
        .add_fusion(hidden_sizes=[64]) \
        .build(d_feat=20)
    
    print(f"  èåˆç­–ç•¥: {config_a.feature_fusion_strategy}")
    print(f"  èåˆè¾“å…¥ç»´åº¦: {config_a.get_fusion_input_dim()}")
    
    # å˜ä½“ B: æ·±å±‚MLP + BatchNorm
    print("\nã€å˜ä½“ B: æ·±å±‚MLP + BatchNormã€‘")
    config_b = ModelConfigBuilder() \
        .add_temporal(rnn_type='gru', hidden_size=64) \
        .add_graph(gat_type='correlation', hidden_dim=32) \
        .add_fusion(
            hidden_sizes=[128, 64, 32],
            use_batch_norm=True,
            activation='gelu'
        ) \
        .build(d_feat=20)
    
    print(f"  éšè—å±‚: {config_b.fusion.hidden_sizes}")
    print(f"  BatchNorm: {config_b.fusion.use_batch_norm}")
    
    # å˜ä½“ C: æ®‹å·®è¿æ¥
    print("\nã€å˜ä½“ C: æ®‹å·®è¿æ¥ã€‘")
    config_c = ModelConfigBuilder() \
        .add_temporal(rnn_type='lstm', hidden_size=64) \
        .add_graph(gat_type='standard', hidden_dim=32) \
        .add_fusion(
            hidden_sizes=[96, 96],  # ä¸è¾“å…¥ç»´åº¦ç›¸åŒä»¥æ”¯æŒæ®‹å·®
            use_residual=True
        ) \
        .build(d_feat=20)
    
    print(f"  æ®‹å·®è¿æ¥: {config_c.fusion.use_residual}")
    
    return config_a, config_b, config_c


def example_7_predefined_templates():
    """
    ç¤ºä¾‹ 7: ä½¿ç”¨é¢„å®šä¹‰æ¨¡æ¿
    """
    print("\n" + "=" * 80)
    print("ç¤ºä¾‹ 7: ä½¿ç”¨é¢„å®šä¹‰æ¨¡æ¿")
    print("=" * 80)
    
    # æ¨¡æ¿ A: å°å‹çº¯æ—¶åºæ¨¡å‹
    print("\nã€æ¨¡æ¿ A: å°å‹çº¯æ—¶åºæ¨¡å‹ã€‘")
    config_small = ConfigTemplates.pure_temporal(d_feat=20, model_size='small')
    print(f"  RNNéšè—å±‚: {config_small.temporal.hidden_size}")
    print(f"  RNNå±‚æ•°: {config_small.temporal.num_layers}")
    
    # æ¨¡æ¿ B: é»˜è®¤æ··åˆæ¨¡å‹
    print("\nã€æ¨¡æ¿ B: é»˜è®¤æ··åˆæ¨¡å‹ã€‘")
    config_default = ConfigTemplates.temporal_with_graph(
        d_feat=20,
        gat_type='standard',
        model_size='default'
    )
    print(f"  RNNéšè—å±‚: {config_default.temporal.hidden_size}")
    print(f"  GATéšè—å±‚: {config_default.graph.hidden_dim}")
    
    # æ¨¡æ¿ C: å¤§å‹é«˜çº§æ¨¡å‹
    print("\nã€æ¨¡æ¿ C: å¤§å‹é«˜çº§æ¨¡å‹ã€‘")
    config_large = ConfigTemplates.temporal_with_graph(
        d_feat=20,
        gat_type='correlation',
        model_size='large'
    )
    print(f"  RNNéšè—å±‚: {config_large.temporal.hidden_size}")
    print(f"  GATéšè—å±‚: {config_large.graph.hidden_dim}")
    
    # æ¨¡æ¿ D: å¤šå¤´æ³¨æ„åŠ›+ç›¸å…³æ€§å›¾
    print("\nã€æ¨¡æ¿ D: å¤šå¤´æ³¨æ„åŠ›+ç›¸å…³æ€§å›¾ã€‘")
    config_advanced = ConfigTemplates.attention_graph_fusion(
        d_feat=20,
        attention_type='multi_head',
        gat_type='correlation'
    )
    print(f"  æ³¨æ„åŠ›ç±»å‹: {config_advanced.temporal.attention_type}")
    print(f"  æ³¨æ„åŠ›å¤´æ•°: {config_advanced.temporal.attention_heads}")
    print(f"  GATç±»å‹: {config_advanced.graph.gat_type}")
    print(f"  èåˆå±‚æ•°: {len(config_advanced.fusion.hidden_sizes)}")
    
    return config_small, config_default, config_large, config_advanced


def example_8_save_and_load():
    """
    ç¤ºä¾‹ 8: ä¿å­˜å’ŒåŠ è½½é…ç½®
    """
    print("\n" + "=" * 80)
    print("ç¤ºä¾‹ 8: ä¿å­˜å’ŒåŠ è½½é…ç½®")
    print("=" * 80)
    
    # åˆ›å»ºé…ç½®
    config = ModelConfigBuilder() \
        .add_temporal(rnn_type='lstm', hidden_size=64) \
        .add_graph(gat_type='correlation', hidden_dim=32) \
        .add_fusion(hidden_sizes=[64]) \
        .build(d_feat=20)
    
    # ä¿å­˜ä¸º YAML
    yaml_path = '/tmp/my_model_config.yaml'
    config.to_yaml(yaml_path)
    print(f"âœ… é…ç½®å·²ä¿å­˜åˆ°: {yaml_path}")
    
    # åŠ è½½é…ç½®
    loaded_config = CompositeModelConfig.from_yaml(yaml_path)
    print(f"âœ… é…ç½®å·²åŠ è½½")
    print(f"  æ—¶åºæ¨¡å—: {loaded_config.temporal.rnn_type}")
    print(f"  å›¾æ¨¡å—: {loaded_config.graph.gat_type}")
    
    # ä¿å­˜ä¸º JSON
    json_path = '/tmp/my_model_config.json'
    config.to_json(json_path)
    print(f"âœ… é…ç½®å·²ä¿å­˜åˆ°: {json_path}")
    
    return config


def example_9_customize_module():
    """
    ç¤ºä¾‹ 9: è‡ªå®šä¹‰æ–°çš„æ¨¡å—é…ç½® (æ‰©å±•æ€§ç¤ºä¾‹)
    
    å±•ç¤ºå¦‚ä½•åŸºäºç°æœ‰æ¨¡å—é…ç½®åˆ›å»ºè‡ªå·±çš„å˜ä½“ã€‚
    """
    print("\n" + "=" * 80)
    print("ç¤ºä¾‹ 9: è‡ªå®šä¹‰æ–°çš„æ¨¡å—é…ç½® (æ‰©å±•æ€§ç¤ºä¾‹)")
    print("=" * 80)
    
    # è‡ªå®šä¹‰æ—¶åºæ¨¡å—: Transformer é£æ ¼
    class TransformerTemporalConfig(TemporalModuleConfig):
        """è‡ªå®šä¹‰: Transformeré£æ ¼çš„æ—¶åºæ¨¡å—"""
        def __init__(self, **kwargs):
            super().__init__(**kwargs)
            # è¦†ç›–é»˜è®¤è®¾ç½®
            self.rnn_type = 'gru'  # åŸºç¡€ä»ç”¨RNN
            self.use_attention = True
            self.attention_type = 'multi_head'
            self.attention_heads = 8
            self.name = 'TransformerTemporal'
    
    # ä½¿ç”¨è‡ªå®šä¹‰æ¨¡å—
    custom_temporal = TransformerTemporalConfig(
        hidden_size=128,
        num_layers=3,
        dropout=0.2
    )
    
    config = CompositeModelConfig(
        temporal=custom_temporal,
        graph=None,
        fusion=FusionModuleConfig(hidden_sizes=[128, 64]),
        d_feat=20
    )
    
    print(f"âœ… è‡ªå®šä¹‰é…ç½®åˆ›å»ºæˆåŠŸ")
    print(f"  æ¨¡å—åç§°: {config.temporal.name}")
    print(f"  æ³¨æ„åŠ›ç±»å‹: {config.temporal.attention_type}")
    print(f"  æ³¨æ„åŠ›å¤´æ•°: {config.temporal.attention_heads}")
    
    return config


def example_10_comparison():
    """
    ç¤ºä¾‹ 10: æ–°æ—§é…ç½®å¯¹æ¯”
    
    å±•ç¤ºä»æ•´ä½“é…ç½®è¿ç§»åˆ°æ¨¡å—åŒ–é…ç½®çš„å¯¹æ¯”ã€‚
    """
    print("\n" + "=" * 80)
    print("ç¤ºä¾‹ 10: æ–°æ—§é…ç½®å¯¹æ¯”")
    print("=" * 80)
    
    # æ—§æ–¹å¼: HybridGraphConfig (æ•´ä½“é…ç½®)
    print("\nã€æ—§æ–¹å¼: HybridGraphConfigã€‘")
    print("from model.model_config import HybridGraphConfig")
    print("config = HybridGraphConfig(")
    print("    d_feat=20,")
    print("    rnn_hidden=64,")
    print("    rnn_layers=2,")
    print("    rnn_type='lstm',")
    print("    gat_hidden=32,")
    print("    gat_heads=4,")
    print("    mlp_hidden_sizes=[64]")
    print(")")
    print("\nâŒ ç¼ºç‚¹:")
    print("  - æ‰€æœ‰å‚æ•°æ··åœ¨ä¸€èµ·ï¼Œéš¾ä»¥ç†è§£")
    print("  - æ‰©å±•æ€§å·®ï¼Œæ·»åŠ æ–°æ¨¡å—éœ€è¦ä¿®æ”¹æ•´ä¸ªç±»")
    print("  - ä¸æ”¯æŒæ¨¡å—å¤ç”¨")
    
    # æ–°æ–¹å¼: CompositeModelConfig (æ¨¡å—åŒ–é…ç½®)
    print("\nã€æ–°æ–¹å¼: CompositeModelConfigã€‘")
    print("from model.modular_config import ModelConfigBuilder")
    print("config = ModelConfigBuilder() \\")
    print("    .add_temporal(rnn_type='lstm', hidden_size=64, num_layers=2) \\")
    print("    .add_graph(gat_type='standard', hidden_dim=32, heads=4) \\")
    print("    .add_fusion(hidden_sizes=[64]) \\")
    print("    .build(d_feat=20)")
    print("\nâœ… ä¼˜ç‚¹:")
    print("  - æ¨¡å—ç‹¬ç«‹ï¼ŒèŒè´£æ¸…æ™°")
    print("  - é«˜æ‰©å±•æ€§ï¼Œè½»æ¾æ·»åŠ æ–°æ¨¡å—æˆ–å˜ä½“")
    print("  - æ”¯æŒæ¨¡å—å¤ç”¨å’Œç»„åˆ")
    print("  - æµå¼APIï¼Œå¯è¯»æ€§å¼º")
    
    # åˆ›å»ºå®é™…å¯¹æ¯”
    new_config = ModelConfigBuilder() \
        .add_temporal(rnn_type='lstm', hidden_size=64, num_layers=2) \
        .add_graph(gat_type='standard', hidden_dim=32, heads=4) \
        .add_fusion(hidden_sizes=[64]) \
        .build(d_feat=20)
    
    print(f"\nâœ… æ–°é…ç½®åˆ›å»ºæˆåŠŸ")
    print(f"  èåˆè¾“å…¥ç»´åº¦: {new_config.get_fusion_input_dim()}")
    
    return new_config


if __name__ == '__main__':
    """è¿è¡Œæ‰€æœ‰ç¤ºä¾‹"""
    
    print("â•”" + "=" * 78 + "â•—")
    print("â•‘" + " " * 20 + "æ¨¡å—åŒ–é…ç½®ç³»ç»Ÿä½¿ç”¨æŒ‡å—" + " " * 26 + "â•‘")
    print("â•š" + "=" * 78 + "â•")
    
    # è¿è¡Œæ‰€æœ‰ç¤ºä¾‹
    try:
        example_1_basic_usage()
        example_2_builder_pattern()
        example_3_pure_temporal()
        example_4_graph_variants()
        example_5_attention_variants()
        example_6_fusion_variants()
        example_7_predefined_templates()
        example_8_save_and_load()
        example_9_customize_module()
        example_10_comparison()
        
        print("\n" + "=" * 80)
        print("âœ… æ‰€æœ‰ç¤ºä¾‹è¿è¡ŒæˆåŠŸ!")
        print("=" * 80)
        
        print("\nğŸ’¡ å¿«é€Ÿå¼€å§‹:")
        print("  1. ä½¿ç”¨æ„å»ºå™¨å¿«é€Ÿåˆ›å»ºé…ç½® (æ¨è)")
        print("  2. ä½¿ç”¨é¢„å®šä¹‰æ¨¡æ¿")
        print("  3. æ‰‹åŠ¨ç»„åˆæ¨¡å— (æœ€çµæ´»)")
        
        print("\nğŸ“š æ›´å¤šæ–‡æ¡£:")
        print("  - modular_config.py: æ¨¡å—åŒ–é…ç½®æºç å’Œè¯¦ç»†æ–‡æ¡£")
        print("  - README_HYBRID_GRAPH.md: æ··åˆæ¨¡å‹ä½¿ç”¨æŒ‡å—")
        
    except Exception as e:
        print(f"\nâŒ è¿è¡Œå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
