"""
Loss Functions - æŸå¤±å‡½æ•°æ¨¡å—

æä¾›å¤šç§å¯é€‰çš„æŸå¤±å‡½æ•°ï¼Œæ”¯æŒç›¸å…³æ€§æ­£åˆ™åŒ–ä»¥æŠ‘åˆ¶ç‰¹å¾å†—ä½™ã€‚

Loss = BaseLoss(pred, target) + lambda * CorrelationPenalty(hidden_features)

Available Losses:
- MSEWithCorrelationLoss: MSE + ç‰¹å¾å»ç›¸å…³æ­£åˆ™
- MAEWithCorrelationLoss: MAE + ç‰¹å¾å»ç›¸å…³æ­£åˆ™
- HuberWithCorrelationLoss: Huber + ç‰¹å¾å»ç›¸å…³æ­£åˆ™
- ICLoss: æ’åº IC Loss (Information Coefficient)
- ICWithCorrelationLoss: IC Loss + ç‰¹å¾å»ç›¸å…³æ­£åˆ™

âš ï¸ ä½¿ç”¨è¦æ±‚:
------------
è¦ä½¿ç”¨å¸¦ç›¸å…³æ€§æ­£åˆ™åŒ–çš„æŸå¤±å‡½æ•° (*WithCorrelationLoss)ï¼Œæ¨¡å‹å¿…é¡»ï¼š

1. åœ¨ forward() ä¸­æ”¯æŒ return_hidden=True å‚æ•°
2. è¿”å› (predictions, hidden_features) å…ƒç»„
3. hidden_features å½¢çŠ¶ä¸º [batch_size, hidden_dim]

ç¤ºä¾‹:
    class MyNet(nn.Module):
        def forward(self, x, return_hidden=False):
            features = self.encoder(x)
            pred = self.head(features)
            if return_hidden:
                return pred, features
            return pred
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Optional, Tuple


# ==================== ç›¸å…³æ€§æ­£åˆ™åŒ–æ¨¡å— ====================

class CorrelationRegularizer(nn.Module):
    """
    ç›¸å…³æ€§æ­£åˆ™åŒ–æ¨¡å—
    
    è®¡ç®—éšè—å±‚ç‰¹å¾ä¹‹é—´çš„ç›¸å…³æ€§æƒ©ç½šï¼Œä¿ƒä½¿æ¨¡å‹å­¦ä¹ æ›´åŠ ç‹¬ç«‹çš„ç‰¹å¾è¡¨è¾¾ã€‚
    
    åŸç†ï¼š
    1. è®¡ç®—ç‰¹å¾çŸ©é˜µçš„ç›¸å…³æ€§çŸ©é˜µ (Correlation Matrix)
    2. ç†æƒ³æƒ…å†µä¸‹ï¼Œç›¸å…³æ€§çŸ©é˜µåº”è¯¥æ¥è¿‘å•ä½çŸ©é˜µï¼ˆå¯¹è§’çº¿ä¸º1ï¼Œå…¶ä½™ä¸º0ï¼‰
    3. ä½¿ç”¨ Frobenius èŒƒæ•°è®¡ç®—ä¸å•ä½çŸ©é˜µçš„å·®å¼‚ä½œä¸ºæƒ©ç½šé¡¹
    
    Args:
        lambda_corr: æ­£åˆ™åŒ–æƒé‡ï¼Œæ§åˆ¶å»ç›¸å…³çš„å¼ºåº¦
        eps: æ•°å€¼ç¨³å®šæ€§çš„å°å¸¸æ•°
    """
    
    def __init__(self, lambda_corr: float = 0.01, eps: float = 1e-8):
        super().__init__()
        self.lambda_corr = lambda_corr
        self.eps = eps
    
    def forward(self, hidden_features: torch.Tensor) -> torch.Tensor:
        """
        è®¡ç®—ç›¸å…³æ€§æƒ©ç½š
        
        Args:
            hidden_features: [batch_size, hidden_dim] éšè—å±‚ç‰¹å¾
            
        Returns:
            correlation_penalty: æ ‡é‡ï¼Œç›¸å…³æ€§æƒ©ç½šå€¼
        """
        if hidden_features is None or self.lambda_corr <= 0:
            return torch.tensor(0.0, device=hidden_features.device if hidden_features is not None else 'cpu')
        
        batch_size, hidden_dim = hidden_features.shape
        
        # æ ·æœ¬æ•°å¤ªå°‘æ—¶ï¼Œç›¸å…³æ€§çŸ©é˜µä¼°è®¡ä¸å¯é 
        if batch_size < 2:
            return torch.tensor(0.0, device=hidden_features.device)
        
        # 1. æ ‡å‡†åŒ–ç‰¹å¾ (Z-Score normalization along batch dimension)
        mean = hidden_features.mean(dim=0, keepdim=True)
        std = hidden_features.std(dim=0, keepdim=True) + self.eps
        normalized = (hidden_features - mean) / std
        
        # 2. è®¡ç®—ç›¸å…³æ€§çŸ©é˜µ: (X^T @ X) / (N - 1)
        # ç»“æœæ˜¯ [hidden_dim, hidden_dim] çš„å¯¹ç§°çŸ©é˜µ
        corr_matrix = (normalized.t() @ normalized) / (batch_size - 1)
        
        # 3. ç›®æ ‡ï¼šç›¸å…³æ€§çŸ©é˜µåº”æ¥è¿‘å•ä½çŸ©é˜µ
        # å¯¹è§’çº¿ä¸º 1ï¼ˆè‡ªç›¸å…³ï¼‰ï¼Œéå¯¹è§’çº¿ä¸º 0ï¼ˆæ— å†—ä½™ï¼‰
        identity = torch.eye(hidden_dim, device=corr_matrix.device)
        
        # 4. ä½¿ç”¨ Frobenius èŒƒæ•°è®¡ç®—å·®å¼‚
        # å¯é€‰ï¼šåªæƒ©ç½šéå¯¹è§’çº¿å…ƒç´ ï¼ˆæ›´ç²¾ç¡®ï¼‰
        off_diagonal_mask = 1 - identity
        off_diagonal_corr = corr_matrix * off_diagonal_mask
        
        # Frobenius èŒƒæ•°çš„å¹³æ–¹
        corr_penalty = torch.sum(off_diagonal_corr ** 2)
        
        # å½’ä¸€åŒ–ï¼ˆé™¤ä»¥éå¯¹è§’çº¿å…ƒç´ æ•°é‡ï¼‰
        n_off_diagonal = hidden_dim * (hidden_dim - 1)
        if n_off_diagonal > 0:
            corr_penalty = corr_penalty / n_off_diagonal
        
        return self.lambda_corr * corr_penalty


# ==================== åŸºç¡€æŸå¤±å‡½æ•° + ç›¸å…³æ€§æ­£åˆ™åŒ– ====================

class MSEWithCorrelationLoss(nn.Module):
    """
    MSE Loss + ç‰¹å¾ç›¸å…³æ€§æ­£åˆ™åŒ–
    
    Loss = MSE(pred, target) + lambda * CorrelationPenalty(hidden_features)
    
    Args:
        lambda_corr: ç›¸å…³æ€§æ­£åˆ™åŒ–æƒé‡ï¼Œå»ºè®®èŒƒå›´ [0.001, 0.1]
    
    Example:
        >>> criterion = MSEWithCorrelationLoss(lambda_corr=0.01)
        >>> pred, hidden = model(x)  # æ¨¡å‹éœ€è¿”å› (é¢„æµ‹, éšè—ç‰¹å¾)
        >>> loss = criterion(pred, target, hidden)
    """
    
    def __init__(self, lambda_corr: float = 0.01):
        super().__init__()
        self.mse = nn.MSELoss()
        self.corr_reg = CorrelationRegularizer(lambda_corr=lambda_corr)
        self.lambda_corr = lambda_corr
    
    def forward(
        self, 
        preds: torch.Tensor, 
        targets: torch.Tensor, 
        hidden_features: Optional[torch.Tensor] = None
    ) -> torch.Tensor:
        """
        è®¡ç®—å¸¦æ­£åˆ™é¡¹çš„æŸå¤±
        
        Args:
            preds: [batch_size] æˆ– [batch_size, 1] é¢„æµ‹å€¼
            targets: [batch_size] æˆ– [batch_size, 1] ç›®æ ‡å€¼
            hidden_features: [batch_size, hidden_dim] éšè—å±‚ç‰¹å¾ï¼ˆå¯é€‰ï¼‰
            
        Returns:
            loss: æ ‡é‡ï¼Œæ€»æŸå¤±
        """
        # åŸºç¡€ MSE æŸå¤±
        base_loss = self.mse(preds.flatten(), targets.flatten())
        
        # ç›¸å…³æ€§æ­£åˆ™åŒ–
        corr_penalty = self.corr_reg(hidden_features)
        
        return base_loss + corr_penalty
    
    def extra_repr(self) -> str:
        return f'lambda_corr={self.lambda_corr}'


class MAEWithCorrelationLoss(nn.Module):
    """
    MAE Loss + ç‰¹å¾ç›¸å…³æ€§æ­£åˆ™åŒ–
    
    Loss = MAE(pred, target) + lambda * CorrelationPenalty(hidden_features)
    """
    
    def __init__(self, lambda_corr: float = 0.01):
        super().__init__()
        self.mae = nn.L1Loss()
        self.corr_reg = CorrelationRegularizer(lambda_corr=lambda_corr)
        self.lambda_corr = lambda_corr
    
    def forward(
        self, 
        preds: torch.Tensor, 
        targets: torch.Tensor, 
        hidden_features: Optional[torch.Tensor] = None
    ) -> torch.Tensor:
        base_loss = self.mae(preds.flatten(), targets.flatten())
        corr_penalty = self.corr_reg(hidden_features)
        return base_loss + corr_penalty


class HuberWithCorrelationLoss(nn.Module):
    """
    Huber Loss + ç‰¹å¾ç›¸å…³æ€§æ­£åˆ™åŒ–
    
    Loss = Huber(pred, target) + lambda * CorrelationPenalty(hidden_features)
    
    Huber Loss ç»“åˆäº† MSE å’Œ MAE çš„ä¼˜ç‚¹ï¼š
    - å¯¹äºå°è¯¯å·®ï¼Œä½¿ç”¨ MSEï¼ˆå¹³æ»‘ä¸”å¯å¾®ï¼‰
    - å¯¹äºå¤§è¯¯å·®ï¼Œä½¿ç”¨ MAEï¼ˆå¯¹å¼‚å¸¸å€¼é²æ£’ï¼‰
    """
    
    def __init__(self, lambda_corr: float = 0.01, delta: float = 1.0):
        super().__init__()
        self.huber = nn.HuberLoss(delta=delta)
        self.corr_reg = CorrelationRegularizer(lambda_corr=lambda_corr)
        self.lambda_corr = lambda_corr
    
    def forward(
        self, 
        preds: torch.Tensor, 
        targets: torch.Tensor, 
        hidden_features: Optional[torch.Tensor] = None
    ) -> torch.Tensor:
        base_loss = self.huber(preds.flatten(), targets.flatten())
        corr_penalty = self.corr_reg(hidden_features)
        return base_loss + corr_penalty


# ==================== IC Loss (æ’åºæŸå¤±) ====================

class ICLoss(nn.Module):
    """
    IC Loss - Information Coefficient Loss
    
    åŸºäºæ’åºç›¸å…³æ€§çš„æŸå¤±å‡½æ•°ï¼Œä¼˜åŒ–é¢„æµ‹å€¼ä¸çœŸå®å€¼ä¹‹é—´çš„ Rank ICã€‚
    
    IC = Pearson(rank(pred), rank(target))
    Loss = 1 - IC  (æœ€å¤§åŒ– IC ç­‰ä»·äºæœ€å°åŒ– 1-IC)
    
    ä¼˜ç‚¹ï¼š
    - å…³æ³¨ç›¸å¯¹æ’åºï¼Œè€Œéç»å¯¹å€¼é¢„æµ‹
    - æ›´ç¬¦åˆé‡åŒ–ç­–ç•¥çš„å®é™…éœ€æ±‚ï¼ˆé€‰è‚¡æ’åï¼‰
    
    Note:
        éœ€è¦åŒä¸€æˆªé¢ï¼ˆåŒä¸€å¤©ï¼‰çš„æ‰€æœ‰è‚¡ç¥¨åœ¨ä¸€ä¸ª batch ä¸­
    """
    
    def __init__(self):
        super().__init__()
    
    def forward(
        self, 
        preds: torch.Tensor, 
        targets: torch.Tensor,
        hidden_features: Optional[torch.Tensor] = None
    ) -> torch.Tensor:
        """
        è®¡ç®— IC Loss
        
        Args:
            preds: [batch_size] é¢„æµ‹å€¼
            targets: [batch_size] ç›®æ ‡å€¼
            hidden_features: æœªä½¿ç”¨ï¼Œä¿æŒæ¥å£ä¸€è‡´
            
        Returns:
            loss: 1 - IC
        """
        preds = preds.flatten()
        targets = targets.flatten()
        
        # å¤„ç† NaN
        valid_mask = ~(torch.isnan(preds) | torch.isnan(targets))
        if valid_mask.sum() < 2:
            return torch.tensor(0.0, device=preds.device, requires_grad=True)
        
        preds = preds[valid_mask]
        targets = targets[valid_mask]
        
        # ä¸­å¿ƒåŒ–
        pred_mean = preds.mean()
        target_mean = targets.mean()
        pred_centered = preds - pred_mean
        target_centered = targets - target_mean
        
        # Pearson ç›¸å…³ç³»æ•°
        cov = (pred_centered * target_centered).sum()
        pred_std = torch.sqrt((pred_centered ** 2).sum() + 1e-8)
        target_std = torch.sqrt((target_centered ** 2).sum() + 1e-8)
        
        ic = cov / (pred_std * target_std)
        
        # IC èŒƒå›´æ˜¯ [-1, 1]ï¼Œæˆ‘ä»¬å¸Œæœ›æœ€å¤§åŒ– IC
        # å› æ­¤ Loss = 1 - IC (èŒƒå›´ [0, 2])
        # æˆ–è€…ä½¿ç”¨ -IC (ä½†è¿™æ · loss å¯èƒ½ä¸ºè´Ÿ)
        return 1.0 - ic


class ICWithCorrelationLoss(nn.Module):
    """
    IC Loss + ç‰¹å¾ç›¸å…³æ€§æ­£åˆ™åŒ–
    
    Loss = (1 - IC) + lambda * CorrelationPenalty(hidden_features)
    
    ç»“åˆæ’åºä¼˜åŒ–å’Œç‰¹å¾å»ç›¸å…³ï¼Œé€‚ç”¨äºé‡åŒ–é€‰è‚¡æ¨¡å‹ã€‚
    """
    
    def __init__(self, lambda_corr: float = 0.01):
        super().__init__()
        self.ic_loss = ICLoss()
        self.corr_reg = CorrelationRegularizer(lambda_corr=lambda_corr)
        self.lambda_corr = lambda_corr
    
    def forward(
        self, 
        preds: torch.Tensor, 
        targets: torch.Tensor, 
        hidden_features: Optional[torch.Tensor] = None
    ) -> torch.Tensor:
        base_loss = self.ic_loss(preds, targets)
        corr_penalty = self.corr_reg(hidden_features)
        return base_loss + corr_penalty


# ==================== ç»„åˆæŸå¤±å‡½æ•° ====================

class CombinedLoss(nn.Module):
    """
    ç»„åˆæŸå¤±å‡½æ•°
    
    æ”¯æŒåŒæ—¶ä½¿ç”¨å¤šç§æŸå¤±çš„åŠ æƒç»„åˆï¼š
    Loss = w1 * MSE + w2 * IC + lambda * CorrelationPenalty
    
    Args:
        mse_weight: MSE æŸå¤±æƒé‡
        ic_weight: IC æŸå¤±æƒé‡
        lambda_corr: ç›¸å…³æ€§æ­£åˆ™åŒ–æƒé‡
    """
    
    def __init__(
        self, 
        mse_weight: float = 1.0,
        ic_weight: float = 0.0,
        lambda_corr: float = 0.01
    ):
        super().__init__()
        self.mse_weight = mse_weight
        self.ic_weight = ic_weight
        
        self.mse = nn.MSELoss()
        self.ic_loss = ICLoss()
        self.corr_reg = CorrelationRegularizer(lambda_corr=lambda_corr)
    
    def forward(
        self, 
        preds: torch.Tensor, 
        targets: torch.Tensor, 
        hidden_features: Optional[torch.Tensor] = None
    ) -> torch.Tensor:
        total_loss = torch.tensor(0.0, device=preds.device)
        
        if self.mse_weight > 0:
            total_loss = total_loss + self.mse_weight * self.mse(preds.flatten(), targets.flatten())
        
        if self.ic_weight > 0:
            total_loss = total_loss + self.ic_weight * self.ic_loss(preds, targets)
        
        # ç›¸å…³æ€§æ­£åˆ™åŒ–
        corr_penalty = self.corr_reg(hidden_features)
        total_loss = total_loss + corr_penalty
        
        return total_loss


# ==================== ç»Ÿä¸€æŸå¤±å‡½æ•° (é‡æ„æ–°å¢) ====================

class UnifiedLoss(nn.Module):
    """
    ğŸ†• ç»Ÿä¸€æŸå¤±å‡½æ•° - é‡æ„åçš„æ ‡å‡†æŸå¤±æ¥å£
    
    æ”¯æŒå¤šç§åŸºç¡€æŸå¤± + å¯é€‰çš„ç›¸å…³æ€§æ­£åˆ™åŒ– + å¯é€‰çš„å¤šå› å­æ­£äº¤åŒ–ã€‚
    
    Loss = BaseLoss(pred, target) 
         + lambda_corr * CorrelationPenalty(hidden_features)
         + lambda_ortho * OrthoPenalty(factors)
    
    Args:
        base_loss: åŸºç¡€æŸå¤±ç±»å‹ ('mse' | 'mae' | 'huber' | 'ic' | 'rankic')
        lambda_corr: ç›¸å…³æ€§æ­£åˆ™åŒ–æƒé‡ï¼Œ0 è¡¨ç¤ºä¸ä½¿ç”¨
        lambda_ortho: å¤šå› å­æ­£äº¤åŒ–æƒé‡ï¼Œ0 è¡¨ç¤ºä¸ä½¿ç”¨
        reduction: æŸå¤± reduction æ¨¡å¼
        **kwargs: ä¼ é€’ç»™åŸºç¡€æŸå¤±çš„é¢å¤–å‚æ•°ï¼ˆå¦‚ Huber çš„ deltaï¼‰
        
    Example:
        >>> criterion = UnifiedLoss(base_loss='mse', lambda_corr=0.01)
        >>> loss = criterion(pred, target, hidden_features=hidden)
        
        >>> # å¤šå› å­æ¨¡å¼
        >>> criterion = UnifiedLoss(base_loss='mse', lambda_ortho=0.01)
        >>> loss = criterion(factor_preds, target, factors=factor_preds)
    """
    
    def __init__(
        self,
        base_loss: str = 'mse',
        lambda_corr: float = 0.0,
        lambda_ortho: float = 0.0,
        reduction: str = 'mean',
        **kwargs
    ):
        super().__init__()
        
        self.base_loss_name = base_loss.lower()
        self.lambda_corr = lambda_corr
        self.lambda_ortho = lambda_ortho
        self.reduction = reduction
        
        # åˆ›å»ºåŸºç¡€æŸå¤±å‡½æ•°
        if self.base_loss_name == 'mse':
            self.base_loss = nn.MSELoss(reduction=reduction)
        elif self.base_loss_name == 'mae':
            self.base_loss = nn.L1Loss(reduction=reduction)
        elif self.base_loss_name == 'huber':
            delta = kwargs.get('delta', 1.0)
            self.base_loss = nn.HuberLoss(delta=delta, reduction=reduction)
        elif self.base_loss_name in ['ic', 'rankic']:
            self.base_loss = ICLoss()
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„åŸºç¡€æŸå¤±ç±»å‹: {base_loss}")
        
        # ç›¸å…³æ€§æ­£åˆ™åŒ–å™¨
        if lambda_corr > 0:
            self.corr_reg = CorrelationRegularizer(lambda_corr=lambda_corr)
        else:
            self.corr_reg = None
        
        # å¤šå› å­æ­£äº¤åŒ–æ­£åˆ™åŒ–å™¨
        if lambda_ortho > 0:
            self.ortho_reg = CorrelationRegularizer(lambda_corr=lambda_ortho)
        else:
            self.ortho_reg = None
    
    def forward(
        self,
        preds: torch.Tensor,
        targets: torch.Tensor,
        hidden_features: Optional[torch.Tensor] = None,
        factors: Optional[torch.Tensor] = None
    ) -> torch.Tensor:
        """
        è®¡ç®—ç»Ÿä¸€æŸå¤±
        
        Args:
            preds: [batch_size] æˆ– [batch_size, num_factors] é¢„æµ‹å€¼
            targets: [batch_size] æˆ– [batch_size, num_factors] ç›®æ ‡å€¼
            hidden_features: [batch_size, hidden_dim] éšè—ç‰¹å¾ï¼ˆç”¨äºç›¸å…³æ€§æ­£åˆ™åŒ–ï¼‰
            factors: [batch_size, num_factors] å¤šå› å­è¾“å‡ºï¼ˆç”¨äºæ­£äº¤åŒ–ï¼‰
            
        Returns:
            loss: æ ‡é‡ï¼Œæ€»æŸå¤±
        """
        # åŸºç¡€æŸå¤±
        loss = self.base_loss(preds.flatten(), targets.flatten())
        
        # ç›¸å…³æ€§æ­£åˆ™åŒ–
        if self.corr_reg is not None and hidden_features is not None:
            loss = loss + self.corr_reg(hidden_features)
        
        # å¤šå› å­æ­£äº¤åŒ–
        if self.ortho_reg is not None and factors is not None:
            # å¯¹å› å­çŸ©é˜µçš„è½¬ç½®åšç›¸å…³æ€§æƒ©ç½šï¼Œä¿ƒä½¿å› å­é—´æ­£äº¤
            # factors: [batch, num_factors] -> è½¬ç½®åè®¡ç®—å› å­é—´ç›¸å…³æ€§
            if factors.dim() == 2 and factors.size(1) > 1:
                loss = loss + self.ortho_reg(factors)
        
        return loss
    
    def extra_repr(self) -> str:
        return (
            f"base_loss={self.base_loss_name}, "
            f"lambda_corr={self.lambda_corr}, "
            f"lambda_ortho={self.lambda_ortho}"
        )


# ==================== æŸå¤±å‡½æ•°å·¥å‚ ====================

def get_loss_fn(
    loss_type: str = 'mse',
    lambda_corr: float = 0.0,
    lambda_ortho: float = 0.0,
    **kwargs
) -> nn.Module:
    """
    æŸå¤±å‡½æ•°å·¥å‚
    
    Args:
        loss_type: æŸå¤±å‡½æ•°ç±»å‹
            - 'mse': MSE Loss
            - 'mae': MAE Loss
            - 'huber': Huber Loss
            - 'ic': IC Loss
            - 'mse_corr': MSE + ç›¸å…³æ€§æ­£åˆ™
            - 'mae_corr': MAE + ç›¸å…³æ€§æ­£åˆ™
            - 'huber_corr': Huber + ç›¸å…³æ€§æ­£åˆ™
            - 'ic_corr': IC + ç›¸å…³æ€§æ­£åˆ™
            - 'combined': ç»„åˆæŸå¤±
            - 'unified': ğŸ†• ç»Ÿä¸€æŸå¤±ï¼ˆæ¨èï¼‰
        lambda_corr: ç›¸å…³æ€§æ­£åˆ™åŒ–æƒé‡ï¼ˆä»…å¯¹ *_corr å’Œ unified ç±»å‹æœ‰æ•ˆï¼‰
        lambda_ortho: ğŸ†• å¤šå› å­æ­£äº¤åŒ–æƒé‡ï¼ˆä»…å¯¹ unified ç±»å‹æœ‰æ•ˆï¼‰
        **kwargs: é¢å¤–å‚æ•°
        
    Returns:
        æŸå¤±å‡½æ•°æ¨¡å—
        
    Example:
        >>> criterion = get_loss_fn('mse_corr', lambda_corr=0.01)
        >>> loss = criterion(pred, target, hidden_features)
        
        >>> # ä½¿ç”¨ç»Ÿä¸€æŸå¤±ï¼ˆæ¨èï¼‰
        >>> criterion = get_loss_fn('unified', base_loss='mse', lambda_corr=0.01)
    """
    loss_type = loss_type.lower()
    
    # ğŸ†• ç»Ÿä¸€æŸå¤±ï¼ˆæ¨èï¼‰
    if loss_type == 'unified':
        base_loss = kwargs.pop('base_loss', 'mse')
        return UnifiedLoss(
            base_loss=base_loss,
            lambda_corr=lambda_corr,
            lambda_ortho=lambda_ortho,
            **kwargs
        )
    
    # æ ‡å‡†æŸå¤±ï¼ˆæ— æ­£åˆ™åŒ–ï¼‰
    if loss_type == 'mse':
        return nn.MSELoss()
    elif loss_type == 'mae':
        return nn.L1Loss()
    elif loss_type == 'huber':
        return nn.HuberLoss(**kwargs)
    elif loss_type == 'ic':
        return ICLoss()
    
    # å¸¦ç›¸å…³æ€§æ­£åˆ™åŒ–çš„æŸå¤±
    elif loss_type == 'mse_corr':
        return MSEWithCorrelationLoss(lambda_corr=lambda_corr)
    elif loss_type == 'mae_corr':
        return MAEWithCorrelationLoss(lambda_corr=lambda_corr)
    elif loss_type == 'huber_corr':
        return HuberWithCorrelationLoss(lambda_corr=lambda_corr, **kwargs)
    elif loss_type == 'ic_corr':
        return ICWithCorrelationLoss(lambda_corr=lambda_corr)
    
    # ç»„åˆæŸå¤±
    elif loss_type == 'combined':
        return CombinedLoss(lambda_corr=lambda_corr, **kwargs)
    
    else:
        raise ValueError(f"Unknown loss type: {loss_type}. "
                        f"Supported: mse, mae, huber, ic, mse_corr, mae_corr, huber_corr, ic_corr, combined, unified")


if __name__ == '__main__':
    print("=" * 80)
    print("Loss Functions æµ‹è¯•")
    print("=" * 80)
    
    # æ¨¡æ‹Ÿæ•°æ®
    batch_size = 32
    hidden_dim = 64
    
    preds = torch.randn(batch_size)
    targets = torch.randn(batch_size)
    hidden = torch.randn(batch_size, hidden_dim)
    
    # æµ‹è¯•å„ç§æŸå¤±å‡½æ•°
    print("\n1. æµ‹è¯• MSEWithCorrelationLoss:")
    criterion = MSEWithCorrelationLoss(lambda_corr=0.01)
    loss = criterion(preds, targets, hidden)
    print(f"   Loss: {loss.item():.6f}")
    
    print("\n2. æµ‹è¯• ICLoss:")
    criterion = ICLoss()
    loss = criterion(preds, targets)
    print(f"   Loss: {loss.item():.6f}")
    
    print("\n3. æµ‹è¯• ICWithCorrelationLoss:")
    criterion = ICWithCorrelationLoss(lambda_corr=0.01)
    loss = criterion(preds, targets, hidden)
    print(f"   Loss: {loss.item():.6f}")
    
    print("\n4. æµ‹è¯• CombinedLoss:")
    criterion = CombinedLoss(mse_weight=0.5, ic_weight=0.5, lambda_corr=0.01)
    loss = criterion(preds, targets, hidden)
    print(f"   Loss: {loss.item():.6f}")
    
    print("\n5. æµ‹è¯•æŸå¤±å‡½æ•°å·¥å‚:")
    for loss_type in ['mse', 'mse_corr', 'ic', 'ic_corr']:
        criterion = get_loss_fn(loss_type, lambda_corr=0.01)
        print(f"   {loss_type}: {criterion.__class__.__name__}")
    
    print("\n" + "=" * 80)
    print("âœ… Loss Functions æµ‹è¯•å®Œæˆ")
    print("=" * 80)
