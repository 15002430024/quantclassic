"""
SimpleTrainer - ç®€å•è®­ç»ƒå™¨

æ¥ç®¡å¸¸è§„è®­ç»ƒï¼Œæ›¿æ¢åŸ PyTorchModel.fit çš„å®ç°ã€‚
é€‚ç”¨äºå•çª—å£ã€å•æ•°æ®é›†çš„æ ‡å‡†è®­ç»ƒåœºæ™¯ã€‚
"""

import torch
import torch.nn as nn
from typing import Optional, Dict, Any, Tuple

from .base_trainer import BaseTrainer, TrainerConfig


class SimpleTrainer(BaseTrainer):
    """
    ç®€å•è®­ç»ƒå™¨
    
    å®ç°æ ‡å‡†çš„è®­ç»ƒå¾ªç¯ï¼Œæ”¯æŒï¼š
    - è‡ªåŠ¨ GPU ç®¡ç†
    - æ—©åœæœºåˆ¶
    - å­¦ä¹ ç‡è°ƒåº¦
    - æ£€æŸ¥ç‚¹ä¿å­˜
    - ç›¸å…³æ€§æ­£åˆ™åŒ–
    
    ç”¨äºæ›¿ä»£ PyTorchModel.fit ä¸­çš„è®­ç»ƒé€»è¾‘ã€‚
    
    Example:
        >>> model = LSTMNet(d_feat=20, hidden_size=64, num_layers=2, dropout=0.1)
        >>> config = TrainerConfig(n_epochs=100, lr=0.001)
        >>> trainer = SimpleTrainer(model, config)
        >>> result = trainer.train(train_loader, val_loader)
    """
    
    def __init__(
        self,
        model: nn.Module,
        config: Optional[TrainerConfig] = None,
        device: Optional[str] = None,
        **kwargs
    ):
        """
        åˆå§‹åŒ–ç®€å•è®­ç»ƒå™¨
        
        Args:
            model: PyTorch æ¨¡å‹
            config: è®­ç»ƒé…ç½®ï¼ˆNone åˆ™ä½¿ç”¨é»˜è®¤é…ç½®ï¼‰
            device: è®¡ç®—è®¾å¤‡
            **kwargs: é¢å¤–é…ç½®å‚æ•°ï¼ˆä¼šè¦†ç›– configï¼‰
        """
        if config is None:
            config = TrainerConfig(**kwargs)
        elif kwargs:
            # ç”¨ kwargs è¦†ç›– config å­—æ®µï¼ˆdataclass æ—  update æ–¹æ³•ï¼‰
            from dataclasses import replace
            config = replace(config, **kwargs)
        
        super().__init__(model, config, device)
        
        # ç›¸å…³æ€§æ­£åˆ™åŒ–æ ‡å¿—
        self._use_corr_loss = config.lambda_corr > 0
        self._supports_return_hidden = self._check_return_hidden_support()
        
        if self._use_corr_loss:
            if self._supports_return_hidden:
                self.logger.info("  âœ… ç›¸å…³æ€§æ­£åˆ™åŒ–å·²å¯ç”¨ (æ¨¡å‹æ”¯æŒ return_hidden)")
            else:
                self.logger.warning("  âš ï¸ æ¨¡å‹ä¸æ”¯æŒ return_hiddenï¼Œç›¸å…³æ€§æ­£åˆ™åŒ–é™çº§")
                self._use_corr_loss = False
    
    def _check_return_hidden_support(self) -> bool:
        """æ£€æŸ¥æ¨¡å‹æ˜¯å¦æ”¯æŒ return_hidden å‚æ•°"""
        import inspect
        
        if hasattr(self.model, 'forward'):
            sig = inspect.signature(self.model.forward)
            return 'return_hidden' in sig.parameters
        return False
    
    def _forward_with_hidden(self, x: torch.Tensor, adj: Optional[torch.Tensor] = None):
        """
        å¸¦ hidden è¾“å‡ºçš„å‰å‘ä¼ æ’­
        
        Args:
            x: è¾“å…¥ç‰¹å¾
            adj: é‚»æ¥çŸ©é˜µï¼ˆå¯é€‰ï¼‰
            
        Returns:
            (predictions, hidden_features) æˆ– (predictions, None)
        """
        if self._use_corr_loss and self._supports_return_hidden:
            try:
                # å°è¯•ä¼ é€’ adj
                try:
                    output = self.model(x, adj=adj, return_hidden=True)
                except TypeError:
                    output = self.model(x, return_hidden=True)
                
                if isinstance(output, tuple) and len(output) >= 2:
                    return output[0], output[-1]  # (pred, hidden)
                else:
                    return output, None
            except Exception as e:
                self.logger.warning(f"return_hidden è°ƒç”¨å¤±è´¥: {e}")
                return self.model(x), None
        else:
            # æ ‡å‡†å‰å‘ä¼ æ’­
            try:
                return self.model(x, adj=adj) if adj is not None else self.model(x), None
            except TypeError:
                return self.model(x), None
    
    def train_batch(self, batch_data) -> float:
        """
        è®­ç»ƒå•ä¸ª batch
        
        Args:
            batch_data: DataLoader è¿”å›çš„æ•°æ®
            
        Returns:
            batch æŸå¤±å€¼ï¼ˆç©ºæ‰¹æ¬¡è¿”å› 0.0ï¼‰
        """
        x, y, adj, idx = self._parse_batch_data(batch_data)
        
        # ğŸ†• è·³è¿‡ç©ºæ‰¹æ¬¡ï¼ˆé¿å… GAT å±‚ N=0 reshape å¼‚å¸¸ï¼‰
        if x.size(0) == 0:
            return 0.0
        
        # ç§»åŠ¨æ•°æ®åˆ°è®¾å¤‡
        x = x.to(self.device)
        y = y.to(self.device)
        if adj is not None:
            adj = adj.to(self.device)
        
        # å‰å‘ä¼ æ’­
        self.optimizer.zero_grad()
        
        pred, hidden = self._forward_with_hidden(x, adj)
        
        # è®¡ç®—æŸå¤±
        if self._use_corr_loss and hidden is not None:
            # å¸¦ç›¸å…³æ€§æ­£åˆ™åŒ–çš„æŸå¤±
            loss = self.criterion(pred, y, hidden)
        else:
            # æ ‡å‡†æŸå¤±
            loss = self.criterion(pred, y)
        
        # åå‘ä¼ æ’­
        loss.backward()
        self.optimizer.step()
        
        return loss.item()
    
    def validate_epoch(self, val_loader) -> float:
        """
        éªŒè¯ä¸€ä¸ª epoch
        
        Args:
            val_loader: éªŒè¯æ•°æ®åŠ è½½å™¨
            
        Returns:
            å¹³å‡éªŒè¯æŸå¤±
        """
        self.model.eval()
        total_loss = 0.0
        n_batches = 0
        
        with torch.no_grad():
            for batch_data in val_loader:
                x, y, adj, idx = self._parse_batch_data(batch_data)
                
                # ğŸ†• è·³è¿‡ç©ºæ‰¹æ¬¡ï¼ˆé¿å… GAT å±‚ N=0 reshape å¼‚å¸¸ï¼‰
                if x.size(0) == 0:
                    continue
                
                x = x.to(self.device)
                y = y.to(self.device)
                if adj is not None:
                    adj = adj.to(self.device)
                
                # å‰å‘ä¼ æ’­ï¼ˆéªŒè¯æ—¶ä¸éœ€è¦ hiddenï¼‰
                try:
                    pred = self.model(x, adj=adj) if adj is not None else self.model(x)
                except TypeError:
                    pred = self.model(x)
                
                # å¦‚æœæ¨¡å‹è¿”å›å…ƒç»„ï¼Œå–ç¬¬ä¸€ä¸ªå…ƒç´ 
                if isinstance(pred, tuple):
                    pred = pred[0]
                
                # ğŸ†• å¤šå› å­é¢„æµ‹èšåˆï¼šå¦‚æœ pred æ˜¯ [batch, F]ï¼Œå–å‡å€¼å¾—åˆ° [batch]
                if pred.dim() == 2 and pred.size(1) > 1:
                    pred = pred.mean(dim=1)
                
                # è®¡ç®—æŸå¤±ï¼ˆéªŒè¯æ—¶ä¸ä½¿ç”¨ç›¸å…³æ€§æ­£åˆ™åŒ–ï¼‰
                if hasattr(self.criterion, 'base_loss'):
                    # å¯¹äºå¸¦ç›¸å…³æ€§çš„æŸå¤±å‡½æ•°ï¼Œåªä½¿ç”¨åŸºç¡€æŸå¤±
                    loss = self.criterion.base_loss(pred, y)
                else:
                    loss = self.criterion(pred, y)
                
                total_loss += loss.item()
                n_batches += 1
        
        return total_loss / max(n_batches, 1)
    
    def predict(self, test_loader, return_numpy: bool = True):
        """
        é¢„æµ‹ - ä¼˜å…ˆå§”æ‰˜æ¨¡å‹çš„ predict æ–¹æ³•
        
        å¦‚æœæ¨¡å‹æœ‰è‡ªå·±çš„ predict()ï¼ˆå¦‚ PyTorchModel å­ç±»ï¼‰ï¼Œç›´æ¥å§”æ‰˜è°ƒç”¨ï¼›
        å¦åˆ™å›é€€åˆ° Trainer è‡ªå·±çš„å®ç°ï¼ˆç”¨äºçº¯ nn.Moduleï¼‰ã€‚
        
        Args:
            test_loader: æµ‹è¯•æ•°æ®åŠ è½½å™¨
            return_numpy: æ˜¯å¦è¿”å› numpy æ•°ç»„
            
        Returns:
            é¢„æµ‹ç»“æœ
        """
        # ğŸ†• æ£€æŸ¥æ¨¡å‹æ˜¯å¦æœ‰ predict æ–¹æ³•ï¼ˆPyTorchModel å­ç±»æœ‰ï¼Œçº¯ nn.Module æ²¡æœ‰ï¼‰
        if hasattr(self.model, 'predict') and callable(getattr(self.model, 'predict')):
            try:
                # å§”æ‰˜ç»™æ¨¡å‹ï¼ˆæ¨¡å‹çš„ predict å·²åŒ…å«å®Œæ•´çš„æ‰¹æ¬¡è§£æã€è®¾å¤‡è¿ç§»ã€ç©ºå¤„ç†é€»è¾‘ï¼‰
                return self.model.predict(test_loader, return_numpy)
            except (TypeError, AttributeError):
                # æ¨¡å‹çš„ predict ç­¾åä¸å…¼å®¹ï¼Œå›é€€åˆ° Trainer å®ç°
                pass
        
        # å›é€€ï¼šTrainer è‡ªå·±çš„å®ç°ï¼ˆç”¨äºçº¯ nn.Moduleï¼‰
        self.model.eval()
        predictions = []
        labels = []  # ğŸ†• åŒæ—¶æ”¶é›†æ ‡ç­¾ç”¨äºè¿”å›
        
        with torch.no_grad():
            for batch_data in test_loader:
                x, y, adj, _ = self._parse_batch_data(batch_data)
                
                # ğŸ†• è·³è¿‡ç©ºæ‰¹æ¬¡ï¼ˆé¿å… GAT å±‚ N=0 reshape å¼‚å¸¸ï¼‰
                if x.size(0) == 0:
                    continue
                
                x = x.to(self.device)
                if adj is not None:
                    adj = adj.to(self.device)
                
                try:
                    pred = self.model(x, adj=adj) if adj is not None else self.model(x)
                except TypeError:
                    pred = self.model(x)
                
                if isinstance(pred, tuple):
                    pred = pred[0]
                
                predictions.append(pred.cpu())
                labels.append(y.cpu())
        
        if len(predictions) == 0:
            import numpy as np
            empty_result = np.array([]) if return_numpy else torch.tensor([])
            return empty_result, empty_result
        
        result = torch.cat(predictions, dim=0)
        labels_result = torch.cat(labels, dim=0)
        
        if return_numpy:
            return result.numpy(), labels_result.numpy()
        return result, labels_result


def create_simple_trainer(
    model: nn.Module,
    n_epochs: int = 100,
    lr: float = 0.001,
    early_stop: int = 20,
    device: Optional[str] = None,
    **kwargs
) -> SimpleTrainer:
    """
    åˆ›å»º SimpleTrainer çš„ä¾¿æ·å‡½æ•°
    
    Args:
        model: PyTorch æ¨¡å‹
        n_epochs: è®­ç»ƒè½®æ•°
        lr: å­¦ä¹ ç‡
        early_stop: æ—©åœè€å¿ƒå€¼
        device: è®¡ç®—è®¾å¤‡
        **kwargs: å…¶ä»– TrainerConfig å‚æ•°
        
    Returns:
        SimpleTrainer å®ä¾‹
    """
    config = TrainerConfig(
        n_epochs=n_epochs,
        lr=lr,
        early_stop=early_stop,
        **kwargs
    )
    return SimpleTrainer(model, config, device)
