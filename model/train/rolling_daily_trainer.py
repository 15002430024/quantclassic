"""
RollingDailyTrainer - æ—¥çº§æ»šåŠ¨è®­ç»ƒå™¨

åŸºäº RollingWindowTrainerï¼Œä¸“é—¨å¤„ç†æ—¥çº§ç²’åº¦çš„æ»šåŠ¨è®­ç»ƒã€‚
å¢åŠ æ˜¾å­˜ç®¡ç†ï¼Œæ”¯æŒé«˜é¢‘æ¨¡å‹åˆ‡æ¢ã€‚
"""

import gc
import copy
import time
from dataclasses import dataclass, field
from pathlib import Path
from typing import (
    Any, Callable, Dict, List, Optional, Union
)

import torch
import torch.nn as nn
import numpy as np
import pandas as pd

from .base_trainer import TrainerConfig
from .rolling_window_trainer import (
    RollingWindowTrainer, 
    RollingTrainerConfig,
    WindowResult
)


@dataclass
class DailyRollingConfig(RollingTrainerConfig):
    """
    æ—¥çº§æ»šåŠ¨è®­ç»ƒé…ç½®
    
    ç»§æ‰¿ RollingTrainerConfigï¼Œå¢åŠ æ˜¾å­˜ç®¡ç†ç›¸å…³é…ç½®ã€‚
    
    Args:
        gc_interval: æ¯éš”å¤šå°‘çª—å£è¿›è¡Œä¸€æ¬¡åƒåœ¾å›æ”¶ï¼ˆ0è¡¨ç¤ºæ¯çª—å£éƒ½å›æ”¶ï¼‰
        offload_to_cpu: åˆ‡çª—æ—¶æ˜¯å¦å°†æ¨¡å‹ç§»åˆ°CPU
        clear_cache_on_window_end: çª—å£ç»“æŸæ—¶æ˜¯å¦æ¸…ç†CUDAç¼“å­˜
    """
    gc_interval: int = 1
    offload_to_cpu: bool = True
    clear_cache_on_window_end: bool = True


class RollingDailyTrainer(RollingWindowTrainer):
    """
    æ—¥çº§æ»šåŠ¨è®­ç»ƒå™¨
    
    åŸºäº RollingWindowTrainerï¼Œä¸“é—¨å¤„ç†æ—¥çº§ç²’åº¦çš„æ»šåŠ¨è®­ç»ƒã€‚
    ä¸»è¦å¢å¼º:
    1. æ˜¾å­˜ç®¡ç†ï¼šåˆ‡çª—æ—¶é‡Šæ”¾æ˜¾å­˜ï¼Œé¿å…OOM
    2. æ—¥çº§çª—å£æ”¯æŒï¼šé€‚é…æ—¥æ‰¹æ¬¡æ•°æ®åŠ è½½å™¨
    3. å…¼å®¹æ€§ï¼šä¿æŒä¸æ—§ rolling_daily_trainer.py æ¥å£å…¼å®¹
    
    Example:
        >>> from quantclassic.model.train import RollingDailyTrainer, DailyRollingConfig
        >>> 
        >>> config = DailyRollingConfig(
        ...     n_epochs=20,
        ...     weight_inheritance=True,
        ...     gc_interval=5
        ... )
        >>> 
        >>> trainer = RollingDailyTrainer(
        ...     model_factory=lambda: HybridGraphModel.from_config(cfg, d_feat=20).model,
        ...     config=config,
        ...     device='cuda'
        ... )
        >>> 
        >>> # rolling_loaders æ¥è‡ª DataManager.create_rolling_daily_loaders()
        >>> results = trainer.fit(rolling_loaders, save_dir='output/models')
    """
    
    def __init__(
        self,
        model_factory: Callable[[], nn.Module],
        config: Optional[DailyRollingConfig] = None,
        device: Optional[str] = None,
        warm_start: Optional[bool] = None,  # å…¼å®¹æ—§å‚æ•°å
        save_each_window: Optional[bool] = None,  # å…¼å®¹æ—§å‚æ•°å
        **kwargs
    ):
        """
        åˆå§‹åŒ–æ—¥çº§æ»šåŠ¨è®­ç»ƒå™¨
        
        Args:
            model_factory: æ¨¡å‹å·¥å‚å‡½æ•°
            config: è®­ç»ƒé…ç½®
            device: è®¡ç®—è®¾å¤‡
            warm_start: æ˜¯å¦ç»§æ‰¿æƒé‡ï¼ˆå…¼å®¹æ—§APIï¼‰
            save_each_window: æ˜¯å¦ä¿å­˜æ¯çª—å£æ¨¡å‹ï¼ˆå…¼å®¹æ—§APIï¼‰
            **kwargs: å…¶ä»–é…ç½®å‚æ•°
        """
        # å¤„ç†å…¼å®¹æ€§å‚æ•°
        if config is None:
            config = DailyRollingConfig(**kwargs)
        
        if warm_start is not None:
            config.weight_inheritance = warm_start
        if save_each_window is not None:
            config.save_each_window = save_each_window
        
        super().__init__(model_factory, config, device)
        
        # æ—¥çº§ç‰¹æœ‰çŠ¶æ€
        self._window_count = 0
    
    def _cleanup_memory(self, force: bool = False):
        """
        æ¸…ç†æ˜¾å­˜
        
        Args:
            force: æ˜¯å¦å¼ºåˆ¶æ¸…ç†ï¼ˆå¿½ç•¥gc_intervalï¼‰
        """
        self._window_count += 1
        gc_interval = getattr(self.config, 'gc_interval', 1)
        
        if force or gc_interval == 0 or self._window_count % gc_interval == 0:
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            gc.collect()
            self.logger.debug("  ğŸ§¹ å·²æ¸…ç†æ˜¾å­˜")
    
    def _offload_model(self, model: nn.Module):
        """
        å°†æ¨¡å‹ç§»åˆ°CPU
        
        Args:
            model: å¾…å¸è½½çš„æ¨¡å‹
        """
        if getattr(self.config, 'offload_to_cpu', True):
            model.cpu()
            self.logger.debug("  ğŸ“¤ æ¨¡å‹å·²ç§»è‡³CPU")
    
    def train(
        self,
        rolling_loaders,
        n_epochs: Optional[int] = None,
        save_dir: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        è®­ç»ƒæ‰€æœ‰æ»šåŠ¨çª—å£ï¼ˆè¦†ç›–çˆ¶ç±»ä»¥æ·»åŠ æ˜¾å­˜ç®¡ç†ï¼‰
        
        Args:
            rolling_loaders: æ»šåŠ¨çª—å£æ•°æ®åŠ è½½å™¨é›†åˆ
            n_epochs: æ¯ä¸ªçª—å£çš„è®­ç»ƒè½®æ•°
            save_dir: æ¨¡å‹ä¿å­˜ç›®å½•
            
        Returns:
            è®­ç»ƒç»“æœæ±‡æ€»å­—å…¸
        """
        n_epochs = n_epochs or self.config.window_epochs or self.config.n_epochs
        n_windows = len(rolling_loaders)
        
        self.logger.info("\n" + "=" * 80)
        self.logger.info("ğŸš€ å¼€å§‹æ—¥çº§æ»šåŠ¨çª—å£è®­ç»ƒ (Walk-Forward)")
        self.logger.info("=" * 80)
        self.logger.info(f"  æ€»çª—å£æ•°: {n_windows}")
        self.logger.info(f"  æ¯çª—å£è½®æ•°: {n_epochs}")
        self.logger.info(f"  æ˜¾å­˜æ¸…ç†é—´éš”: {getattr(self.config, 'gc_interval', 1)}")
        
        if save_dir:
            save_dir = Path(save_dir)
            save_dir.mkdir(parents=True, exist_ok=True)
            self.logger.info(f"  ä¿å­˜ç›®å½•: {save_dir}")
        
        start_time = time.time()
        self.window_results = []
        self.all_predictions = []
        self._window_count = 0
        
        for window_idx, loaders in enumerate(rolling_loaders):
            self.logger.info("\n" + "-" * 60)
            self.logger.info(f"ğŸ“… çª—å£ {window_idx + 1}/{n_windows}")
            self.logger.info("-" * 60)
            
            # è§£æ loaders
            train_loader = loaders.train if hasattr(loaders, 'train') else loaders[0]
            val_loader = loaders.val if hasattr(loaders, 'val') else (loaders[1] if len(loaders) > 1 else None)
            test_loader = loaders.test if hasattr(loaders, 'test') else (loaders[2] if len(loaders) > 2 else None)
            
            # æ£€æŸ¥è®­ç»ƒé›†æ˜¯å¦ä¸ºç©º
            if train_loader is None or len(train_loader) == 0:
                self.logger.warning(f"  âš ï¸ çª—å£ {window_idx + 1} è®­ç»ƒé›†ä¸ºç©ºï¼Œè·³è¿‡")
                continue
            
            # ç¡®å®šä¿å­˜è·¯å¾„
            save_path = None
            if save_dir and self.config.save_each_window:
                save_path = str(save_dir / f"window_{window_idx + 1}.pth")
            
            # æ£€æŸ¥æ˜¯å¦å­˜åœ¨å·²è®­ç»ƒæ¨¡å‹
            existing_checkpoint = self._check_existing_model(save_path)
            
            if existing_checkpoint:
                # ä½¿ç”¨å·²æœ‰æ¨¡å‹
                model = self.model_factory()
                model.to(self.device)
                model.load_state_dict(existing_checkpoint['model_state_dict'])
                
                window_result = WindowResult(
                    window_id=window_idx + 1,
                    best_epoch=existing_checkpoint.get('best_epoch', 0),
                    best_val_loss=existing_checkpoint.get('best_score', 0.0),
                    save_path=save_path,
                    skipped=True
                )
            else:
                # è®­ç»ƒæ–°æ¨¡å‹
                window_start = time.time()
                
                # è·å–æ¨¡å‹
                model = self._get_model_for_window(window_idx)
                
                # åˆ›å»ºçª—å£è®­ç»ƒå™¨
                from .simple_trainer import SimpleTrainer
                window_trainer = SimpleTrainer(model, self.config, str(self.device))
                
                # é‡ç½®ä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨
                if self.config.reset_optimizer or window_idx == 0:
                    window_trainer.optimizer = None
                if self.config.reset_scheduler or window_idx == 0:
                    window_trainer.scheduler = None
                
                # æ—¥å¿—
                self.logger.info(f"  è®­ç»ƒå¤©æ•°: {len(train_loader)}")
                if val_loader:
                    self.logger.info(f"  éªŒè¯å¤©æ•°: {len(val_loader)}")
                
                # è®­ç»ƒ
                train_result = window_trainer.train(
                    train_loader=train_loader,
                    val_loader=val_loader,
                    n_epochs=n_epochs,
                    save_path=save_path
                )
                
                window_elapsed = time.time() - window_start
                
                window_result = WindowResult(
                    window_id=window_idx + 1,
                    best_epoch=train_result['best_epoch'],
                    best_val_loss=train_result['best_score'],
                    train_losses=train_result['train_losses'],
                    val_losses=train_result['valid_losses'],
                    save_path=save_path,
                    elapsed_time=window_elapsed,
                    skipped=False
                )
                
                model = window_trainer.get_model()
            
            # ä¿å­˜æ¨¡å‹çŠ¶æ€ï¼ˆç”¨äºä¸‹ä¸€çª—å£ç»§æ‰¿ï¼‰
            if self.config.weight_inheritance:
                self.current_model_state = copy.deepcopy(model.state_dict())
            
            # åœ¨æµ‹è¯•é›†ä¸Šé¢„æµ‹
            if test_loader and len(test_loader) > 0:
                self.logger.info(f"  æµ‹è¯•å¤©æ•°: {len(test_loader)}")
                predictions = self._predict_daily_window(model, test_loader, window_idx)
                if predictions is not None:
                    window_result.predictions = predictions
                    self.all_predictions.append(predictions)
                    self.logger.info(f"  é¢„æµ‹æ ·æœ¬: {len(predictions):,}")
            
            self.window_results.append(window_result)
            
            self.logger.info(
                f"âœ… çª—å£ {window_idx + 1} å®Œæˆ | "
                f"best_epoch={window_result.best_epoch + 1} | "
                f"best_val_loss={window_result.best_val_loss:.6f}"
            )
            
            # æ˜¾å­˜ç®¡ç†ï¼šå°†æ¨¡å‹ç§»åˆ°CPUå¹¶æ¸…ç†ç¼“å­˜
            if getattr(self.config, 'clear_cache_on_window_end', True):
                self._offload_model(model)
                self._cleanup_memory()
            
            # ä¿æŒå½“å‰æ¨¡å‹å¼•ç”¨ï¼ˆåœ¨CPUä¸Šï¼‰
            self.current_model = model
        
        # æ±‡æ€»ç»Ÿè®¡
        elapsed = time.time() - start_time
        summary = self._build_summary(elapsed)
        
        self._print_summary(summary)
        
        return summary
    
    def _predict_daily_window(
        self,
        model: nn.Module,
        test_loader,
        window_idx: int
    ) -> Optional[pd.DataFrame]:
        """
        æ—¥çº§çª—å£é¢„æµ‹ï¼ˆé€‚é… DailyGraphDataLoaderï¼‰
        
        Args:
            model: è®­ç»ƒå¥½çš„æ¨¡å‹
            test_loader: æµ‹è¯•æ•°æ®åŠ è½½å™¨
            window_idx: çª—å£ç´¢å¼•
            
        Returns:
            é¢„æµ‹ç»“æœ DataFrame
        """
        model.eval()
        model.to(self.device)
        
        all_records = []
        
        with torch.no_grad():
            for batch_data in test_loader:
                # è§£æ DailyGraphDataLoader æ ¼å¼: (X, y, adj, stock_ids, date)
                if isinstance(batch_data, (list, tuple)):
                    if len(batch_data) >= 5:
                        x, y, adj, stock_ids, trade_date = batch_data[:5]
                    elif len(batch_data) >= 3:
                        x, y, adj = batch_data[:3]
                        stock_ids = batch_data[3] if len(batch_data) > 3 else None
                        trade_date = batch_data[4] if len(batch_data) > 4 else None
                    else:
                        x, y = batch_data[:2]
                        adj, stock_ids, trade_date = None, None, None
                else:
                    x = batch_data.get('x') or batch_data.get('features')
                    y = batch_data.get('y') or batch_data.get('labels')
                    adj = batch_data.get('adj')
                    stock_ids = batch_data.get('stock_ids')
                    trade_date = batch_data.get('trade_date')
                
                x = x.to(self.device)
                if adj is not None:
                    adj = adj.to(self.device)
                
                # å‰å‘ä¼ æ’­
                try:
                    pred = model(x, adj=adj) if adj is not None else model(x)
                except TypeError:
                    pred = model(x)
                
                if isinstance(pred, tuple):
                    pred = pred[0]
                
                pred = pred.cpu().numpy()
                
                # æ„å»ºè®°å½•
                n_samples = len(pred)
                
                # å¤„ç†æ ‡ç­¾
                if y is not None:
                    y_np = y.cpu().numpy() if torch.is_tensor(y) else np.array(y)
                else:
                    y_np = None
                
                for i in range(n_samples):
                    record = {
                        'pred': float(pred[i]) if pred.ndim == 1 else float(pred[i].item()),
                        'window_idx': window_idx + 1
                    }
                    
                    if y_np is not None:
                        record['y_true'] = float(y_np[i]) if y_np.ndim == 1 else float(y_np[i].item())
                    
                    if stock_ids is not None:
                        if isinstance(stock_ids, (list, np.ndarray)):
                            record['order_book_id'] = stock_ids[i]
                        elif torch.is_tensor(stock_ids):
                            record['order_book_id'] = stock_ids[i].item()
                    
                    if trade_date is not None:
                        if isinstance(trade_date, str):
                            record['trade_date'] = trade_date
                        elif hasattr(trade_date, 'strftime'):
                            record['trade_date'] = trade_date.strftime('%Y-%m-%d')
                        else:
                            record['trade_date'] = str(trade_date)
                    
                    all_records.append(record)
        
        if all_records:
            return pd.DataFrame(all_records)
        return None
    
    # ==================== å…¼å®¹æ—§API ====================
    
    def fit(
        self,
        rolling_loaders,
        n_epochs: Optional[int] = None,
        save_dir: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        è®­ç»ƒæ–¹æ³•ï¼ˆå…¼å®¹æ—§ rolling_daily_trainer.py çš„ fit æ¥å£ï¼‰
        
        Args:
            rolling_loaders: æ»šåŠ¨çª—å£æ•°æ®åŠ è½½å™¨
            n_epochs: æ¯çª—å£è½®æ•°
            save_dir: ä¿å­˜ç›®å½•
            
        Returns:
            è®­ç»ƒç»“æœæ±‡æ€»
        """
        return self.train(rolling_loaders, n_epochs, save_dir)


# ==================== å·¥å‚å‡½æ•° ====================

def create_rolling_daily_trainer(
    model_class,
    model_config: Dict[str, Any],
    d_feat: int,
    device: str = 'cuda',
    warm_start: bool = True,
    save_each_window: bool = True,
    **trainer_kwargs
) -> RollingDailyTrainer:
    """
    åˆ›å»ºæ—¥çº§æ»šåŠ¨è®­ç»ƒå™¨çš„ä¾¿æ·å‡½æ•°
    
    Args:
        model_class: æ¨¡å‹ç±»ï¼ˆå¦‚ HybridGraphModelï¼‰
        model_config: æ¨¡å‹é…ç½®å­—å…¸
        d_feat: ç‰¹å¾ç»´åº¦
        device: è®¡ç®—è®¾å¤‡
        warm_start: æ˜¯å¦ç»§æ‰¿æƒé‡
        save_each_window: æ˜¯å¦ä¿å­˜æ¯çª—å£æ¨¡å‹
        **trainer_kwargs: å…¶ä»–è®­ç»ƒé…ç½®
        
    Returns:
        RollingDailyTrainer å®ä¾‹
    """
    def model_factory():
        if hasattr(model_class, 'from_config'):
            return model_class.from_config(model_config, d_feat=d_feat).model
        else:
            return model_class(d_feat=d_feat, **model_config)
    
    config = DailyRollingConfig(
        weight_inheritance=warm_start,
        save_each_window=save_each_window,
        **trainer_kwargs
    )
    
    return RollingDailyTrainer(
        model_factory=model_factory,
        config=config,
        device=device
    )
