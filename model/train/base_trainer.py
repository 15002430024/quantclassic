"""
BaseTrainer - è®­ç»ƒåŸºç±»

å®šä¹‰é€šç”¨çš„è®­ç»ƒå¾ªç¯ã€æ—©åœã€æ£€æŸ¥ç‚¹ä¿å­˜å’Œæ—¥å¿—é€»è¾‘ã€‚
æ‰€æœ‰è®­ç»ƒå™¨éƒ½åº”ç»§æ‰¿æ­¤åŸºç±»ã€‚

æ ¸å¿ƒè®¾è®¡:
1. TrainerArtifacts: è®­ç»ƒæ‰€éœ€çš„æ‰€æœ‰ç»„ä»¶ï¼ˆæ¨¡å‹ã€ä¼˜åŒ–å™¨ã€åŠ è½½å™¨ç­‰ï¼‰
2. TrainerConfig: è®­ç»ƒé…ç½®å‚æ•°å®¹å™¨
3. TrainerCallback: è®­ç»ƒè¿‡ç¨‹ä¸­çš„å›è°ƒæœºåˆ¶
4. BaseTrainer: è®­ç»ƒå¾ªç¯çš„æŠ½è±¡åŸºç±»
"""

import abc
import logging
import copy
import time
from dataclasses import dataclass, field
from pathlib import Path
from typing import (
    Any, Callable, Dict, List, Optional, Tuple, Union
)

import torch
import torch.nn as nn
from torch.utils.data import DataLoader

# ä½¿ç”¨ç›¸å¯¹å¯¼å…¥ï¼ˆç›¸å¯¹äº quantclassic åŒ…ï¼‰
# ç§»é™¤é™çº§ç‰ˆ BaseConfigï¼Œå¼ºåˆ¶ä¾èµ–æ­£ç¡®çš„åŸºç±»
try:
    from ...config.base_config import BaseConfig
except ImportError:
    # ç›´æ¥è¿è¡Œè„šæœ¬æ—¶çš„åå¤‡å¯¼å…¥
    from config.base_config import BaseConfig


# ==================== é…ç½®æ•°æ®ç±» ====================

@dataclass
class TrainerConfig(BaseConfig):
    """
    è®­ç»ƒå™¨é…ç½®
    
    åŒ…å«æ‰€æœ‰è®­ç»ƒç›¸å…³çš„è¶…å‚æ•°ï¼Œé€šè¿‡é…ç½®é©±åŠ¨è®­ç»ƒè¡Œä¸ºã€‚
    
    Args:
        n_epochs: è®­ç»ƒè½®æ•°
        lr: å­¦ä¹ ç‡
        weight_decay: L2 æ­£åˆ™åŒ–ç³»æ•°
        early_stop: æ—©åœè€å¿ƒå€¼
        optimizer: ä¼˜åŒ–å™¨åç§° ('adam', 'sgd', 'adamw')
        loss_fn: æŸå¤±å‡½æ•°åç§° ('mse', 'mae', 'huber', 'ic')
        loss_kwargs: æŸå¤±å‡½æ•°é¢å¤–å‚æ•°
        use_scheduler: æ˜¯å¦ä½¿ç”¨å­¦ä¹ ç‡è°ƒåº¦å™¨
        scheduler_type: è°ƒåº¦å™¨ç±»å‹ ('plateau', 'cosine', 'step')
        scheduler_patience: ReduceLROnPlateau çš„è€å¿ƒå€¼
        scheduler_factor: å­¦ä¹ ç‡è¡°å‡å› å­
        scheduler_min_lr: æœ€å°å­¦ä¹ ç‡
        lambda_corr: ç›¸å…³æ€§æ­£åˆ™åŒ–æƒé‡
        checkpoint_dir: æ£€æŸ¥ç‚¹ä¿å­˜ç›®å½•
        save_best_only: æ˜¯å¦åªä¿å­˜æœ€ä½³æ¨¡å‹
        verbose: æ˜¯å¦æ‰“å°è¯¦ç»†æ—¥å¿—
        log_interval: æ—¥å¿—æ‰“å°é—´éš”ï¼ˆbatchæ•°ï¼‰
    """
    # åŸºæœ¬è®­ç»ƒå‚æ•°
    n_epochs: int = 100
    lr: float = 0.001
    weight_decay: float = 0.0
    early_stop: int = 20
    
    # ä¼˜åŒ–å™¨é…ç½®
    optimizer: str = 'adam'
    
    # æŸå¤±å‡½æ•°é…ç½®
    loss_fn: str = 'mse'
    loss_kwargs: Dict[str, Any] = field(default_factory=dict)
    lambda_corr: float = 0.0
    
    # å­¦ä¹ ç‡è°ƒåº¦å™¨é…ç½®
    use_scheduler: bool = True
    scheduler_type: str = 'plateau'
    scheduler_patience: int = 5
    scheduler_factor: float = 0.5
    scheduler_min_lr: float = 1e-6
    
    # æ£€æŸ¥ç‚¹é…ç½®
    checkpoint_dir: Optional[str] = None
    save_best_only: bool = True
    
    # æ—¥å¿—é…ç½®
    verbose: bool = True
    log_interval: int = 50
    
    def validate(self) -> bool:
        """éªŒè¯é…ç½®æœ‰æ•ˆæ€§"""
        if self.n_epochs <= 0:
            raise ValueError("n_epochs å¿…é¡»å¤§äº 0")
        if self.lr <= 0:
            raise ValueError("lr å¿…é¡»å¤§äº 0")
        if self.early_stop < 0:
            raise ValueError("early_stop ä¸èƒ½ä¸ºè´Ÿæ•°")
        if self.optimizer not in ['adam', 'sgd', 'adamw']:
            raise ValueError(f"ä¸æ”¯æŒçš„ä¼˜åŒ–å™¨: {self.optimizer}")
        
        # ğŸ†• æ‰©å±•æŸå¤±å‡½æ•°æ”¯æŒåˆ—è¡¨ï¼Œä¸ loss.get_loss_fn ä¿æŒä¸€è‡´
        supported_losses = [
            'mse', 'mae', 'huber', 'ic',  # æ ‡å‡†æŸå¤±
            'mse_corr', 'mae_corr', 'huber_corr', 'ic_corr',  # å¸¦ç›¸å…³æ€§æ­£åˆ™
            'combined', 'unified'  # ç»„åˆ/ç»Ÿä¸€æŸå¤±
        ]
        if self.loss_fn not in supported_losses:
            raise ValueError(
                f"ä¸æ”¯æŒçš„æŸå¤±å‡½æ•°: {self.loss_fn}. "
                f"æ”¯æŒçš„æŸå¤±: {', '.join(supported_losses)}"
            )
        return True


@dataclass
class TrainerArtifacts:
    """
    è®­ç»ƒç»„ä»¶å®¹å™¨
    
    å°è£…è®­ç»ƒæ‰€éœ€çš„æ‰€æœ‰ç»„ä»¶ï¼Œç»Ÿä¸€ä¼ é€’ç»™ Trainerã€‚
    
    Args:
        model: PyTorch æ¨¡å‹ (nn.Module)
        optimizer: ä¼˜åŒ–å™¨
        scheduler: å­¦ä¹ ç‡è°ƒåº¦å™¨ï¼ˆå¯é€‰ï¼‰
        criterion: æŸå¤±å‡½æ•°
        train_loader: è®­ç»ƒæ•°æ®åŠ è½½å™¨
        val_loader: éªŒè¯æ•°æ®åŠ è½½å™¨ï¼ˆå¯é€‰ï¼‰
        test_loader: æµ‹è¯•æ•°æ®åŠ è½½å™¨ï¼ˆå¯é€‰ï¼‰
        device: è®¡ç®—è®¾å¤‡
        metrics: è¯„ä¼°æŒ‡æ ‡å­—å…¸ï¼ˆå¯é€‰ï¼‰
        callbacks: å›è°ƒåˆ—è¡¨ï¼ˆå¯é€‰ï¼‰
    """
    model: nn.Module
    optimizer: torch.optim.Optimizer
    criterion: nn.Module
    train_loader: DataLoader
    device: torch.device
    
    scheduler: Optional[Any] = None
    val_loader: Optional[DataLoader] = None
    test_loader: Optional[DataLoader] = None
    metrics: Optional[Dict[str, Callable]] = None
    callbacks: Optional[List['TrainerCallback']] = None


# ==================== å›è°ƒæœºåˆ¶ ====================

class TrainerCallback(abc.ABC):
    """
    è®­ç»ƒå›è°ƒåŸºç±»
    
    å®šä¹‰è®­ç»ƒè¿‡ç¨‹ä¸­çš„é’©å­å‡½æ•°ï¼Œå­ç±»å¯é‡å†™ä»¥å®ç°è‡ªå®šä¹‰è¡Œä¸ºã€‚
    """
    
    def on_train_begin(self, trainer: 'BaseTrainer', **kwargs):
        """è®­ç»ƒå¼€å§‹æ—¶è°ƒç”¨"""
        pass
    
    def on_train_end(self, trainer: 'BaseTrainer', **kwargs):
        """è®­ç»ƒç»“æŸæ—¶è°ƒç”¨"""
        pass
    
    def on_epoch_begin(self, trainer: 'BaseTrainer', epoch: int, **kwargs):
        """æ¯ä¸ª epoch å¼€å§‹æ—¶è°ƒç”¨"""
        pass
    
    def on_epoch_end(self, trainer: 'BaseTrainer', epoch: int, 
                     train_loss: float, val_loss: Optional[float] = None, **kwargs):
        """æ¯ä¸ª epoch ç»“æŸæ—¶è°ƒç”¨"""
        pass
    
    def on_batch_begin(self, trainer: 'BaseTrainer', batch_idx: int, **kwargs):
        """æ¯ä¸ª batch å¼€å§‹æ—¶è°ƒç”¨"""
        pass
    
    def on_batch_end(self, trainer: 'BaseTrainer', batch_idx: int, 
                     loss: float, **kwargs):
        """æ¯ä¸ª batch ç»“æŸæ—¶è°ƒç”¨"""
        pass


class EarlyStoppingCallback(TrainerCallback):
    """
    æ—©åœå›è°ƒ
    
    å½“éªŒè¯æŸå¤±ä¸å†æ”¹å–„æ—¶åœæ­¢è®­ç»ƒã€‚
    
    Args:
        patience: ç­‰å¾…æ”¹å–„çš„ epoch æ•°
        min_delta: è¢«è§†ä¸ºæ”¹å–„çš„æœ€å°å˜åŒ–é‡
        mode: 'min' æˆ– 'max'ï¼Œç›‘æ§æŒ‡æ ‡çš„ä¼˜åŒ–æ–¹å‘
    """
    
    def __init__(self, patience: int = 10, min_delta: float = 0.0, mode: str = 'min'):
        self.patience = patience
        self.min_delta = min_delta
        self.mode = mode
        
        self.best_score = float('inf') if mode == 'min' else float('-inf')
        self.counter = 0
        self.best_epoch = 0
        self.should_stop = False
    
    def _is_improvement(self, score: float) -> bool:
        """æ£€æŸ¥æ˜¯å¦æœ‰æ”¹å–„"""
        if self.mode == 'min':
            return score < self.best_score - self.min_delta
        else:
            return score > self.best_score + self.min_delta
    
    def on_epoch_end(self, trainer: 'BaseTrainer', epoch: int,
                     train_loss: float, val_loss: Optional[float] = None, **kwargs):
        """æ£€æŸ¥æ˜¯å¦åº”è¯¥æ—©åœ"""
        score = val_loss if val_loss is not None else train_loss
        
        if self._is_improvement(score):
            self.best_score = score
            self.best_epoch = epoch
            self.counter = 0
        else:
            self.counter += 1
            if self.counter >= self.patience:
                self.should_stop = True
                trainer.logger.info(
                    f"âš¡ æ—©åœè§¦å‘ at epoch {epoch + 1} "
                    f"(best epoch: {self.best_epoch + 1}, best score: {self.best_score:.6f})"
                )


class CheckpointCallback(TrainerCallback):
    """
    æ£€æŸ¥ç‚¹ä¿å­˜å›è°ƒ
    
    å®šæœŸä¿å­˜æ¨¡å‹æ£€æŸ¥ç‚¹ã€‚
    
    Args:
        checkpoint_dir: ä¿å­˜ç›®å½•
        save_best_only: æ˜¯å¦åªä¿å­˜æœ€ä½³æ¨¡å‹
        monitor: ç›‘æ§çš„æŒ‡æ ‡åç§°
        mode: 'min' æˆ– 'max'
    """
    
    def __init__(self, checkpoint_dir: str, save_best_only: bool = True,
                 monitor: str = 'val_loss', mode: str = 'min'):
        self.checkpoint_dir = Path(checkpoint_dir)
        self.checkpoint_dir.mkdir(parents=True, exist_ok=True)
        self.save_best_only = save_best_only
        self.monitor = monitor
        self.mode = mode
        
        self.best_score = float('inf') if mode == 'min' else float('-inf')
    
    def _is_better(self, score: float) -> bool:
        """æ£€æŸ¥åˆ†æ•°æ˜¯å¦æ›´å¥½"""
        if self.mode == 'min':
            return score < self.best_score
        else:
            return score > self.best_score
    
    def on_epoch_end(self, trainer: 'BaseTrainer', epoch: int,
                     train_loss: float, val_loss: Optional[float] = None, **kwargs):
        """ä¿å­˜æ£€æŸ¥ç‚¹"""
        score = val_loss if val_loss is not None else train_loss
        
        should_save = False
        if self.save_best_only:
            if self._is_better(score):
                self.best_score = score
                should_save = True
        else:
            should_save = True
        
        if should_save:
            checkpoint_path = self.checkpoint_dir / f"checkpoint_epoch_{epoch + 1}.pth"
            trainer.save_checkpoint(checkpoint_path)
            
            if self.save_best_only:
                # åŒæ—¶ä¿å­˜ä¸º best.pth
                best_path = self.checkpoint_dir / "best.pth"
                trainer.save_checkpoint(best_path)


# ==================== åŸºç¡€è®­ç»ƒå™¨ ====================

class BaseTrainer(abc.ABC):
    """
    è®­ç»ƒåŸºç±»
    
    å®šä¹‰é€šç”¨çš„è®­ç»ƒå¾ªç¯æ¡†æ¶ï¼Œå­ç±»åªéœ€å®ç° train_batch å’Œ validate_epochã€‚
    
    æ ¸å¿ƒæ–¹æ³•:
    - train(): ä¸»è®­ç»ƒå¾ªç¯
    - train_epoch(): å•ä¸ª epoch è®­ç»ƒ
    - train_batch(): å•ä¸ª batch è®­ç»ƒï¼ˆæŠ½è±¡ï¼Œå­ç±»å®ç°ï¼‰
    - validate_epoch(): éªŒè¯ä¸€ä¸ª epochï¼ˆæŠ½è±¡ï¼Œå­ç±»å®ç°ï¼‰
    - save_checkpoint(): ä¿å­˜æ£€æŸ¥ç‚¹
    - load_checkpoint(): åŠ è½½æ£€æŸ¥ç‚¹
    """
    
    def __init__(
        self,
        model: nn.Module,
        config: TrainerConfig,
        device: Optional[str] = None,
        **kwargs
    ):
        """
        åˆå§‹åŒ–è®­ç»ƒå™¨
        
        Args:
            model: PyTorch æ¨¡å‹
            config: è®­ç»ƒé…ç½®
            device: è®¡ç®—è®¾å¤‡
        """
        self.model = model
        self.config = config
        self.device = torch.device(
            device if device else ('cuda' if torch.cuda.is_available() else 'cpu')
        )
        
        # ç§»åŠ¨æ¨¡å‹åˆ°è®¾å¤‡
        self.model = self.model.to(self.device)
        
        # åˆå§‹åŒ–ä¼˜åŒ–å™¨å’ŒæŸå¤±å‡½æ•°ï¼ˆå»¶è¿Ÿåˆ° train æ—¶åˆ›å»ºï¼‰
        self.optimizer: Optional[torch.optim.Optimizer] = None
        self.scheduler: Optional[Any] = None
        self.criterion: Optional[nn.Module] = None
        
        # è®­ç»ƒçŠ¶æ€
        self.current_epoch = 0
        self.global_step = 0
        self.train_losses: List[float] = []
        self.valid_losses: List[float] = []
        self.lr_history: List[float] = []
        self.best_score = float('inf')
        self.best_epoch = 0
        
        # å›è°ƒ
        self.callbacks: List[TrainerCallback] = []
        
        # æ—¥å¿—
        self.logger = self._setup_logger()
        
        self.logger.info(f"åˆå§‹åŒ– {self.__class__.__name__}:")
        self.logger.info(f"  è®¾å¤‡: {self.device}")
        self.logger.info(f"  è®­ç»ƒè½®æ•°: {config.n_epochs}")
        self.logger.info(f"  å­¦ä¹ ç‡: {config.lr}")
    
    def _setup_logger(self) -> logging.Logger:
        """é…ç½®æ—¥å¿—"""
        logger = logging.getLogger(self.__class__.__name__)
        logger.setLevel(logging.INFO)
        
        if not logger.handlers:
            handler = logging.StreamHandler()
            formatter = logging.Formatter(
                '%(asctime)s - %(name)s - %(levelname)s - %(message)s',
                datefmt='%Y-%m-%d %H:%M:%S'
            )
            handler.setFormatter(formatter)
            logger.addHandler(handler)
        
        return logger
    
    def _create_optimizer(self) -> torch.optim.Optimizer:
        """åˆ›å»ºä¼˜åŒ–å™¨"""
        opt_name = self.config.optimizer.lower()
        params = self.model.parameters()
        lr = self.config.lr
        wd = self.config.weight_decay
        
        if opt_name == 'adam':
            return torch.optim.Adam(params, lr=lr, weight_decay=wd)
        elif opt_name == 'sgd':
            return torch.optim.SGD(params, lr=lr, weight_decay=wd)
        elif opt_name == 'adamw':
            return torch.optim.AdamW(params, lr=lr, weight_decay=wd)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„ä¼˜åŒ–å™¨: {opt_name}")
    
    def _create_scheduler(self) -> Optional[Any]:
        """åˆ›å»ºå­¦ä¹ ç‡è°ƒåº¦å™¨"""
        if not self.config.use_scheduler or self.optimizer is None:
            return None
        
        sched_type = self.config.scheduler_type.lower()
        
        if sched_type == 'plateau':
            return torch.optim.lr_scheduler.ReduceLROnPlateau(
                self.optimizer,
                mode='min',
                factor=self.config.scheduler_factor,
                patience=self.config.scheduler_patience,
                min_lr=self.config.scheduler_min_lr,
                verbose=True
            )
        elif sched_type == 'cosine':
            return torch.optim.lr_scheduler.CosineAnnealingLR(
                self.optimizer,
                T_max=self.config.n_epochs,
                eta_min=self.config.scheduler_min_lr
            )
        elif sched_type == 'step':
            return torch.optim.lr_scheduler.StepLR(
                self.optimizer,
                step_size=self.config.scheduler_patience,
                gamma=self.config.scheduler_factor
            )
        else:
            self.logger.warning(f"æœªçŸ¥çš„è°ƒåº¦å™¨ç±»å‹: {sched_type}")
            return None
    
    def _create_criterion(self) -> nn.Module:
        """åˆ›å»ºæŸå¤±å‡½æ•°"""
        loss_name = self.config.loss_fn.lower()
        loss_kwargs = self.config.loss_kwargs
        
        # ğŸ†• ä¼˜å…ˆä½¿ç”¨ loss æ¨¡å—çš„ get_loss_fn å·¥å‚å‡½æ•°
        try:
            from ..loss import get_loss_fn
            
            # å¦‚æœå¯ç”¨ç›¸å…³æ€§æ­£åˆ™åŒ–ä¸”æŸå¤±åç§°ä¸å¸¦ _corr
            if self.config.lambda_corr > 0 and not loss_name.endswith('_corr'):
                if loss_name in ['mse', 'mae', 'huber', 'ic']:
                    loss_name = f"{loss_name}_corr"
            
            return get_loss_fn(
                loss_type=loss_name,
                lambda_corr=self.config.lambda_corr,
                **loss_kwargs
            )
        except ImportError:
            self.logger.warning("æ— æ³•å¯¼å…¥ loss æ¨¡å—ï¼Œä½¿ç”¨æ ‡å‡†æŸå¤±")
        except ValueError as e:
            self.logger.warning(f"get_loss_fn ä¸æ”¯æŒ {loss_name}: {e}")
        
        # å¤‡é€‰: æ ‡å‡†æŸå¤±å‡½æ•°
        if loss_name in ['mse', 'mse_corr']:
            return nn.MSELoss()
        elif loss_name in ['mae', 'mae_corr']:
            return nn.L1Loss()
        elif loss_name in ['huber', 'huber_corr']:
            delta = loss_kwargs.get('delta', 1.0)
            return nn.HuberLoss(delta=delta)
        elif loss_name in ['ic', 'ic_corr']:
            # IC æŸå¤±å›é€€åˆ° MSE
            self.logger.warning(f"IC æŸå¤±éœ€è¦ loss æ¨¡å—æ”¯æŒï¼Œå›é€€åˆ° MSE")
            return nn.MSELoss()
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æŸå¤±å‡½æ•°: {loss_name}")
    
    def _step_scheduler(self, val_loss: Optional[float] = None):
        """æ›´æ–°å­¦ä¹ ç‡è°ƒåº¦å™¨"""
        if self.scheduler is None:
            return
        
        current_lr = self.optimizer.param_groups[0]['lr']
        self.lr_history.append(current_lr)
        
        sched_type = self.config.scheduler_type.lower()
        if sched_type == 'plateau' and val_loss is not None:
            self.scheduler.step(val_loss)
        else:
            self.scheduler.step()
        
        new_lr = self.optimizer.param_groups[0]['lr']
        if new_lr != current_lr:
            self.logger.info(f"  ğŸ“‰ å­¦ä¹ ç‡è°ƒæ•´: {current_lr:.2e} â†’ {new_lr:.2e}")
    
    def _parse_batch_data(self, batch_data) -> Tuple[Any, Any, Any, Any]:
        """
        è§£æ Batch æ•°æ®
        
        æ”¯æŒå¤šç§æ ¼å¼ï¼š
        - (x, y): åŸºç¡€æ ¼å¼
        - (x, y, adj): å¸¦é‚»æ¥çŸ©é˜µ
        - (x, y, adj, idx): å¸¦é‚»æ¥çŸ©é˜µå’Œè‚¡ç¥¨ç´¢å¼•
        - dict: å­—å…¸æ ¼å¼
        
        Returns:
            (x, y, adj, idx) - ç‰¹å¾ã€æ ‡ç­¾ã€é‚»æ¥çŸ©é˜µã€è‚¡ç¥¨ç´¢å¼•
        """
        if isinstance(batch_data, dict):
            x = batch_data.get('x') or batch_data.get('features') or batch_data.get('input')
            y = batch_data.get('y') or batch_data.get('labels') or batch_data.get('target')
            adj = batch_data.get('adj') or batch_data.get('adj_matrix')
            idx = batch_data.get('stock_idx') or batch_data.get('idx')
            return x, y, adj, idx
        
        if isinstance(batch_data, (list, tuple)):
            if len(batch_data) == 2:
                return batch_data[0], batch_data[1], None, None
            elif len(batch_data) == 3:
                return batch_data[0], batch_data[1], batch_data[2], None
            elif len(batch_data) >= 4:
                return batch_data[0], batch_data[1], batch_data[2], batch_data[3]
        
        return batch_data, None, None, None
    
    def add_callback(self, callback: TrainerCallback):
        """æ·»åŠ å›è°ƒ"""
        self.callbacks.append(callback)
    
    def _run_callbacks(self, method: str, **kwargs):
        """è¿è¡Œæ‰€æœ‰å›è°ƒçš„æŒ‡å®šæ–¹æ³•"""
        for callback in self.callbacks:
            getattr(callback, method)(self, **kwargs)
    
    def train(
        self,
        train_loader: DataLoader,
        val_loader: Optional[DataLoader] = None,
        n_epochs: Optional[int] = None,
        save_path: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        ä¸»è®­ç»ƒå¾ªç¯
        
        Args:
            train_loader: è®­ç»ƒæ•°æ®åŠ è½½å™¨
            val_loader: éªŒè¯æ•°æ®åŠ è½½å™¨
            n_epochs: è®­ç»ƒè½®æ•°ï¼ˆè¦†ç›–é…ç½®ï¼‰
            save_path: æ¨¡å‹ä¿å­˜è·¯å¾„
            
        Returns:
            è®­ç»ƒç»“æœå­—å…¸
        """
        n_epochs = n_epochs or self.config.n_epochs
        
        # åˆå§‹åŒ–ä¼˜åŒ–å™¨ã€è°ƒåº¦å™¨ã€æŸå¤±å‡½æ•°
        if self.optimizer is None:
            self.optimizer = self._create_optimizer()
        if self.scheduler is None:
            self.scheduler = self._create_scheduler()
        if self.criterion is None:
            self.criterion = self._create_criterion()
        
        # è®¾ç½®æ—©åœå›è°ƒ
        early_stopping = EarlyStoppingCallback(patience=self.config.early_stop)
        self.add_callback(early_stopping)
        
        # è®¾ç½®æ£€æŸ¥ç‚¹å›è°ƒ
        if self.config.checkpoint_dir:
            checkpoint_callback = CheckpointCallback(
                checkpoint_dir=self.config.checkpoint_dir,
                save_best_only=self.config.save_best_only
            )
            self.add_callback(checkpoint_callback)
        
        self.logger.info("=" * 60)
        self.logger.info(f"ğŸš€ å¼€å§‹è®­ç»ƒ ({n_epochs} epochs)")
        self.logger.info("=" * 60)
        
        start_time = time.time()
        self._run_callbacks('on_train_begin')
        
        for epoch in range(n_epochs):
            self.current_epoch = epoch
            self._run_callbacks('on_epoch_begin', epoch=epoch)
            
            # è®­ç»ƒä¸€ä¸ª epoch
            train_loss = self.train_epoch(train_loader)
            self.train_losses.append(train_loss)
            
            # éªŒè¯
            val_loss = None
            if val_loader is not None:
                val_loss = self.validate_epoch(val_loader)
                self.valid_losses.append(val_loss)
            
            # æ—¥å¿—
            if val_loss is not None:
                self.logger.info(
                    f"Epoch {epoch + 1}/{n_epochs} - "
                    f"Train Loss: {train_loss:.6f}, Val Loss: {val_loss:.6f}"
                )
            else:
                self.logger.info(
                    f"Epoch {epoch + 1}/{n_epochs} - Train Loss: {train_loss:.6f}"
                )
            
            # æ›´æ–°è°ƒåº¦å™¨
            self._step_scheduler(val_loss if val_loss is not None else train_loss)
            
            # æ›´æ–°æœ€ä½³åˆ†æ•°
            current_score = val_loss if val_loss is not None else train_loss
            if current_score < self.best_score:
                self.best_score = current_score
                self.best_epoch = epoch
                if save_path:
                    self.save_checkpoint(save_path)
            
            # è¿è¡Œå›è°ƒ
            self._run_callbacks(
                'on_epoch_end', 
                epoch=epoch,
                train_loss=train_loss,
                val_loss=val_loss
            )
            
            # æ£€æŸ¥æ—©åœ
            if early_stopping.should_stop:
                break
        
        elapsed = time.time() - start_time
        self._run_callbacks('on_train_end')
        
        self.logger.info("=" * 60)
        self.logger.info(f"âœ… è®­ç»ƒå®Œæˆ!")
        self.logger.info(f"  æ€»è€—æ—¶: {elapsed:.1f}s")
        self.logger.info(f"  æœ€ä½³ epoch: {self.best_epoch + 1}")
        self.logger.info(f"  æœ€ä½³åˆ†æ•°: {self.best_score:.6f}")
        self.logger.info("=" * 60)
        
        return {
            'train_losses': self.train_losses,
            'valid_losses': self.valid_losses,
            'lr_history': self.lr_history,
            'best_score': self.best_score,
            'best_epoch': self.best_epoch,
            'elapsed_time': elapsed
        }
    
    def train_epoch(self, train_loader: DataLoader) -> float:
        """
        è®­ç»ƒä¸€ä¸ª epoch
        
        Args:
            train_loader: è®­ç»ƒæ•°æ®åŠ è½½å™¨
            
        Returns:
            å¹³å‡è®­ç»ƒæŸå¤±
        """
        self.model.train()
        total_loss = 0.0
        n_batches = 0
        
        for batch_idx, batch_data in enumerate(train_loader):
            self._run_callbacks('on_batch_begin', batch_idx=batch_idx)
            
            loss = self.train_batch(batch_data)
            total_loss += loss
            n_batches += 1
            self.global_step += 1
            
            self._run_callbacks('on_batch_end', batch_idx=batch_idx, loss=loss)
            
            # æ—¥å¿—
            if self.config.verbose and batch_idx % self.config.log_interval == 0:
                self.logger.debug(f"  Batch {batch_idx}: loss={loss:.6f}")
        
        return total_loss / max(n_batches, 1)
    
    @abc.abstractmethod
    def train_batch(self, batch_data) -> float:
        """
        è®­ç»ƒå•ä¸ª batchï¼ˆæŠ½è±¡æ–¹æ³•ï¼Œå­ç±»å®ç°ï¼‰
        
        Args:
            batch_data: DataLoader è¿”å›çš„ batch æ•°æ®
            
        Returns:
            batch æŸå¤±å€¼
        """
        raise NotImplementedError
    
    @abc.abstractmethod
    def validate_epoch(self, val_loader: DataLoader) -> float:
        """
        éªŒè¯ä¸€ä¸ª epochï¼ˆæŠ½è±¡æ–¹æ³•ï¼Œå­ç±»å®ç°ï¼‰
        
        Args:
            val_loader: éªŒè¯æ•°æ®åŠ è½½å™¨
            
        Returns:
            å¹³å‡éªŒè¯æŸå¤±
        """
        raise NotImplementedError
    
    def save_checkpoint(self, path: Union[str, Path]):
        """ä¿å­˜æ£€æŸ¥ç‚¹"""
        path = Path(path)
        path.parent.mkdir(parents=True, exist_ok=True)
        
        checkpoint = {
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict() if self.optimizer else None,
            'scheduler_state_dict': self.scheduler.state_dict() if self.scheduler else None,
            'config': self.config.to_dict(),
            'current_epoch': self.current_epoch,
            'global_step': self.global_step,
            'train_losses': self.train_losses,
            'valid_losses': self.valid_losses,
            'best_score': self.best_score,
            'best_epoch': self.best_epoch,
        }
        
        torch.save(checkpoint, path)
        self.logger.info(f"ğŸ’¾ æ£€æŸ¥ç‚¹å·²ä¿å­˜: {path}")
    
    def load_checkpoint(self, path: Union[str, Path]):
        """åŠ è½½æ£€æŸ¥ç‚¹"""
        path = Path(path)
        if not path.exists():
            raise FileNotFoundError(f"æ£€æŸ¥ç‚¹ä¸å­˜åœ¨: {path}")
        
        checkpoint = torch.load(path, map_location=self.device)
        
        self.model.load_state_dict(checkpoint['model_state_dict'])
        
        if self.optimizer and checkpoint.get('optimizer_state_dict'):
            self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        
        if self.scheduler and checkpoint.get('scheduler_state_dict'):
            self.scheduler.load_state_dict(checkpoint['scheduler_state_dict'])
        
        self.current_epoch = checkpoint.get('current_epoch', 0)
        self.global_step = checkpoint.get('global_step', 0)
        self.train_losses = checkpoint.get('train_losses', [])
        self.valid_losses = checkpoint.get('valid_losses', [])
        self.best_score = checkpoint.get('best_score', float('inf'))
        self.best_epoch = checkpoint.get('best_epoch', 0)
        
        self.logger.info(f"ğŸ“‚ æ£€æŸ¥ç‚¹å·²åŠ è½½: {path}")
    
    def get_model(self) -> nn.Module:
        """è·å–æ¨¡å‹"""
        return self.model
    
    def get_state_dict(self) -> Dict:
        """è·å–æ¨¡å‹çŠ¶æ€å­—å…¸"""
        return copy.deepcopy(self.model.state_dict())
    
    def load_state_dict(self, state_dict: Dict):
        """åŠ è½½æ¨¡å‹çŠ¶æ€å­—å…¸"""
        self.model.load_state_dict(state_dict)
