"""
ModularConfig - æ¨¡å—åŒ–é…ç½®ç³»ç»Ÿ

å°†æ··åˆæ¨¡å‹æ‹†åˆ†ä¸ºç‹¬ç«‹çš„æ¨¡å—é…ç½®ï¼Œæ”¯æŒçµæ´»ç»„åˆå’Œæ‰©å±•å˜ä½“ã€‚

è®¾è®¡æ€æƒ³:
- æ¯ä¸ªåŠŸèƒ½æ¨¡å— (Temporal/Graph/Fusion) éƒ½æœ‰ç‹¬ç«‹çš„é…ç½®ç±»
- é€šè¿‡ç»„åˆå™¨ (CompositeConfig) å°†å¤šä¸ªæ¨¡å—é…ç½®ç»„åˆæˆå®Œæ•´æ¨¡å‹
- æ”¯æŒæ’ä»¶å¼æ‰©å±•ï¼šè½»æ¾æ·»åŠ æ–°çš„æ¨¡å—ç±»å‹æˆ–å˜ä½“

Example:
    # æ–¹å¼1: ä½¿ç”¨ç‹¬ç«‹æ¨¡å—é…ç½®
    temporal_cfg = TemporalModuleConfig(rnn_type='lstm', hidden_size=64, use_attention=True)
    graph_cfg = GraphModuleConfig(gat_type='correlation', hidden_dim=32, heads=4)
    fusion_cfg = FusionModuleConfig(hidden_sizes=[64, 32])
    
    model_cfg = CompositeModelConfig(
        temporal=temporal_cfg,
        graph=graph_cfg,
        fusion=fusion_cfg,
        d_feat=20
    )
    
    # æ–¹å¼2: ä½¿ç”¨æ„å»ºå™¨å¿«é€Ÿåˆ›å»º
    model_cfg = ModelConfigBuilder() \\
        .add_temporal(rnn_type='gru', hidden_size=128) \\
        .add_graph(gat_type='standard', hidden_dim=64) \\
        .add_fusion(hidden_sizes=[128, 64]) \\
        .build(d_feat=20)
"""

from dataclasses import dataclass, field
from typing import Optional, List, Dict, Any, Union
from enum import Enum
from pathlib import Path

# ä½¿ç”¨ç›¸å¯¹å¯¼å…¥ï¼ˆç›¸å¯¹äº quantclassic åŒ…ï¼‰
# ç§»é™¤é™çº§ç‰ˆ BaseConfigï¼Œå¼ºåˆ¶ä¾èµ–æ­£ç¡®çš„åŸºç±»
try:
    from ..config.base_config import BaseConfig
except ImportError:
    # ç›´æ¥è¿è¡Œè„šæœ¬æ—¶çš„åå¤‡å¯¼å…¥
    from config.base_config import BaseConfig

# å¯¼å…¥ BaseModelConfigï¼ˆç”¨äºæ¨¡å‹é…ç½®ç»§æ‰¿ï¼‰
try:
    from .model_config import BaseModelConfig
except ImportError:
    # ç›´æ¥è¿è¡Œè„šæœ¬æ—¶çš„åå¤‡å¯¼å…¥
    from model.model_config import BaseModelConfig


# ==================== æ¨¡å—ç±»å‹æšä¸¾ ====================

class ModuleType(str, Enum):
    """æ¨¡å—ç±»å‹æšä¸¾"""
    TEMPORAL = 'temporal'      # æ—¶åºç‰¹å¾æå–æ¨¡å—
    GRAPH = 'graph'           # å›¾ç¥ç»ç½‘ç»œæ¨¡å—
    FUSION = 'fusion'         # ç‰¹å¾èåˆæ¨¡å—
    ATTENTION = 'attention'   # æ³¨æ„åŠ›æœºåˆ¶æ¨¡å—


class RNNType(str, Enum):
    """RNNç±»å‹"""
    LSTM = 'lstm'
    GRU = 'gru'
    RNN = 'rnn'


class GATType(str, Enum):
    """GATç±»å‹"""
    STANDARD = 'standard'         # åŸºäºè¡Œä¸šå…³ç³»
    CORRELATION = 'correlation'   # åŸºäºæ”¶ç›Šç‡ç›¸å…³æ€§
    DYNAMIC = 'dynamic'           # åŠ¨æ€æ„å»ºå›¾ç»“æ„


class AttentionType(str, Enum):
    """æ³¨æ„åŠ›æœºåˆ¶ç±»å‹"""
    SELF = 'self'                 # Self-Attention
    MULTI_HEAD = 'multi_head'     # Multi-Head Attention
    ADDITIVE = 'additive'         # Additive Attention
    DOT_PRODUCT = 'dot_product'   # Dot-Product Attention


# ==================== æ¨¡å—é…ç½®åŸºç±» ====================

@dataclass
class ModuleConfig(BaseConfig):
    """
    æ¨¡å—é…ç½®åŸºç±»
    
    æ‰€æœ‰åŠŸèƒ½æ¨¡å—é…ç½®çš„åŸºç±»ã€‚
    
    Args:
        enabled (bool): æ˜¯å¦å¯ç”¨è¯¥æ¨¡å—ï¼Œé»˜è®¤ Trueã€‚
        name (Optional[str]): æ¨¡å—åç§°ï¼Œç”¨äºæ ‡è¯†å’Œæ—¥å¿—ã€‚
    """
    enabled: bool = True              # æ˜¯å¦å¯ç”¨è¯¥æ¨¡å—
    name: Optional[str] = None        # æ¨¡å—åç§°
    
    def validate(self) -> bool:
        """éªŒè¯é…ç½®"""
        return True


# ==================== æ—¶åºæ¨¡å—é…ç½® ====================

@dataclass
class TemporalModuleConfig(ModuleConfig):
    """
    æ—¶åºç‰¹å¾æå–æ¨¡å—é…ç½® (RNN + Attention)
    
    è´Ÿè´£ä»æ—¶é—´åºåˆ—æ•°æ®ä¸­æå–æ—¶åºç‰¹å¾ã€‚
    
    Args:
        rnn_type (str): RNNç±»å‹ï¼Œå¯é€‰ 'lstm', 'gru', 'rnn'ã€‚
            - 'lstm': é•¿çŸ­æœŸè®°å¿†ç½‘ç»œï¼Œè®°å¿†èƒ½åŠ›å¼ºï¼Œé€‚åˆå¤æ‚æ¨¡å¼
            - 'gru': é—¨æ§å¾ªç¯å•å…ƒï¼Œå‚æ•°å°‘ï¼Œè®­ç»ƒå¿«
            - 'rnn': æ ‡å‡†RNNï¼Œæœ€ç®€å•
            
        hidden_size (int): RNNéšè—å±‚å¤§å°ï¼Œé»˜è®¤ 64ã€‚
            æ§åˆ¶æ—¶åºç‰¹å¾æå–èƒ½åŠ›ï¼ˆ64-256 å¸¸ç”¨ï¼‰ã€‚
            
        num_layers (int): RNNå±‚æ•°ï¼Œé»˜è®¤ 2ã€‚
            å †å RNNå±‚æ•°ï¼ˆ1-3å±‚ï¼Œè¿‡æ·±æ˜“è¿‡æ‹Ÿåˆï¼‰ã€‚
            
        bidirectional (bool): æ˜¯å¦ä½¿ç”¨åŒå‘RNNï¼Œé»˜è®¤ Falseã€‚
            åŒå‘RNNå¯æ•è·æœªæ¥ä¿¡æ¯ï¼Œä½†å‚æ•°ç¿»å€ã€‚
            
        use_attention (bool): æ˜¯å¦ä½¿ç”¨æ³¨æ„åŠ›æœºåˆ¶ï¼Œé»˜è®¤ Trueã€‚
            å¼ºåŒ–å…³é”®æ—¶é—´ç‚¹çš„æƒé‡ã€‚
            
        attention_type (str): æ³¨æ„åŠ›ç±»å‹ï¼Œé»˜è®¤ 'self'ã€‚
            å¯é€‰: 'self', 'multi_head', 'additive', 'dot_product'
            
        attention_heads (int): å¤šå¤´æ³¨æ„åŠ›çš„å¤´æ•°ï¼Œé»˜è®¤ 4ã€‚
            ä»…åœ¨ attention_type='multi_head' æ—¶æœ‰æ•ˆã€‚
            
        dropout (float): Dropoutæ¦‚ç‡ï¼Œé»˜è®¤ 0.3ã€‚
    """
    # ==================== RNNé…ç½® ====================
    rnn_type: str = 'lstm'                      # RNNç±»å‹: 'lstm', 'gru', 'rnn'
    hidden_size: int = 64                       # éšè—å±‚å¤§å°
    num_layers: int = 2                         # RNNå±‚æ•°
    bidirectional: bool = False                 # æ˜¯å¦åŒå‘
    
    # ==================== æ³¨æ„åŠ›é…ç½® ====================
    use_attention: bool = True                  # æ˜¯å¦ä½¿ç”¨æ³¨æ„åŠ›
    attention_type: str = 'self'                # æ³¨æ„åŠ›ç±»å‹
    attention_heads: int = 4                    # å¤šå¤´æ³¨æ„åŠ›å¤´æ•°
    
    # ==================== æ­£åˆ™åŒ– ====================
    dropout: float = 0.3                        # Dropoutæ¦‚ç‡
    
    # ==================== ğŸ†• æ®‹å·®è¿æ¥ ====================
    use_residual: bool = True                   # æ˜¯å¦ä½¿ç”¨æ®‹å·®è¿æ¥ (LayerNorm + Skip Connection)
    
    def validate(self) -> bool:
        """éªŒè¯é…ç½®"""
        super().validate()
        
        if self.rnn_type not in ['lstm', 'gru', 'rnn']:
            raise ValueError(f"ä¸æ”¯æŒçš„RNNç±»å‹: {self.rnn_type}")
        
        if self.hidden_size <= 0:
            raise ValueError("hidden_size å¿…é¡»å¤§äº 0")
        
        if self.num_layers <= 0:
            raise ValueError("num_layers å¿…é¡»å¤§äº 0")
        
        if self.use_attention:
            if self.attention_type not in ['self', 'multi_head', 'additive', 'dot_product']:
                raise ValueError(f"ä¸æ”¯æŒçš„æ³¨æ„åŠ›ç±»å‹: {self.attention_type}")
            
            if self.attention_type == 'multi_head' and self.attention_heads <= 0:
                raise ValueError("attention_heads å¿…é¡»å¤§äº 0")
        
        if not 0 <= self.dropout <= 1:
            raise ValueError("dropout å¿…é¡»åœ¨ [0, 1] èŒƒå›´å†…")
        
        return True
    
    @property
    def output_dim(self) -> int:
        """è®¡ç®—è¾“å‡ºç»´åº¦"""
        multiplier = 2 if self.bidirectional else 1
        return self.hidden_size * multiplier


# ==================== å›¾æ¨¡å—é…ç½® ====================

@dataclass
class GraphModuleConfig(ModuleConfig):
    """
    å›¾ç¥ç»ç½‘ç»œæ¨¡å—é…ç½® (GAT)
    
    é€šè¿‡å›¾ç»“æ„æ•æ‰æˆªé¢ä¿¡æ¯ï¼ˆè‚¡ç¥¨é—´å…³è”ï¼‰ã€‚
    
    Args:
        gat_type (str): GATç±»å‹ï¼Œé»˜è®¤ 'standard'ã€‚
            - 'standard': åŸºäºè¡Œä¸šåˆ†ç±»çš„é™æ€å›¾
            - 'correlation': åŸºäºæ”¶ç›Šç‡ç›¸å…³æ€§çš„åŠ¨æ€å›¾
            - 'dynamic': å®Œå…¨åŠ¨æ€å­¦ä¹ å›¾ç»“æ„
            
        hidden_dim (int): GATéšè—å±‚ç»´åº¦ï¼Œé»˜è®¤ 32ã€‚
            å¿…é¡»èƒ½è¢« heads æ•´é™¤ã€‚
            
        heads (int): å¤šå¤´æ³¨æ„åŠ›å¤´æ•°ï¼Œé»˜è®¤ 4ã€‚
            
        concat (bool): æ˜¯å¦æ‹¼æ¥å¤šå¤´è¾“å‡ºï¼Œé»˜è®¤ Trueã€‚
            False æ—¶å–å¹³å‡ã€‚
            
        top_k_neighbors (int): ç›¸å…³æ€§GATçš„é‚»å±…æ•°ï¼Œé»˜è®¤ 10ã€‚
            ä»…åœ¨ gat_type='correlation' æ—¶æœ‰æ•ˆã€‚
            
        edge_threshold (float): è¾¹æƒé‡é˜ˆå€¼ï¼Œé»˜è®¤ 0.0ã€‚
            ä½äºæ­¤å€¼çš„è¾¹ä¼šè¢«å‰ªæï¼ˆç”¨äºç¨€ç–åŒ–å›¾ï¼‰ã€‚
            
        use_edge_features (bool): æ˜¯å¦ä½¿ç”¨è¾¹ç‰¹å¾ï¼Œé»˜è®¤ Falseã€‚
            å¦‚æœTrueï¼Œå¯ä»¥åœ¨è¾¹ä¸Šé™„åŠ é¢å¤–ä¿¡æ¯ï¼ˆå¦‚ç›¸å…³ç³»æ•°ï¼‰ã€‚
            
        dropout (float): Dropoutæ¦‚ç‡ï¼Œé»˜è®¤ 0.3ã€‚
    """
    # ==================== GATé…ç½® ====================
    gat_type: str = 'standard'                  # GATç±»å‹
    hidden_dim: int = 32                        # éšè—å±‚ç»´åº¦
    heads: int = 4                              # æ³¨æ„åŠ›å¤´æ•°
    concat: bool = True                         # æ˜¯å¦æ‹¼æ¥å¤šå¤´è¾“å‡º
    
    # ==================== å›¾ç»“æ„é…ç½® ====================
    top_k_neighbors: int = 10                   # Kè¿‘é‚»æ•°é‡
    edge_threshold: float = 0.0                 # è¾¹æƒé‡é˜ˆå€¼
    use_edge_features: bool = False             # æ˜¯å¦ä½¿ç”¨è¾¹ç‰¹å¾
    
    # ==================== æ­£åˆ™åŒ– ====================
    dropout: float = 0.3                        # Dropoutæ¦‚ç‡
    
    # ==================== ğŸ†• æ®‹å·®è¿æ¥ ====================
    use_residual: bool = True                   # æ˜¯å¦ä½¿ç”¨æ®‹å·®è¿æ¥ (LayerNorm + Skip Connection)
    
    def validate(self) -> bool:
        """éªŒè¯é…ç½®"""
        super().validate()
        
        if self.gat_type not in ['standard', 'correlation', 'dynamic']:
            raise ValueError(f"ä¸æ”¯æŒçš„GATç±»å‹: {self.gat_type}")
        
        if self.hidden_dim <= 0:
            raise ValueError("hidden_dim å¿…é¡»å¤§äº 0")
        
        if self.heads <= 0:
            raise ValueError("heads å¿…é¡»å¤§äº 0")
        
        if self.concat and self.hidden_dim % self.heads != 0:
            raise ValueError("concat=True æ—¶ï¼Œhidden_dim å¿…é¡»èƒ½è¢« heads æ•´é™¤")
        
        if self.gat_type == 'correlation' and self.top_k_neighbors <= 0:
            raise ValueError("top_k_neighbors å¿…é¡»å¤§äº 0")
        
        if not 0 <= self.dropout <= 1:
            raise ValueError("dropout å¿…é¡»åœ¨ [0, 1] èŒƒå›´å†…")
        
        return True
    
    @property
    def output_dim(self) -> int:
        """è®¡ç®—è¾“å‡ºç»´åº¦"""
        if self.concat:
            return self.hidden_dim
        else:
            return self.hidden_dim


# ==================== èåˆæ¨¡å—é…ç½® ====================

@dataclass
class FusionModuleConfig(ModuleConfig):
    """
    ç‰¹å¾èåˆæ¨¡å—é…ç½® (MLP)
    
    å°†å¤šä¸ªæ¨¡å—çš„è¾“å‡ºç‰¹å¾èåˆåè¿›è¡Œé¢„æµ‹ã€‚
    
    Args:
        hidden_sizes (List[int]): MLPéšè—å±‚å°ºå¯¸åˆ—è¡¨ï¼Œé»˜è®¤ [64]ã€‚
            ä¾‹å¦‚ [128, 64] è¡¨ç¤ºä¸¤å±‚MLPã€‚
            
        activation (str): æ¿€æ´»å‡½æ•°ï¼Œé»˜è®¤ 'relu'ã€‚
            å¯é€‰: 'relu', 'gelu', 'tanh', 'leaky_relu'
            
        use_batch_norm (bool): æ˜¯å¦ä½¿ç”¨BatchNormï¼Œé»˜è®¤ Falseã€‚
            
        use_residual (bool): æ˜¯å¦ä½¿ç”¨æ®‹å·®è¿æ¥ï¼Œé»˜è®¤ Falseã€‚
            é€‚åˆæ·±å±‚MLPï¼Œé˜²æ­¢æ¢¯åº¦æ¶ˆå¤±ã€‚
            
        dropout (float): Dropoutæ¦‚ç‡ï¼Œé»˜è®¤ 0.3ã€‚
            
        output_dim (int): è¾“å‡ºç»´åº¦ï¼Œé»˜è®¤ 1ã€‚
            é¢„æµ‹ç›®æ ‡æ•°é‡ï¼ˆå•ç›®æ ‡å›å½’ä¸º1ï¼‰ã€‚
    """
    # ==================== MLPé…ç½® ====================
    hidden_sizes: List[int] = field(default_factory=lambda: [64])  # éšè—å±‚å°ºå¯¸
    activation: str = 'relu'                    # æ¿€æ´»å‡½æ•°
    use_batch_norm: bool = False                # æ˜¯å¦ä½¿ç”¨BatchNorm
    use_residual: bool = False                  # æ˜¯å¦ä½¿ç”¨æ®‹å·®è¿æ¥
    
    # ==================== æ­£åˆ™åŒ– ====================
    dropout: float = 0.3                        # Dropoutæ¦‚ç‡
    
    # ==================== è¾“å‡º ====================
    output_dim: int = 1                         # è¾“å‡ºç»´åº¦
    
    def validate(self) -> bool:
        """éªŒè¯é…ç½®"""
        super().validate()
        
        if not self.hidden_sizes:
            raise ValueError("hidden_sizes ä¸èƒ½ä¸ºç©º")
        
        if any(size <= 0 for size in self.hidden_sizes):
            raise ValueError("hidden_sizes ä¸­çš„æ‰€æœ‰å€¼å¿…é¡»å¤§äº 0")
        
        if self.activation not in ['relu', 'gelu', 'tanh', 'leaky_relu']:
            raise ValueError(f"ä¸æ”¯æŒçš„æ¿€æ´»å‡½æ•°: {self.activation}")
        
        if not 0 <= self.dropout <= 1:
            raise ValueError("dropout å¿…é¡»åœ¨ [0, 1] èŒƒå›´å†…")
        
        if self.output_dim <= 0:
            raise ValueError("output_dim å¿…é¡»å¤§äº 0")
        
        return True


# ==================== ç»„åˆæ¨¡å‹é…ç½® ====================

@dataclass
class CompositeModelConfig(BaseModelConfig):
    """
    ç»„åˆæ¨¡å‹é…ç½® (æ¨¡å—åŒ–æ··åˆæ¨¡å‹)
    
    é€šè¿‡ç»„åˆå¤šä¸ªåŠŸèƒ½æ¨¡å—æ„å»ºå®Œæ•´çš„æ··åˆæ¨¡å‹ã€‚
    
    Args:
        temporal (Optional[TemporalModuleConfig]): æ—¶åºæ¨¡å—é…ç½®ã€‚
            å¦‚æœä¸º Noneï¼Œåˆ™ä¸ä½¿ç”¨æ—¶åºæ¨¡å—ã€‚
            
        graph (Optional[GraphModuleConfig]): å›¾æ¨¡å—é…ç½®ã€‚
            å¦‚æœä¸º Noneï¼Œåˆ™ä¸ä½¿ç”¨å›¾æ¨¡å—ã€‚
            
        fusion (FusionModuleConfig): èåˆæ¨¡å—é…ç½®ï¼ˆå¿…éœ€ï¼‰ã€‚
            
        d_feat (int): è¾“å…¥ç‰¹å¾ç»´åº¦ï¼ˆé‡ä»·æ•°æ®ï¼‰ï¼Œé»˜è®¤ 20ã€‚
            
        funda_dim (Optional[int]): åŸºæœ¬é¢æ•°æ®ç»´åº¦ï¼Œé»˜è®¤ Noneã€‚
            å¦‚æœæä¾›åŸºæœ¬é¢æ•°æ®ï¼Œä¼šåœ¨é€‚å½“ä½ç½®æ‹¼æ¥ã€‚
            
        adj_matrix_path (Optional[str]): é‚»æ¥çŸ©é˜µè·¯å¾„ï¼Œé»˜è®¤ Noneã€‚
            å›¾æ¨¡å—ä½¿ç”¨çš„é‚»æ¥çŸ©é˜µæ–‡ä»¶ï¼ˆ.ptæˆ–.npyï¼‰ã€‚
            
        feature_fusion_strategy (str): ç‰¹å¾èåˆç­–ç•¥ï¼Œé»˜è®¤ 'concat'ã€‚
            - 'concat': ç›´æ¥æ‹¼æ¥å„æ¨¡å—è¾“å‡º
            - 'add': ç›¸åŠ ï¼ˆè¦æ±‚ç»´åº¦ç›¸åŒï¼‰
            - 'weighted': åŠ æƒæ±‚å’Œï¼ˆå¯å­¦ä¹ æƒé‡ï¼‰
    
    Example:
        # åˆ›å»ºæ—¶åº+å›¾+èåˆçš„å®Œæ•´æ¨¡å‹
        config = CompositeModelConfig(
            temporal=TemporalModuleConfig(rnn_type='lstm', hidden_size=64),
            graph=GraphModuleConfig(gat_type='correlation', hidden_dim=32),
            fusion=FusionModuleConfig(hidden_sizes=[64]),
            d_feat=20
        )
        
        # åˆ›å»ºçº¯æ—¶åºæ¨¡å‹ï¼ˆä¸ä½¿ç”¨å›¾ï¼‰
        config = CompositeModelConfig(
            temporal=TemporalModuleConfig(rnn_type='gru', hidden_size=128),
            graph=None,
            fusion=FusionModuleConfig(hidden_sizes=[64]),
            d_feat=20
        )
    """
    # ==================== æ¨¡å—é…ç½® ====================
    temporal: Optional[TemporalModuleConfig] = field(default_factory=TemporalModuleConfig)
    graph: Optional[GraphModuleConfig] = None
    fusion: FusionModuleConfig = field(default_factory=FusionModuleConfig)
    
    # ==================== è¾“å…¥ç‰¹å¾ ====================
    d_feat: int = 20                            # è¾“å…¥ç‰¹å¾ç»´åº¦
    funda_dim: Optional[int] = None             # åŸºæœ¬é¢æ•°æ®ç»´åº¦
    
    # ==================== å›¾é…ç½® ====================
    adj_matrix_path: Optional[str] = None       # é‚»æ¥çŸ©é˜µè·¯å¾„
    
    # ==================== èåˆç­–ç•¥ ====================
    feature_fusion_strategy: str = 'concat'     # ç‰¹å¾èåˆç­–ç•¥
    
    # ==================== ğŸ†• Graph-Aware Inference é…ç½® ====================
    graph_inference_mode: str = 'batch'         # å›¾æ¨ç†æ¨¡å¼: 'batch', 'neighbor_sampling', 'cross_sectional'
    max_neighbors: int = 10                     # é‚»å±…é‡‡æ ·æ—¶çš„æœ€å¤§é‚»å±…æ•°
    cache_size: int = 5000                      # èŠ‚ç‚¹çŠ¶æ€ç¼“å­˜å¤§å°
    
    # ==================== ğŸ†• æ•°æ®æ ¼å¼é…ç½® ====================
    stock_idx_position: Optional[int] = None    # è‚¡ç¥¨ç´¢å¼•åœ¨ batch ä¸­çš„ä½ç½®
    funda_position: Optional[int] = None        # åŸºæœ¬é¢æ•°æ®åœ¨ batch ä¸­çš„ä½ç½®
    
    # ==================== ğŸ†• å­¦ä¹ ç‡è°ƒåº¦å™¨é…ç½® ====================
    use_scheduler: bool = True                  # æ˜¯å¦ä½¿ç”¨å­¦ä¹ ç‡è°ƒåº¦å™¨
    scheduler_type: str = 'plateau'             # è°ƒåº¦å™¨ç±»å‹: 'plateau', 'cosine', 'step'
    scheduler_patience: int = 5                 # ReduceLROnPlateau çš„è€å¿ƒå€¼
    scheduler_factor: float = 0.5               # å­¦ä¹ ç‡è¡°å‡å› å­
    scheduler_min_lr: float = 1e-6              # æœ€å°å­¦ä¹ ç‡
    
    # ==================== ğŸ†• æ®‹å·®è¿æ¥é…ç½® ====================
    use_residual: bool = True                   # æ˜¯å¦ä½¿ç”¨æ®‹å·®è¿æ¥ (LayerNorm + Skip Connection)
    
    # ==================== ğŸ†• ç›¸å…³æ€§æ­£åˆ™åŒ–é…ç½® ====================
    lambda_corr: float = 0.0                    # ç›¸å…³æ€§æ­£åˆ™åŒ–æƒé‡ (0=ç¦ç”¨, æ¨è0.001~0.1)
                                                # ä½œç”¨: æŠ‘åˆ¶éšè—å±‚ç‰¹å¾å†—ä½™ï¼Œé¼“åŠ±å­¦ä¹ æ›´ç‹¬ç«‹çš„ç‰¹å¾è¡¨è¾¾
                                                # å…¬å¼: Loss = BaseLoss + lambda_corr * ||Corr(H) - I||_F
    
    def __post_init__(self):
        """åˆå§‹åŒ–åå¤„ç† - è½¬æ¢å­—å…¸ä¸ºæ¨¡å—é…ç½®å¯¹è±¡"""
        # å¦‚æœ temporal æ˜¯å­—å…¸ï¼Œè½¬æ¢ä¸º TemporalModuleConfig
        if isinstance(self.temporal, dict):
            self.temporal = TemporalModuleConfig(**self.temporal)
        
        # å¦‚æœ graph æ˜¯å­—å…¸ï¼Œè½¬æ¢ä¸º GraphModuleConfig
        if isinstance(self.graph, dict):
            self.graph = GraphModuleConfig(**self.graph)
        
        # å¦‚æœ fusion æ˜¯å­—å…¸ï¼Œè½¬æ¢ä¸º FusionModuleConfig
        if isinstance(self.fusion, dict):
            self.fusion = FusionModuleConfig(**self.fusion)
    
    def validate(self) -> bool:
        """éªŒè¯é…ç½®"""
        super().validate()
        
        # è‡³å°‘è¦æœ‰ä¸€ä¸ªç‰¹å¾æå–æ¨¡å—
        if self.temporal is None and self.graph is None:
            raise ValueError("è‡³å°‘éœ€è¦å¯ç”¨ temporal æˆ– graph æ¨¡å—ä¹‹ä¸€")
        
        # éªŒè¯å„æ¨¡å—é…ç½®
        if self.temporal is not None and self.temporal.enabled:
            self.temporal.validate()
        
        if self.graph is not None and self.graph.enabled:
            self.graph.validate()
            
            # é™æ€å›¾éœ€è¦é‚»æ¥çŸ©é˜µï¼ŒåŠ¨æ€å›¾ï¼ˆcorrelation/dynamicï¼‰ä¾èµ–åœ¨çº¿æ„å»º
            if self.adj_matrix_path is None and self.graph.gat_type == 'standard':
                import warnings
                warnings.warn("graph æ¨¡å—å·²å¯ç”¨ä½†æœªæä¾› adj_matrix_path")
        
        self.fusion.validate()
        
        # éªŒè¯ç‰¹å¾ç»´åº¦
        if self.d_feat <= 0:
            raise ValueError("d_feat å¿…é¡»å¤§äº 0")
        
        if self.funda_dim is not None and self.funda_dim <= 0:
            raise ValueError("funda_dim å¿…é¡»å¤§äº 0")
        
        # éªŒè¯èåˆç­–ç•¥
        if self.feature_fusion_strategy not in ['concat', 'add', 'weighted']:
            raise ValueError(f"ä¸æ”¯æŒçš„èåˆç­–ç•¥: {self.feature_fusion_strategy}")
        
        return True
    
    def get_fusion_input_dim(self) -> int:
        """
        è®¡ç®—èåˆæ¨¡å—çš„è¾“å…¥ç»´åº¦
        
        Returns:
            èåˆæ¨¡å—è¾“å…¥ç»´åº¦
        """
        total_dim = 0
        
        # æ—¶åºæ¨¡å—è¾“å‡º
        if self.temporal is not None and self.temporal.enabled:
            total_dim += self.temporal.output_dim
        
        # å›¾æ¨¡å—è¾“å‡º
        if self.graph is not None and self.graph.enabled:
            total_dim += self.graph.output_dim
        
        # åŸºæœ¬é¢æ•°æ®
        if self.funda_dim is not None:
            total_dim += self.funda_dim
        
        # å¦‚æœæ˜¯åŠ æ³•æˆ–åŠ æƒèåˆï¼Œéœ€è¦æ‰€æœ‰ç»´åº¦ç›¸åŒ
        if self.feature_fusion_strategy in ['add', 'weighted']:
            temporal_dim = self.temporal.output_dim if self.temporal else 0
            graph_dim = self.graph.output_dim if self.graph else 0
            
            if temporal_dim > 0 and graph_dim > 0 and temporal_dim != graph_dim:
                raise ValueError(
                    f"ä½¿ç”¨ {self.feature_fusion_strategy} èåˆæ—¶ï¼Œ"
                    f"temporal å’Œ graph çš„è¾“å‡ºç»´åº¦å¿…é¡»ç›¸åŒ "
                    f"(å½“å‰: {temporal_dim} vs {graph_dim})"
                )
            
            # åŠ æ³•/åŠ æƒèåˆåç»´åº¦ä¸å˜
            total_dim = max(temporal_dim, graph_dim)
            if self.funda_dim is not None:
                total_dim += self.funda_dim
        
        return total_dim
    
    def summary(self) -> str:
        """ç”Ÿæˆé…ç½®æ‘˜è¦"""
        lines = ["=" * 60, "ç»„åˆæ¨¡å‹é…ç½®æ‘˜è¦", "=" * 60, ""]
        
        lines.append(f"è¾“å…¥ç‰¹å¾ç»´åº¦: {self.d_feat}")
        if self.funda_dim:
            lines.append(f"åŸºæœ¬é¢ç»´åº¦: {self.funda_dim}")
        lines.append("")
        
        # æ—¶åºæ¨¡å—
        if self.temporal and self.temporal.enabled:
            lines.append("ã€æ—¶åºæ¨¡å—ã€‘")
            lines.append(f"  - RNNç±»å‹: {self.temporal.rnn_type}")
            lines.append(f"  - éšè—å±‚: {self.temporal.hidden_size}")
            lines.append(f"  - å±‚æ•°: {self.temporal.num_layers}")
            lines.append(f"  - åŒå‘: {self.temporal.bidirectional}")
            lines.append(f"  - æ³¨æ„åŠ›: {self.temporal.use_attention}")
            if self.temporal.use_attention:
                lines.append(f"    ç±»å‹: {self.temporal.attention_type}")
            lines.append(f"  - è¾“å‡ºç»´åº¦: {self.temporal.output_dim}")
            lines.append("")
        
        # å›¾æ¨¡å—
        if self.graph and self.graph.enabled:
            lines.append("ã€å›¾æ¨¡å—ã€‘")
            lines.append(f"  - GATç±»å‹: {self.graph.gat_type}")
            lines.append(f"  - éšè—ç»´åº¦: {self.graph.hidden_dim}")
            lines.append(f"  - æ³¨æ„åŠ›å¤´æ•°: {self.graph.heads}")
            lines.append(f"  - æ‹¼æ¥å¤šå¤´: {self.graph.concat}")
            if self.graph.gat_type == 'correlation':
                lines.append(f"  - Kè¿‘é‚»: {self.graph.top_k_neighbors}")
            lines.append(f"  - è¾“å‡ºç»´åº¦: {self.graph.output_dim}")
            if self.adj_matrix_path:
                lines.append(f"  - é‚»æ¥çŸ©é˜µ: {self.adj_matrix_path}")
            lines.append("")
        
        # èåˆæ¨¡å—
        lines.append("ã€èåˆæ¨¡å—ã€‘")
        lines.append(f"  - èåˆç­–ç•¥: {self.feature_fusion_strategy}")
        lines.append(f"  - è¾“å…¥ç»´åº¦: {self.get_fusion_input_dim()}")
        lines.append(f"  - éšè—å±‚: {self.fusion.hidden_sizes}")
        lines.append(f"  - æ¿€æ´»å‡½æ•°: {self.fusion.activation}")
        lines.append(f"  - BatchNorm: {self.fusion.use_batch_norm}")
        lines.append(f"  - æ®‹å·®è¿æ¥: {self.fusion.use_residual}")
        lines.append(f"  - è¾“å‡ºç»´åº¦: {self.fusion.output_dim}")
        lines.append("")
        
        # è®­ç»ƒé…ç½®
        lines.append("ã€è®­ç»ƒé…ç½®ã€‘")
        lines.append(f"  - è®¾å¤‡: {self.device}")
        lines.append(f"  - Epochs: {self.n_epochs}")
        lines.append(f"  - Batch Size: {self.batch_size}")
        lines.append(f"  - å­¦ä¹ ç‡: {self.learning_rate}")
        lines.append(f"  - ä¼˜åŒ–å™¨: {self.optimizer}")
        lines.append(f"  - æŸå¤±å‡½æ•°: {self.loss_fn}")
        lines.append(f"  - æ—©åœ: {self.early_stop}")
        lines.append("")
        
        lines.append("=" * 60)
        
        return "\n".join(lines)


# ==================== é…ç½®æ„å»ºå™¨ ====================

class ModelConfigBuilder:
    """
    æ¨¡å‹é…ç½®æ„å»ºå™¨ (Builder Pattern)
    
    æä¾›æµå¼APIå¿«é€Ÿæ„å»ºæ¨¡å‹é…ç½®ã€‚
    
    Example:
        config = ModelConfigBuilder() \\
            .set_input(d_feat=20, funda_dim=10) \\
            .add_temporal(rnn_type='lstm', hidden_size=64, use_attention=True) \\
            .add_graph(gat_type='correlation', hidden_dim=32, heads=4) \\
            .add_fusion(hidden_sizes=[64, 32]) \\
            .set_training(n_epochs=100, batch_size=256) \\
            .build()
    """
    
    def __init__(self):
        self._d_feat = 20
        self._funda_dim = None
        self._temporal_config = None
        self._graph_config = None
        self._fusion_config = FusionModuleConfig()
        self._adj_matrix_path = None
        self._training_kwargs = {}
    
    def set_input(self, d_feat: int, funda_dim: Optional[int] = None) -> 'ModelConfigBuilder':
        """è®¾ç½®è¾“å…¥ç‰¹å¾ç»´åº¦"""
        self._d_feat = d_feat
        self._funda_dim = funda_dim
        return self
    
    def add_temporal(
        self,
        rnn_type: str = 'lstm',
        hidden_size: int = 64,
        num_layers: int = 2,
        bidirectional: bool = False,
        use_attention: bool = True,
        attention_type: str = 'self',
        dropout: float = 0.3,
        **kwargs
    ) -> 'ModelConfigBuilder':
        """æ·»åŠ æ—¶åºæ¨¡å—"""
        self._temporal_config = TemporalModuleConfig(
            rnn_type=rnn_type,
            hidden_size=hidden_size,
            num_layers=num_layers,
            bidirectional=bidirectional,
            use_attention=use_attention,
            attention_type=attention_type,
            dropout=dropout,
            **kwargs
        )
        return self
    
    def add_graph(
        self,
        gat_type: str = 'standard',
        hidden_dim: int = 32,
        heads: int = 4,
        concat: bool = True,
        top_k_neighbors: int = 10,
        adj_matrix_path: Optional[str] = None,
        dropout: float = 0.3,
        **kwargs
    ) -> 'ModelConfigBuilder':
        """æ·»åŠ å›¾æ¨¡å—"""
        self._graph_config = GraphModuleConfig(
            gat_type=gat_type,
            hidden_dim=hidden_dim,
            heads=heads,
            concat=concat,
            top_k_neighbors=top_k_neighbors,
            dropout=dropout,
            **kwargs
        )
        if adj_matrix_path:
            self._adj_matrix_path = adj_matrix_path
        return self
    
    def add_fusion(
        self,
        hidden_sizes: List[int] = None,
        activation: str = 'relu',
        use_batch_norm: bool = False,
        use_residual: bool = False,
        dropout: float = 0.3,
        output_dim: int = 1,
        **kwargs
    ) -> 'ModelConfigBuilder':
        """æ·»åŠ èåˆæ¨¡å—"""
        self._fusion_config = FusionModuleConfig(
            hidden_sizes=hidden_sizes or [64],
            activation=activation,
            use_batch_norm=use_batch_norm,
            use_residual=use_residual,
            dropout=dropout,
            output_dim=output_dim,
            **kwargs
        )
        return self
    
    def set_training(
        self,
        device: str = 'cuda',
        n_epochs: int = 100,
        batch_size: int = 256,
        learning_rate: float = 0.001,
        optimizer: str = 'adam',
        loss_fn: str = 'mse',
        early_stop: int = 20,
        **kwargs
    ) -> 'ModelConfigBuilder':
        """è®¾ç½®è®­ç»ƒå‚æ•°"""
        self._training_kwargs.update({
            'device': device,
            'n_epochs': n_epochs,
            'batch_size': batch_size,
            'learning_rate': learning_rate,
            'optimizer': optimizer,
            'loss_fn': loss_fn,
            'early_stop': early_stop,
            **kwargs
        })
        return self
    
    def build(self, **override_kwargs) -> CompositeModelConfig:
        """
        æ„å»ºé…ç½®å¯¹è±¡
        
        Args:
            **override_kwargs: è¦†ç›–å‚æ•°
            
        Returns:
            CompositeModelConfig å¯¹è±¡
        """
        config = CompositeModelConfig(
            temporal=self._temporal_config,
            graph=self._graph_config,
            fusion=self._fusion_config,
            d_feat=self._d_feat,
            funda_dim=self._funda_dim,
            adj_matrix_path=self._adj_matrix_path,
            **self._training_kwargs,
            **override_kwargs
        )
        
        config.validate()
        return config


# ==================== é¢„å®šä¹‰æ¨¡æ¿ ====================

class ConfigTemplates:
    """é¢„å®šä¹‰é…ç½®æ¨¡æ¿"""
    
    @staticmethod
    def pure_temporal(d_feat: int = 20, model_size: str = 'default') -> CompositeModelConfig:
        """
        çº¯æ—¶åºæ¨¡å‹ï¼ˆä¸ä½¿ç”¨å›¾ï¼‰
        
        Args:
            d_feat: è¾“å…¥ç‰¹å¾ç»´åº¦
            model_size: æ¨¡å‹å¤§å° ('small', 'default', 'large')
        """
        size_map = {
            'small': {'rnn_hidden': 32, 'rnn_layers': 1, 'mlp_hidden': [32]},
            'default': {'rnn_hidden': 64, 'rnn_layers': 2, 'mlp_hidden': [64]},
            'large': {'rnn_hidden': 128, 'rnn_layers': 3, 'mlp_hidden': [128, 64]},
        }
        
        params = size_map.get(model_size, size_map['default'])
        
        return ModelConfigBuilder() \
            .set_input(d_feat=d_feat) \
            .add_temporal(
                rnn_type='lstm',
                hidden_size=params['rnn_hidden'],
                num_layers=params['rnn_layers'],
                use_attention=True
            ) \
            .add_fusion(hidden_sizes=params['mlp_hidden']) \
            .build()
    
    @staticmethod
    def temporal_with_graph(
        d_feat: int = 20,
        gat_type: str = 'standard',
        adj_matrix_path: Optional[str] = None,
        model_size: str = 'default'
    ) -> CompositeModelConfig:
        """
        æ—¶åº+å›¾æ··åˆæ¨¡å‹
        
        Args:
            d_feat: è¾“å…¥ç‰¹å¾ç»´åº¦
            gat_type: GATç±»å‹ ('standard' æˆ– 'correlation')
            adj_matrix_path: é‚»æ¥çŸ©é˜µè·¯å¾„
            model_size: æ¨¡å‹å¤§å° ('small', 'default', 'large')
        """
        size_map = {
            'small': {
                'rnn_hidden': 32, 'rnn_layers': 1,
                'gat_hidden': 16, 'gat_heads': 2,
                'mlp_hidden': [32]
            },
            'default': {
                'rnn_hidden': 64, 'rnn_layers': 2,
                'gat_hidden': 32, 'gat_heads': 4,
                'mlp_hidden': [64]
            },
            'large': {
                'rnn_hidden': 128, 'rnn_layers': 3,
                'gat_hidden': 64, 'gat_heads': 8,
                'mlp_hidden': [128, 64]
            },
        }
        
        params = size_map.get(model_size, size_map['default'])
        
        return ModelConfigBuilder() \
            .set_input(d_feat=d_feat) \
            .add_temporal(
                rnn_type='lstm',
                hidden_size=params['rnn_hidden'],
                num_layers=params['rnn_layers'],
                use_attention=True
            ) \
            .add_graph(
                gat_type=gat_type,
                hidden_dim=params['gat_hidden'],
                heads=params['gat_heads'],
                adj_matrix_path=adj_matrix_path
            ) \
            .add_fusion(hidden_sizes=params['mlp_hidden']) \
            .build()
    
    @staticmethod
    def attention_graph_fusion(
        d_feat: int = 20,
        attention_type: str = 'multi_head',
        gat_type: str = 'correlation',
        adj_matrix_path: Optional[str] = None
    ) -> CompositeModelConfig:
        """
        å¤šå¤´æ³¨æ„åŠ› + ç›¸å…³æ€§å›¾ + æ·±å±‚èåˆ
        
        Args:
            d_feat: è¾“å…¥ç‰¹å¾ç»´åº¦
            attention_type: æ³¨æ„åŠ›ç±»å‹
            gat_type: GATç±»å‹
            adj_matrix_path: é‚»æ¥çŸ©é˜µè·¯å¾„
        """
        return ModelConfigBuilder() \
            .set_input(d_feat=d_feat) \
            .add_temporal(
                rnn_type='gru',
                hidden_size=64,
                num_layers=2,
                use_attention=True,
                attention_type=attention_type,
                attention_heads=8
            ) \
            .add_graph(
                gat_type=gat_type,
                hidden_dim=64,
                heads=8,
                top_k_neighbors=15,
                adj_matrix_path=adj_matrix_path
            ) \
            .add_fusion(
                hidden_sizes=[128, 64, 32],
                use_batch_norm=True,
                use_residual=True
            ) \
            .build()


if __name__ == '__main__':
    print("=" * 80)
    print("ModularConfig æµ‹è¯•")
    print("=" * 80)
    
    # æµ‹è¯• 1: åˆ›å»ºç‹¬ç«‹æ¨¡å—é…ç½®
    print("\n1. åˆ›å»ºç‹¬ç«‹æ¨¡å—é…ç½®:")
    temporal_cfg = TemporalModuleConfig(
        rnn_type='lstm',
        hidden_size=64,
        num_layers=2,
        use_attention=True
    )
    print(f"  æ—¶åºæ¨¡å—: {temporal_cfg.rnn_type}, è¾“å‡ºç»´åº¦={temporal_cfg.output_dim}")
    
    graph_cfg = GraphModuleConfig(
        gat_type='correlation',
        hidden_dim=32,
        heads=4
    )
    print(f"  å›¾æ¨¡å—: {graph_cfg.gat_type}, è¾“å‡ºç»´åº¦={graph_cfg.output_dim}")
    
    fusion_cfg = FusionModuleConfig(
        hidden_sizes=[64, 32],
        activation='relu'
    )
    print(f"  èåˆæ¨¡å—: {fusion_cfg.hidden_sizes}")
    
    # æµ‹è¯• 2: ç»„åˆæ¨¡å‹é…ç½®
    print("\n2. ç»„åˆæ¨¡å‹é…ç½®:")
    composite_cfg = CompositeModelConfig(
        temporal=temporal_cfg,
        graph=graph_cfg,
        fusion=fusion_cfg,
        d_feat=20
    )
    composite_cfg.validate()
    print(f"  èåˆè¾“å…¥ç»´åº¦: {composite_cfg.get_fusion_input_dim()}")
    
    # æµ‹è¯• 3: ä½¿ç”¨æ„å»ºå™¨
    print("\n3. ä½¿ç”¨æ„å»ºå™¨:")
    builder_cfg = ModelConfigBuilder() \
        .set_input(d_feat=20) \
        .add_temporal(rnn_type='gru', hidden_size=128) \
        .add_graph(gat_type='standard', hidden_dim=64) \
        .add_fusion(hidden_sizes=[128, 64]) \
        .set_training(n_epochs=100, batch_size=256) \
        .build()
    
    print(f"  âœ… Builder åˆ›å»ºæˆåŠŸ")
    
    # æµ‹è¯• 4: çº¯æ—¶åºæ¨¡å‹
    print("\n4. çº¯æ—¶åºæ¨¡å‹æ¨¡æ¿:")
    pure_temporal = ConfigTemplates.pure_temporal(d_feat=20, model_size='default')
    print(f"  æ—¶åºæ¨¡å—: {pure_temporal.temporal.rnn_type}")
    print(f"  å›¾æ¨¡å—: {pure_temporal.graph}")
    
    # æµ‹è¯• 5: æ—¶åº+å›¾æ··åˆæ¨¡å‹
    print("\n5. æ··åˆæ¨¡å‹æ¨¡æ¿:")
    hybrid = ConfigTemplates.temporal_with_graph(
        d_feat=20,
        gat_type='correlation',
        model_size='large'
    )
    print(f"  æ—¶åºéšè—å±‚: {hybrid.temporal.hidden_size}")
    print(f"  å›¾éšè—å±‚: {hybrid.graph.hidden_dim}")
    
    # æµ‹è¯• 6: é…ç½®æ‘˜è¦
    print("\n6. é…ç½®æ‘˜è¦:")
    print(hybrid.summary())
    
    # æµ‹è¯• 7: YAMLåºåˆ—åŒ–
    print("\n7. YAMLåºåˆ—åŒ–:")
    yaml_path = '/tmp/composite_config.yaml'
    hybrid.to_yaml(yaml_path)
    print(f"  å·²ä¿å­˜åˆ°: {yaml_path}")
    
    loaded_cfg = CompositeModelConfig.from_yaml(yaml_path)
    print(f"  å·²åŠ è½½: æ—¶åºhidden={loaded_cfg.temporal.hidden_size}")
    
    print("\n" + "=" * 80)
    print("âœ… ModularConfig æµ‹è¯•å®Œæˆ")
    print("=" * 80)
