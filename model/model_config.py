"""
ModelConfig - æ¨¡å‹é…ç½®ç±»

ä½¿ç”¨é¢å‘å¯¹è±¡çš„é…ç½®æ›¿ä»£å­—å…¸é…ç½®
æ”¯æŒæ‰€æœ‰ QuantClassic æ¨¡å‹çš„ç»Ÿä¸€é…ç½®æ¥å£
"""

from dataclasses import dataclass, field
from typing import Optional, List, Dict, Any
import sys
from pathlib import Path

# æ·»åŠ çˆ¶ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent))
from config.base_config import BaseConfig


@dataclass
class BaseModelConfig(BaseConfig):
    """
    åŸºç¡€æ¨¡å‹é…ç½®ç±»

    ç®¡ç†è®­ç»ƒ/ä¼˜åŒ–/ä¿å­˜ç­‰æ¨¡å‹é€šç”¨å‚æ•°ï¼Œç”¨äºæ‰€æœ‰æ¨¡å‹é…ç½®çš„åŸºç±»ã€‚

    Args:
        device (str): è®¡ç®—è®¾å¤‡ï¼Œä¾‹å¦‚ 'cuda' æˆ– 'cpu'ã€‚
            - 'cuda': GPU åŠ é€Ÿï¼ˆæ¨èç”¨äºå¤§è§„æ¨¡è®­ç»ƒï¼‰
            - 'cpu': CPU è®¡ç®—ï¼ˆæ”¯æŒæ‰€æœ‰è®¾å¤‡ï¼Œä½†é€Ÿåº¦è¾ƒæ…¢ï¼‰
            - 'cuda:0'/'cuda:1': æŒ‡å®šç‰¹å®š GPU è®¾å¤‡
            
        n_epochs (int): è®­ç»ƒè½®æ•°ï¼Œé»˜è®¤ 100ã€‚
            æ¨¡å‹åœ¨æ•´ä¸ªè®­ç»ƒé›†ä¸Šçš„å®Œæ•´è¿­ä»£æ¬¡æ•°ï¼ˆ50-200å¸¸ç”¨ï¼‰ã€‚
            
        batch_size (int): è®­ç»ƒæ‰¹æ¬¡å¤§å°ï¼Œé»˜è®¤ 256ã€‚
            æ¯ä¸ªæ‰¹æ¬¡åŒ…å«çš„æ ·æœ¬æ•°ï¼ˆ256-1024ï¼Œè¶Šå¤§è®­ç»ƒè¶Šå¿«ä½†éœ€æ›´å¤šæ˜¾å­˜ï¼‰ã€‚
            
        learning_rate (float): åˆå§‹å­¦ä¹ ç‡ï¼Œé»˜è®¤ 0.001ã€‚
            ä¼˜åŒ–å™¨çš„æ­¥é•¿ï¼ˆ0.0001-0.01ï¼ŒAdamä¼˜åŒ–å™¨å¸¸ç”¨0.001ï¼‰ã€‚
            
        early_stop (int): æ—©åœè½®æ•°ï¼Œé»˜è®¤ 20ã€‚
            éªŒè¯é›†æ€§èƒ½ä¸æå‡è¶…è¿‡æ­¤è½®æ•°æ—¶åœæ­¢è®­ç»ƒã€‚
            
        optimizer (str): ä¼˜åŒ–å™¨ç±»å‹ï¼Œå¯é€‰å€¼:
            - 'adam': Adam ä¼˜åŒ–å™¨ï¼ˆå¸¸ç”¨ï¼Œæ¨èé»˜è®¤ï¼‰
            - 'adamw': Adam with Weight Decayï¼ˆå¸¦æƒé‡è¡°å‡ï¼‰
            - 'sgd': éšæœºæ¢¯åº¦ä¸‹é™ï¼ˆå¤å…¸æ–¹æ³•ï¼‰
            
        loss_fn (str): æŸå¤±å‡½æ•°ï¼Œå¯é€‰å€¼:
            - 'mse': å‡æ–¹è¯¯å·®ï¼ˆé»˜è®¤ï¼Œç”¨äºå›å½’ä»»åŠ¡ï¼‰
            - 'mae': å¹³å‡ç»å¯¹è¯¯å·®ï¼ˆå¯¹å¼‚å¸¸å€¼ä¸æ•æ„Ÿï¼‰
            - 'huber': Huber æŸå¤±ï¼ˆç»“åˆ MSE å’Œ MAE ä¼˜ç‚¹ï¼‰
            
        weight_decay (float): L2 æ­£åˆ™åŒ–ç³»æ•°ï¼Œé»˜è®¤ 0.0ã€‚
            ç”¨äºé˜²æ­¢è¿‡æ‹Ÿåˆï¼ˆ0.0001-0.01ï¼Œè¾ƒå°å€¼é€šå¸¸æœ‰æ•ˆï¼‰ã€‚
            
        model_save_path (str): æ¨¡å‹ä¿å­˜è·¯å¾„ï¼Œé»˜è®¤ 'output/best_model.pth'ã€‚
            è®­ç»ƒå®Œæˆåä¿å­˜æœ€ä½³æ¨¡å‹åˆ°æ­¤è·¯å¾„ã€‚
            
        log_dir (str): æ—¥å¿—ç›®å½•ï¼Œé»˜è®¤ 'logs'ã€‚
            è®­ç»ƒæ—¥å¿—ã€TensorBoard äº‹ä»¶ç­‰ä¿å­˜ä½ç½®ã€‚
            
        verbose (bool): æ˜¯å¦æ˜¾ç¤ºè¯¦ç»†è®­ç»ƒä¿¡æ¯ï¼Œé»˜è®¤ Trueã€‚
            æ‰“å°æ¯ä¸ª epoch çš„æŸå¤±ã€æŒ‡æ ‡ç­‰ã€‚
            
        seed (Optional[int]): éšæœºç§å­ï¼Œé»˜è®¤ Noneã€‚
            ç”¨äºå¤ç°ç»“æœçš„éšæœºæ•°ç§å­ï¼ˆè®¾ç½®åç»“æœå¯å¤ç°ï¼‰ã€‚
    """
    # ==================== è®¾å¤‡é…ç½® ====================
    device: str = 'cuda'                        # è®¡ç®—è®¾å¤‡ï¼š'cuda'(GPU), 'cpu', 'cuda:0'(æŒ‡å®šGPU)
    
    # ==================== è®­ç»ƒå‚æ•° ====================
    n_epochs: int = 100                         # è®­ç»ƒè½®æ•°ï¼ˆ50-200å¸¸ç”¨ï¼‰
    batch_size: int = 256                       # æ‰¹æ¬¡å¤§å°ï¼ˆ256-1024ï¼Œè¶Šå¤§è®­ç»ƒè¶Šå¿«ä½†éœ€æ›´å¤šæ˜¾å­˜ï¼‰
    learning_rate: float = 0.001                # å­¦ä¹ ç‡ï¼ˆ0.0001-0.01ï¼ŒAdamä¼˜åŒ–å™¨å¸¸ç”¨0.001ï¼‰
    early_stop: int = 20                        # æ—©åœè½®æ•°ï¼ˆéªŒè¯é›†æ€§èƒ½ä¸æå‡åˆ™åœæ­¢ï¼‰
    
    # ==================== ä¼˜åŒ–å™¨å’ŒæŸå¤±å‡½æ•° ====================
    optimizer: str = 'adam'                     # ä¼˜åŒ–å™¨ï¼š'adam'(å¸¸ç”¨), 'adamw'(å¸¦æƒé‡è¡°å‡), 'sgd'
    loss_fn: str = 'mse'                        # æŸå¤±å‡½æ•°ï¼š'mse'(å‡æ–¹è¯¯å·®), 'mae'(å¹³å‡ç»å¯¹è¯¯å·®), 'huber'
    weight_decay: float = 0.0                   # L2æ­£åˆ™åŒ–ç³»æ•°ï¼ˆ0.0001-0.01ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆï¼‰
    
    # ==================== ä¿å­˜è·¯å¾„ ====================
    model_save_path: str = 'output/best_model.pth'  # æ¨¡å‹ä¿å­˜è·¯å¾„
    log_dir: str = 'logs'                       # æ—¥å¿—ç›®å½•
    
    # ==================== æ—¥å¿—å’Œè°ƒè¯• ====================
    verbose: bool = True                        # æ˜¯å¦æ˜¾ç¤ºè¯¦ç»†è®­ç»ƒä¿¡æ¯
    seed: Optional[int] = None                  # éšæœºç§å­ï¼ˆç”¨äºå¤ç°ç»“æœï¼‰
    
    def validate(self) -> bool:
        """éªŒè¯é…ç½®"""
        if self.n_epochs <= 0:
            raise ValueError("n_epochs å¿…é¡»å¤§äº 0")
        
        if self.batch_size <= 0:
            raise ValueError("batch_size å¿…é¡»å¤§äº 0")
        
        if self.learning_rate <= 0:
            raise ValueError("learning_rate å¿…é¡»å¤§äº 0")
        if self.weight_decay < 0:
            raise ValueError("weight_decay å¿…é¡»éè´Ÿ")
        if self.optimizer not in ['adam', 'adamw', 'sgd']:
            raise ValueError(f"ä¸æ”¯æŒçš„ä¼˜åŒ–å™¨: {self.optimizer}")
        
        if self.loss_fn not in ['mse', 'mae', 'huber']:
            raise ValueError(f"ä¸æ”¯æŒçš„æŸå¤±å‡½æ•°: {self.loss_fn}")
        
        return True


@dataclass
class LSTMConfig(BaseModelConfig):
    """
    LSTM æ¨¡å‹é…ç½®
    
    é•¿çŸ­æœŸè®°å¿†ç½‘ç»œï¼Œè®°å¿†èƒ½åŠ›æ›´å¼ºï¼Œé€‚åˆå¤æ‚æ—¶åºæ¨¡å¼ã€‚
    
    Args:
        d_feat (Optional[int]): è¾“å…¥ç‰¹å¾ç»´åº¦ï¼Œé»˜è®¤ Noneã€‚
            æ¨¡å‹è¾“å…¥ç‰¹å¾æ•°é‡ã€‚ä¸º None æ—¶åœ¨åŠ è½½æ•°æ®åè‡ªåŠ¨æ¨æ–­ã€‚
            
        hidden_size (int): LSTM éšè—å±‚å•å…ƒæ•°ï¼Œé»˜è®¤ 64ã€‚
            éšè—çŠ¶æ€çš„ç»´åº¦ï¼ˆ64-256å¸¸ç”¨èŒƒå›´ï¼‰ã€‚è¶Šå¤§æ¨¡å‹å®¹é‡è¶Šå¤§ä½†å‚æ•°è¶Šå¤šã€‚
            
        num_layers (int): LSTM å±‚æ•°ï¼Œé»˜è®¤ 2ã€‚
            å †å  LSTM å•å…ƒçš„æ•°é‡ï¼ˆ1-3å±‚ï¼Œè¿‡æ·±æ˜“è¿‡æ‹Ÿåˆï¼‰ã€‚
            
        dropout (float): Dropout æ¦‚ç‡ï¼Œé»˜è®¤ 0.3ã€‚
            å±‚é—´ dropoutï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆï¼ˆèŒƒå›´ [0, 1]ï¼Œ0.1-0.5 å¸¸ç”¨ï¼‰ã€‚
            
        bidirectional (bool): æ˜¯å¦ä½¿ç”¨åŒå‘ LSTMï¼Œé»˜è®¤ Falseã€‚
            åŒå‘ LSTM å¯æ•è·æœªæ¥ä¿¡æ¯ï¼Œä½†å‚æ•°æ•°é‡ç¿»å€ï¼Œè®¡ç®—é‡å¢åŠ ã€‚
            
        output_dim (int): è¾“å‡ºç»´åº¦ï¼Œé»˜è®¤ 1ã€‚
            é¢„æµ‹ç›®æ ‡çš„ç»´åº¦ï¼ˆé€šå¸¸ä¸º 1ï¼Œé¢„æµ‹å•ä¸ªç›®æ ‡ï¼‰ã€‚
    """
    # ==================== æ¨¡å‹æ¶æ„å‚æ•° ====================
    d_feat: Optional[int] = None        # è¾“å…¥ç‰¹å¾ç»´åº¦ï¼ˆè‡ªåŠ¨æ¨æ–­ï¼‰
    hidden_size: int = 64               # éšè—å±‚å•å…ƒæ•°ï¼ˆ64-256å¸¸ç”¨èŒƒå›´ï¼‰
    num_layers: int = 2                 # LSTMå±‚æ•°ï¼ˆ1-3å±‚ï¼Œè¿‡æ·±æ˜“è¿‡æ‹Ÿåˆï¼‰
    dropout: float = 0.3                # Dropoutæ¦‚ç‡ï¼ˆå±‚é—´dropoutï¼Œé˜²è¿‡æ‹Ÿåˆï¼‰
    bidirectional: bool = False         # åŒå‘LSTMï¼ˆå¯æ•è·æœªæ¥ä¿¡æ¯ï¼Œå‚æ•°x2ï¼‰
    
    # ==================== è¾“å‡ºå‚æ•° ====================
    output_dim: int = 1                 # è¾“å‡ºç»´åº¦ï¼ˆé€šå¸¸ä¸º1ï¼Œé¢„æµ‹å•ä¸ªç›®æ ‡ï¼‰
    
    def validate(self) -> bool:
        """
        éªŒè¯ LSTM é…ç½®å‚æ•°çš„æœ‰æ•ˆæ€§ã€‚
        """
        """éªŒè¯ LSTM é…ç½®"""
        super().validate()
        
        if self.hidden_size <= 0:
            raise ValueError("hidden_size å¿…é¡»å¤§äº 0")
        
        if self.num_layers <= 0:
            raise ValueError("num_layers å¿…é¡»å¤§äº 0")
        
        if not 0 <= self.dropout <= 1:
            raise ValueError("dropout å¿…é¡»åœ¨ [0, 1] èŒƒå›´å†…")
        
        return True


@dataclass
class GRUConfig(BaseModelConfig):
    """
    GRU æ¨¡å‹é…ç½®
    
    é—¨æ§å¾ªç¯å•å…ƒæ¨¡å‹ï¼Œå‚æ•°æ›´å°‘ï¼Œè®­ç»ƒæ›´å¿«ï¼Œé€‚åˆå¿«é€Ÿå®éªŒã€‚
    
    Args:
        d_feat (Optional[int]): è¾“å…¥ç‰¹å¾ç»´åº¦ï¼Œé»˜è®¤ Noneã€‚
            æ¨¡å‹è¾“å…¥ç‰¹å¾æ•°é‡ã€‚ä¸º None æ—¶åœ¨åŠ è½½æ•°æ®åè‡ªåŠ¨æ¨æ–­ã€‚
            
        hidden_size (int): GRU éšè—å±‚å•å…ƒæ•°ï¼Œé»˜è®¤ 64ã€‚
            éšè—çŠ¶æ€çš„ç»´åº¦ï¼ˆè¶Šå¤§æ¨¡å‹å®¹é‡è¶Šå¤§ï¼‰ã€‚
            
        num_layers (int): GRU å±‚æ•°ï¼Œé»˜è®¤ 2ã€‚
            å †å  GRU å•å…ƒçš„æ•°é‡ï¼ˆ2-3å±‚é€šå¸¸æ•ˆæœè¾ƒå¥½ï¼‰ã€‚
            
        dropout (float): Dropout æ¦‚ç‡ï¼Œé»˜è®¤ 0.3ã€‚
            å±‚é—´ dropoutï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆï¼ˆèŒƒå›´ [0, 1]ï¼Œ0.1-0.5 å¸¸ç”¨ï¼‰ã€‚
            
        bidirectional (bool): æ˜¯å¦ä½¿ç”¨åŒå‘ GRUï¼Œé»˜è®¤ Falseã€‚
            åŒå‘ GRU å¯æå‡æ€§èƒ½ä½†å‚æ•°ç¿»å€ï¼Œè®¡ç®—é‡å¢åŠ ã€‚
            
        output_dim (int): è¾“å‡ºç»´åº¦ï¼Œé»˜è®¤ 1ã€‚
            é¢„æµ‹ç›®æ ‡çš„ç»´åº¦ï¼ˆé€šå¸¸ä¸º 1ï¼Œé¢„æµ‹å•ä¸ªç›®æ ‡ï¼‰ã€‚
    """
    # ==================== æ¨¡å‹æ¶æ„å‚æ•° ====================
    d_feat: Optional[int] = None        # è¾“å…¥ç‰¹å¾ç»´åº¦ï¼ˆè‡ªåŠ¨æ¨æ–­ï¼‰
    hidden_size: int = 64               # éšè—å±‚å•å…ƒæ•°ï¼ˆè¶Šå¤§æ¨¡å‹å®¹é‡è¶Šå¤§ï¼‰
    num_layers: int = 2                 # GRUå±‚æ•°ï¼ˆ2-3å±‚é€šå¸¸æ•ˆæœè¾ƒå¥½ï¼‰
    dropout: float = 0.3                # Dropoutæ¦‚ç‡ï¼ˆ0.1-0.5ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆï¼‰
    bidirectional: bool = False         # æ˜¯å¦ä½¿ç”¨åŒå‘GRUï¼ˆå¯æå‡æ€§èƒ½ä½†å‚æ•°ç¿»å€ï¼‰
    
    # ==================== è¾“å‡ºå‚æ•° ====================
    output_dim: int = 1                 # è¾“å‡ºç»´åº¦ï¼ˆé¢„æµ‹ç›®æ ‡æ•°é‡ï¼‰
    
    def validate(self) -> bool:
        """
        éªŒè¯ GRU é…ç½®å‚æ•°çš„æœ‰æ•ˆæ€§ã€‚
        """
        """éªŒè¯ GRU é…ç½®"""
        super().validate()
        
        if self.hidden_size <= 0:
            raise ValueError("hidden_size å¿…é¡»å¤§äº 0")
        
        if self.num_layers <= 0:
            raise ValueError("num_layers å¿…é¡»å¤§äº 0")
        
        if not 0 <= self.dropout <= 1:
            raise ValueError("dropout å¿…é¡»åœ¨ [0, 1] èŒƒå›´å†…")
        
        return True


@dataclass
class TransformerConfig(BaseModelConfig):
    """
    Transformer æ¨¡å‹é…ç½®
    """
    # æ¨¡å‹æ¶æ„
    d_feat: Optional[int] = None
    d_model: int = 64
    nhead: int = 4
    num_layers: int = 2
    dim_feedforward: int = 256
    dropout: float = 0.3
    
    # ä½ç½®ç¼–ç 
    use_positional_encoding: bool = True
    max_seq_len: int = 60
    
    # è¾“å‡ºç»´åº¦
    output_dim: int = 1
    
    def validate(self) -> bool:
        """
        éªŒè¯ Transformer é…ç½®å‚æ•°çš„æœ‰æ•ˆæ€§ã€‚
        """
        """éªŒè¯ Transformer é…ç½®"""
        super().validate()
        
        if self.d_model <= 0:
            raise ValueError("d_model å¿…é¡»å¤§äº 0")
        
        if self.nhead <= 0:
            raise ValueError("nhead å¿…é¡»å¤§äº 0")
        
        if self.d_model % self.nhead != 0:
            raise ValueError("d_model å¿…é¡»èƒ½è¢« nhead æ•´é™¤")
        
        if self.num_layers <= 0:
            raise ValueError("num_layers å¿…é¡»å¤§äº 0")
        
        if not 0 <= self.dropout <= 1:
            raise ValueError("dropout å¿…é¡»åœ¨ [0, 1] èŒƒå›´å†…")
        
        return True


@dataclass
class VAEConfig(BaseModelConfig):
    """
    VAE (Variational Autoencoder) æ¨¡å‹é…ç½®
    """
    # æ¨¡å‹æ¶æ„
    input_dim: Optional[int] = None  # è¾“å…¥ç‰¹å¾ç»´åº¦ï¼ˆè‡ªåŠ¨æ¨æ–­ï¼‰
    hidden_dim: int = 128
    latent_dim: int = 16
    num_layers: int = 2
    dropout: float = 0.3
    bidirectional: bool = False
    
    # ç¼–ç å™¨ç±»å‹
    encoder_type: str = 'gru'  # 'gru', 'lstm', 'mlp'
    decoder_type: str = 'gru'  # 'gru', 'lstm', 'mlp'
    
    # VAE æŸå¤±æƒé‡
    alpha_recon: float = 0.1  # é‡æ„æŸå¤±æƒé‡
    beta_kl: float = 0.001  # KL æ•£åº¦æŸå¤±æƒé‡
    gamma_pred: float = 1.0  # é¢„æµ‹æŸå¤±æƒé‡
    
    # é‡‡æ ·ç­–ç•¥
    use_reparameterization: bool = True
    
    def validate(self) -> bool:
        """
        éªŒè¯ VAE é…ç½®å‚æ•°çš„æœ‰æ•ˆæ€§ã€‚
        """
        """éªŒè¯ VAE é…ç½®"""
        super().validate()
        
        if self.hidden_dim <= 0:
            raise ValueError("hidden_dim å¿…é¡»å¤§äº 0")
        
        if self.latent_dim <= 0:
            raise ValueError("latent_dim å¿…é¡»å¤§äº 0")
        
        if self.num_layers <= 0:
            raise ValueError("num_layers å¿…é¡»å¤§äº 0")
        
        if not 0 <= self.dropout <= 1:
            raise ValueError("dropout å¿…é¡»åœ¨ [0, 1] èŒƒå›´å†…")
        
        if self.encoder_type not in ['gru', 'lstm', 'mlp']:
            raise ValueError(f"ä¸æ”¯æŒçš„ç¼–ç å™¨ç±»å‹: {self.encoder_type}")
        
        if self.decoder_type not in ['gru', 'lstm', 'mlp']:
            raise ValueError(f"ä¸æ”¯æŒçš„è§£ç å™¨ç±»å‹: {self.decoder_type}")
        
        if self.alpha_recon < 0:
            raise ValueError("alpha_recon å¿…é¡»éè´Ÿ")
        
        if self.beta_kl < 0:
            raise ValueError("beta_kl å¿…é¡»éè´Ÿ")
        
        if self.gamma_pred < 0:
            raise ValueError("gamma_pred å¿…é¡»éè´Ÿ")
        
        return True


@dataclass
class MLPConfig(BaseModelConfig):
    """
    MLP (Multi-Layer Perceptron) æ¨¡å‹é…ç½®
    """
    # æ¨¡å‹æ¶æ„
    d_feat: Optional[int] = None
    hidden_sizes: List[int] = field(default_factory=lambda: [128, 64])
    dropout: float = 0.3
    batch_norm: bool = True
    activation: str = 'relu'  # 'relu', 'tanh', 'gelu'
    
    # è¾“å‡ºç»´åº¦
    output_dim: int = 1
    
    def validate(self) -> bool:
        """
        éªŒè¯ MLP é…ç½®å‚æ•°çš„æœ‰æ•ˆæ€§ã€‚
        """
        """éªŒè¯ MLP é…ç½®"""
        super().validate()
        
        if not self.hidden_sizes:
            raise ValueError("hidden_sizes ä¸èƒ½ä¸ºç©º")
        
        if any(size <= 0 for size in self.hidden_sizes):
            raise ValueError("hidden_sizes ä¸­çš„æ‰€æœ‰å€¼å¿…é¡»å¤§äº 0")
        
        if not 0 <= self.dropout <= 1:
            raise ValueError("dropout å¿…é¡»åœ¨ [0, 1] èŒƒå›´å†…")
        
        if self.activation not in ['relu', 'tanh', 'gelu']:
            raise ValueError(f"ä¸æ”¯æŒçš„æ¿€æ´»å‡½æ•°: {self.activation}")
        
        return True


@dataclass
class HybridGraphConfig(BaseModelConfig):
    """
    RNN+Attention+GAT+MLP æ··åˆæ¨¡å‹é…ç½® (å…¼å®¹æ¨¡å¼)
    
    âš ï¸ æ³¨æ„: è¿™æ˜¯æ•´ä½“é…ç½®ç±»ï¼Œé€‚åˆå¿«é€Ÿä½¿ç”¨ä½†æ‰©å±•æ€§è¾ƒå¼±ã€‚
    
    ğŸ†• æ¨èä½¿ç”¨æ¨¡å—åŒ–é…ç½® (modular_config.py)ï¼š
        - ç‹¬ç«‹é…ç½®æ¯ä¸ªæ¨¡å— (Temporal/Graph/Fusion)
        - çµæ´»ç»„åˆä¸åŒçš„æ¨¡å—
        - æ”¯æŒå˜ä½“æ‰©å±• (å¦‚æ›¿æ¢ä¸åŒç±»å‹çš„Attentionã€GATç­‰)
        
        Example:
            from model.modular_config import ModelConfigBuilder
            
            config = ModelConfigBuilder() \\
                .add_temporal(rnn_type='lstm', hidden_size=64) \\
                .add_graph(gat_type='correlation', hidden_dim=32) \\
                .add_fusion(hidden_sizes=[64]) \\
                .build(d_feat=20)
    
    ç»“åˆæ—¶åºç‰¹å¾æå–ï¼ˆRNN+Self-Attentionï¼‰å’Œæˆªé¢ä¿¡æ¯äº¤äº’ï¼ˆGATï¼‰çš„æ··åˆæ¶æ„ã€‚
    - RNN: å¤„ç†å•åªè‚¡ç¥¨çš„æ—¶é—´åºåˆ—ç‰¹å¾
    - Self-Attention: å¼ºåŒ–å…³é”®æ—¶é—´ç‚¹æƒé‡
    - GAT: æ•æ‰è‚¡ç¥¨é—´çš„æˆªé¢å…³è”ï¼ˆè¡Œä¸šè”åŠ¨æˆ–ç›¸å…³æ€§ï¼‰
    - MLP: èåˆé¢„æµ‹å™¨
    
    Args:
        d_feat (int): è¾“å…¥ç‰¹å¾ç»´åº¦ï¼ˆé‡ä»·æ•°æ®ç»´åº¦ï¼‰ï¼Œé»˜è®¤ 20ã€‚
        
        rnn_hidden (int): RNN éšè—å±‚å¤§å°ï¼Œé»˜è®¤ 64ã€‚
            æ§åˆ¶æ—¶åºç‰¹å¾æå–èƒ½åŠ›ï¼ˆ64-256å¸¸ç”¨ï¼‰ã€‚
            
        rnn_layers (int): RNN å±‚æ•°ï¼Œé»˜è®¤ 2ã€‚
            å †å  LSTM å±‚æ•°ï¼ˆ2-3å±‚æ•ˆæœè¾ƒå¥½ï¼‰ã€‚
            
        rnn_type (str): RNN ç±»å‹ï¼Œé»˜è®¤ 'lstm'ã€‚
            - 'lstm': é•¿çŸ­æœŸè®°å¿†ç½‘ç»œï¼ˆè®°å¿†èƒ½åŠ›æ›´å¼ºï¼‰
            - 'gru': é—¨æ§å¾ªç¯å•å…ƒï¼ˆå‚æ•°æ›´å°‘ï¼Œè®­ç»ƒæ›´å¿«ï¼‰
            
        use_attention (bool): æ˜¯å¦ä½¿ç”¨ Self-Attentionï¼Œé»˜è®¤ Trueã€‚
            å¼ºåŒ–å…³é”®æ—¶é—´ç‚¹çš„æƒé‡ï¼Œæå‡æ—¶åºå»ºæ¨¡èƒ½åŠ›ã€‚
            
        use_graph (bool): æ˜¯å¦ä½¿ç”¨å›¾ç¥ç»ç½‘ç»œï¼Œé»˜è®¤ Trueã€‚
            å¯ç”¨ GAT è¿›è¡Œæˆªé¢ä¿¡æ¯äº¤äº’ã€‚
            
        gat_heads (int): GAT æ³¨æ„åŠ›å¤´æ•°ï¼Œé»˜è®¤ 4ã€‚
            å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶ï¼ˆ4-8å¤´å¸¸ç”¨ï¼‰ã€‚
            
        gat_hidden (int): GAT éšè—å±‚ç»´åº¦ï¼Œé»˜è®¤ 32ã€‚
            å¿…é¡»èƒ½è¢« gat_heads æ•´é™¤ã€‚
            
        gat_type (str): GAT ç±»å‹ï¼Œé»˜è®¤ 'standard'ã€‚
            - 'standard': åŸºäºè¡Œä¸šå…³ç³»çš„ GATï¼ˆä½¿ç”¨è¡Œä¸šåˆ†ç±»ï¼‰
            - 'correlation': åŸºäºç›¸å…³æ€§çš„ GATï¼ˆä½¿ç”¨æ”¶ç›Šç‡ç›¸å…³æ€§ï¼‰
            
        top_k_neighbors (int): ç›¸å…³æ€§ GAT çš„é‚»å±…æ•°ï¼Œé»˜è®¤ 10ã€‚
            ä»…åœ¨ gat_type='correlation' æ—¶æœ‰æ•ˆã€‚
            
        funda_dim (Optional[int]): åŸºæœ¬é¢æ•°æ®ç»´åº¦ï¼Œé»˜è®¤ Noneã€‚
            å¦‚æœæä¾›åŸºæœ¬é¢æ•°æ®ï¼Œåœ¨è¿›å…¥ GAT å‰æ‹¼æ¥ã€‚
            
        mlp_hidden_sizes (List[int]): MLP éšè—å±‚å°ºå¯¸ï¼Œé»˜è®¤ [64]ã€‚
            èåˆé¢„æµ‹å™¨çš„éšè—å±‚é…ç½®ã€‚
            
        dropout (float): Dropout æ¦‚ç‡ï¼Œé»˜è®¤ 0.3ã€‚
            å…¨å±€ dropout ç‡ï¼ˆ0.1-0.5ï¼‰ã€‚
            
        adj_matrix_path (Optional[str]): é‚»æ¥çŸ©é˜µè·¯å¾„ï¼Œé»˜è®¤ Noneã€‚
            é¢„è®¡ç®—çš„é‚»æ¥çŸ©é˜µæ–‡ä»¶è·¯å¾„ï¼ˆ.pt æˆ– .npy æ ¼å¼ï¼‰ã€‚
    """
    # ==================== è¾“å…¥ç‰¹å¾ ====================
    d_feat: int = 20                            # è¾“å…¥ç‰¹å¾ç»´åº¦ï¼ˆé‡ä»·æ•°æ®ï¼‰
    funda_dim: Optional[int] = None             # åŸºæœ¬é¢æ•°æ®ç»´åº¦ï¼ˆå¯é€‰ï¼‰
    
    # ==================== æ—¶åºæ¨¡å— (RNN + Attention) ====================
    rnn_type: str = 'lstm'                      # RNNç±»å‹ï¼š'lstm', 'gru'
    rnn_hidden: int = 64                        # RNNéšè—å±‚å¤§å°ï¼ˆ64-256å¸¸ç”¨ï¼‰
    rnn_layers: int = 2                         # RNNå±‚æ•°ï¼ˆ2-3å±‚æ•ˆæœè¾ƒå¥½ï¼‰
    use_attention: bool = True                  # æ˜¯å¦ä½¿ç”¨Self-Attention
    
    # ==================== æˆªé¢æ¨¡å— (GAT) ====================
    use_graph: bool = True                      # æ˜¯å¦ä½¿ç”¨å›¾ç¥ç»ç½‘ç»œ
    gat_type: str = 'standard'                  # GATç±»å‹ï¼š'standard'(è¡Œä¸š), 'correlation'(ç›¸å…³æ€§)
    gat_heads: int = 4                          # GATæ³¨æ„åŠ›å¤´æ•°ï¼ˆ4-8å¤´å¸¸ç”¨ï¼‰
    gat_hidden: int = 32                        # GATéšè—å±‚ç»´åº¦ï¼ˆå¿…é¡»èƒ½è¢«gat_headsæ•´é™¤ï¼‰
    top_k_neighbors: int = 10                   # ç›¸å…³æ€§GATçš„é‚»å±…æ•°ï¼ˆgat_type='correlation'æ—¶ä½¿ç”¨ï¼‰
    
    # ==================== èåˆæ¨¡å— (MLP) ====================
    mlp_hidden_sizes: List[int] = field(default_factory=lambda: [64])  # MLPéšè—å±‚å°ºå¯¸
    
    # ==================== æ­£åˆ™åŒ– ====================
    dropout: float = 0.3                        # å…¨å±€Dropoutæ¦‚ç‡ï¼ˆ0.1-0.5ï¼‰
    
    # ==================== é‚»æ¥çŸ©é˜µ ====================
    adj_matrix_path: Optional[str] = None       # é‚»æ¥çŸ©é˜µè·¯å¾„ï¼ˆ.ptæˆ–.npyæ ¼å¼ï¼‰
    
    # ==================== è¾“å‡ºå‚æ•° ====================
    output_dim: int = 1                         # è¾“å‡ºç»´åº¦ï¼ˆé€šå¸¸ä¸º1ï¼Œé¢„æµ‹å•ä¸ªç›®æ ‡ï¼‰
    
    def validate(self) -> bool:
        """
        éªŒè¯ HybridGraph é…ç½®å‚æ•°çš„æœ‰æ•ˆæ€§ã€‚
        """
        super().validate()
        
        # éªŒè¯ RNN å‚æ•°
        if self.rnn_hidden <= 0:
            raise ValueError("rnn_hidden å¿…é¡»å¤§äº 0")
        
        if self.rnn_layers <= 0:
            raise ValueError("rnn_layers å¿…é¡»å¤§äº 0")
        
        if self.rnn_type not in ['lstm', 'gru']:
            raise ValueError(f"ä¸æ”¯æŒçš„ RNN ç±»å‹: {self.rnn_type}")
        
        # éªŒè¯ GAT å‚æ•°
        if self.use_graph:
            if self.gat_hidden <= 0:
                raise ValueError("gat_hidden å¿…é¡»å¤§äº 0")
            
            if self.gat_heads <= 0:
                raise ValueError("gat_heads å¿…é¡»å¤§äº 0")
            
            if self.gat_hidden % self.gat_heads != 0:
                raise ValueError("gat_hidden å¿…é¡»èƒ½è¢« gat_heads æ•´é™¤")
            
            if self.gat_type not in ['standard', 'correlation']:
                raise ValueError(f"ä¸æ”¯æŒçš„ GAT ç±»å‹: {self.gat_type}")
            
            if self.gat_type == 'correlation' and self.top_k_neighbors <= 0:
                raise ValueError("top_k_neighbors å¿…é¡»å¤§äº 0")
        
        # éªŒè¯ MLP å‚æ•°
        if not self.mlp_hidden_sizes:
            raise ValueError("mlp_hidden_sizes ä¸èƒ½ä¸ºç©º")
        
        if any(size <= 0 for size in self.mlp_hidden_sizes):
            raise ValueError("mlp_hidden_sizes ä¸­çš„æ‰€æœ‰å€¼å¿…é¡»å¤§äº 0")
        
        # éªŒè¯ dropout
        if not 0 <= self.dropout <= 1:
            raise ValueError("dropout å¿…é¡»åœ¨ [0, 1] èŒƒå›´å†…")
        
        return True


# é…ç½®å·¥å‚
class ModelConfigFactory:
    """
    æ¨¡å‹é…ç½®å·¥å‚ - æ ¹æ®æ¨¡å‹ç±»å‹åˆ›å»ºé…ç½®
    """
    
    _config_map = {
        'lstm': LSTMConfig,
        'gru': GRUConfig,
        'transformer': TransformerConfig,
        'vae': VAEConfig,
        'mlp': MLPConfig,
        'hybrid_graph': HybridGraphConfig,
    }
    
    @classmethod
    def create(cls, model_type: str, **kwargs) -> BaseModelConfig:
        """
        åˆ›å»ºæ¨¡å‹é…ç½®
        
        Args:
            model_type: æ¨¡å‹ç±»å‹ï¼ˆ'lstm', 'gru', 'transformer', 'vae', 'mlp'ï¼‰
            **kwargs: é…ç½®å‚æ•°
            
        Returns:
            æ¨¡å‹é…ç½®å¯¹è±¡
            
        Example:
            config = ModelConfigFactory.create('vae', hidden_dim=256, latent_dim=32)
        """
        model_type = model_type.lower()
        
        if model_type not in cls._config_map:
            raise ValueError(
                f"ä¸æ”¯æŒçš„æ¨¡å‹ç±»å‹: {model_type}\n"
                f"æ”¯æŒçš„ç±»å‹: {list(cls._config_map.keys())}"
            )
        
        config_class = cls._config_map[model_type]
        return config_class(**kwargs)
    
    @classmethod
    def from_dict(cls, config_dict: Dict[str, Any]) -> BaseModelConfig:
        """
        ä»å­—å…¸åˆ›å»ºé…ç½®ï¼ˆè‡ªåŠ¨æ£€æµ‹æ¨¡å‹ç±»å‹ï¼‰
        
        Args:
            config_dict: é…ç½®å­—å…¸ï¼Œå¿…é¡»åŒ…å« 'model_type' å­—æ®µ
            
        Returns:
            æ¨¡å‹é…ç½®å¯¹è±¡
        """
        if 'model_type' not in config_dict:
            raise ValueError("é…ç½®å­—å…¸å¿…é¡»åŒ…å« 'model_type' å­—æ®µ")
        
        model_type = config_dict.pop('model_type')
        return cls.create(model_type, **config_dict)
    
    @classmethod
    def get_template(cls, model_type: str, template: str = 'default') -> BaseModelConfig:
        """
        è·å–é¢„å®šä¹‰æ¨¡æ¿
        
        Args:
            model_type: æ¨¡å‹ç±»å‹
            template: æ¨¡æ¿åç§°ï¼ˆ'default', 'small', 'large'ï¼‰
            
        Returns:
            æ¨¡å‹é…ç½®å¯¹è±¡
        """
        templates = {
            'vae': {
                'default': VAEConfig(),
                'small': VAEConfig(
                    hidden_dim=64,
                    latent_dim=8,
                    num_layers=1,
                    n_epochs=50,
                ),
                'large': VAEConfig(
                    hidden_dim=256,
                    latent_dim=32,
                    num_layers=3,
                    n_epochs=200,
                ),
            },
            'lstm': {
                'default': LSTMConfig(),
                'small': LSTMConfig(hidden_size=32, num_layers=1),
                'large': LSTMConfig(hidden_size=128, num_layers=3),
            },
            'gru': {
                'default': GRUConfig(),
                'small': GRUConfig(hidden_size=32, num_layers=1),
                'large': GRUConfig(hidden_size=128, num_layers=3),
            },
            'transformer': {
                'default': TransformerConfig(),
                'small': TransformerConfig(d_model=32, nhead=2, num_layers=1),
                'large': TransformerConfig(d_model=128, nhead=8, num_layers=4),
            },
            'mlp': {
                'default': MLPConfig(),
                'small': MLPConfig(hidden_sizes=[64]),
                'large': MLPConfig(hidden_sizes=[256, 128, 64]),
            },
            'hybrid_graph': {
                'default': HybridGraphConfig(),
                'small': HybridGraphConfig(
                    rnn_hidden=32,
                    rnn_layers=1,
                    gat_hidden=16,
                    gat_heads=2,
                    mlp_hidden_sizes=[32],
                    n_epochs=50,
                ),
                'large': HybridGraphConfig(
                    rnn_hidden=128,
                    rnn_layers=3,
                    gat_hidden=64,
                    gat_heads=8,
                    mlp_hidden_sizes=[128, 64],
                    n_epochs=200,
                ),
            },
        }
        
        model_type = model_type.lower()
        
        if model_type not in templates:
            raise ValueError(f"ä¸æ”¯æŒçš„æ¨¡å‹ç±»å‹: {model_type}")
        
        if template not in templates[model_type]:
            raise ValueError(
                f"ä¸æ”¯æŒçš„æ¨¡æ¿: {template}\n"
                f"å¯ç”¨æ¨¡æ¿: {list(templates[model_type].keys())}"
            )
        
        return templates[model_type][template]


if __name__ == '__main__':
    # æµ‹è¯•æ¨¡å‹é…ç½®
    print("=" * 80)
    print("ModelConfig æµ‹è¯•")
    print("=" * 80)
    
    # æµ‹è¯• 1: åˆ›å»º VAE é…ç½®
    print("\n1. åˆ›å»º VAE é…ç½®:")
    vae_config = VAEConfig(
        hidden_dim=128,
        latent_dim=16,
        n_epochs=100,
        learning_rate=0.001,
    )
    print(f"  hidden_dim: {vae_config.hidden_dim}")
    print(f"  latent_dim: {vae_config.latent_dim}")
    print(f"  optimizer: {vae_config.optimizer}")
    
    # æµ‹è¯• 2: ä¿å­˜å’ŒåŠ è½½ YAML
    print("\n2. YAML åºåˆ—åŒ–:")
    yaml_path = '/tmp/vae_config.yaml'
    vae_config.to_yaml(yaml_path)
    print(f"  å·²ä¿å­˜åˆ°: {yaml_path}")
    
    vae_config2 = VAEConfig.from_yaml(yaml_path)
    print(f"  å·²åŠ è½½: latent_dim={vae_config2.latent_dim}")
    
    # æµ‹è¯• 3: è½¬æ¢ä¸ºå­—å…¸
    print("\n3. è½¬æ¢ä¸ºå­—å…¸:")
    config_dict = vae_config.to_dict()
    print(f"  keys: {list(config_dict.keys())[:5]}...")
    
    # æµ‹è¯• 4: é…ç½®å·¥å‚
    print("\n4. ä½¿ç”¨é…ç½®å·¥å‚:")
    lstm_config = ModelConfigFactory.create('lstm', hidden_size=128)
    print(f"  LSTM: {lstm_config.hidden_size}")
    
    # æµ‹è¯• 5: è·å–æ¨¡æ¿
    print("\n5. ä½¿ç”¨æ¨¡æ¿:")
    small_vae = ModelConfigFactory.get_template('vae', 'small')
    print(f"  å°å‹ VAE: hidden_dim={small_vae.hidden_dim}, latent_dim={small_vae.latent_dim}")
    
    large_vae = ModelConfigFactory.get_template('vae', 'large')
    print(f"  å¤§å‹ VAE: hidden_dim={large_vae.hidden_dim}, latent_dim={large_vae.latent_dim}")
    
    # æµ‹è¯• 6: é…ç½®éªŒè¯
    print("\n6. é…ç½®éªŒè¯:")
    try:
        invalid_config = VAEConfig(hidden_dim=-10)
    except ValueError as e:
        print(f"  âœ… æ•è·åˆ°éªŒè¯é”™è¯¯: {e}")
    
    # æµ‹è¯• 7: æ›´æ–°é…ç½®
    print("\n7. æ›´æ–°é…ç½®:")
    vae_config.update(learning_rate=0.002, n_epochs=150)
    print(f"  æ–°å­¦ä¹ ç‡: {vae_config.learning_rate}")
    print(f"  æ–°è®­ç»ƒè½®æ•°: {vae_config.n_epochs}")
    
    print("\n" + "=" * 80)
    print("âœ… ModelConfig æµ‹è¯•å®Œæˆ")
    print("=" * 80)
