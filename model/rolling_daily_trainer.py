"""
rolling_daily_trainer.py - æ»šåŠ¨çª—å£æ—¥æ‰¹æ¬¡è®­ç»ƒå™¨

å®ç°çœŸæ­£çš„ Walk-Forward è®­ç»ƒï¼š
- æ¯ä¸ªæ»šåŠ¨çª—å£ç‹¬ç«‹è®­ç»ƒ
- æ”¯æŒæ¨¡å‹æƒé‡ç»§æ‰¿ï¼ˆwarm_startï¼‰æˆ–ç‹¬ç«‹è®­ç»ƒ
- æ”¯æŒæ¯ä¸ªçª—å£ä¿å­˜ç‹¬ç«‹æ¨¡å‹
- è‡ªåŠ¨åˆå¹¶æ‰€æœ‰çª—å£çš„é¢„æµ‹ç»“æœ

ä¸ DynamicGraphTrainer çš„åŒºåˆ«ï¼š
- DynamicGraphTrainer: åœ¨åˆå¹¶åçš„å¤§æ•°æ®é›†ä¸Šè®­ç»ƒå•ä¸ªæ¨¡å‹
- RollingDailyTrainer: éå†å¤šä¸ªçª—å£ï¼Œæ¯ä¸ªçª—å£è®­ç»ƒä¸€æ¬¡

ä½¿ç”¨ç¤ºä¾‹ï¼š
    from quantclassic.model.rolling_daily_trainer import RollingDailyTrainer
    
    # åˆ›å»ºæ»šåŠ¨çª—å£åŠ è½½å™¨
    rolling_loaders = dm.create_rolling_daily_loaders()
    
    # åˆ›å»ºè®­ç»ƒå™¨
    rolling_trainer = RollingDailyTrainer(
        model_factory=lambda: HybridGraphModel.from_config(config, d_feat=input_dim).model,
        config=trainer_config,
        device='cuda',
        warm_start=True,  # ç»§æ‰¿ä¸Šä¸€çª—å£æ¨¡å‹æƒé‡
        save_each_window=True  # ä¿å­˜æ¯ä¸ªçª—å£çš„æ¨¡å‹
    )
    
    # è®­ç»ƒæ‰€æœ‰çª—å£
    results = rolling_trainer.fit(rolling_loaders, save_dir='output/rolling_models')
    
    # è·å–åˆå¹¶çš„é¢„æµ‹
    all_predictions = rolling_trainer.get_all_predictions()
"""

import torch
import torch.nn as nn
import numpy as np
import pandas as pd
from typing import Optional, Dict, Any, List, Tuple, Union, Callable
from dataclasses import dataclass, field
from pathlib import Path
import logging
from tqdm import tqdm
import time
import copy

from .dynamic_graph_trainer import DynamicGraphTrainer, DynamicTrainerConfig


@dataclass
class RollingTrainerConfig:
    """æ»šåŠ¨çª—å£è®­ç»ƒå™¨é…ç½®"""
    # ç»§æ‰¿åŸºç¡€è®­ç»ƒé…ç½®
    n_epochs: int = 20
    learning_rate: float = 0.001
    early_stop: int = 5
    weight_decay: float = 0.0
    
    # æ»šåŠ¨çª—å£ç‰¹æœ‰é…ç½®
    warm_start: bool = True  # æ˜¯å¦ç»§æ‰¿ä¸Šä¸€çª—å£çš„æ¨¡å‹æƒé‡
    save_each_window: bool = True  # æ˜¯å¦ä¿å­˜æ¯ä¸ªçª—å£çš„æ¨¡å‹
    reset_optimizer: bool = True  # æ¯ä¸ªçª—å£æ˜¯å¦é‡ç½®ä¼˜åŒ–å™¨
    reset_scheduler: bool = True  # æ¯ä¸ªçª—å£æ˜¯å¦é‡ç½®å­¦ä¹ ç‡è°ƒåº¦å™¨
    
    # ğŸ†• å­¦ä¹ ç‡è°ƒåº¦å™¨é…ç½®ï¼ˆé€ä¼ ç»™ DynamicGraphTrainerï¼‰
    use_scheduler: bool = True
    scheduler_type: str = 'plateau'
    scheduler_patience: int = 5
    scheduler_factor: float = 0.5
    scheduler_min_lr: float = 1e-6
    
    # æŸå¤±å‡½æ•°
    loss_fn: str = 'mse'
    lambda_corr: float = 0.01
    
    # æ—¥å¿—
    verbose: bool = True
    log_interval: int = 10


class RollingDailyTrainer:
    """
    æ»šåŠ¨çª—å£æ—¥æ‰¹æ¬¡è®­ç»ƒå™¨
    
    å®ç°çœŸæ­£çš„ Walk-Forward è®­ç»ƒç­–ç•¥ï¼š
    1. éå†æ¯ä¸ªæ»šåŠ¨çª—å£
    2. åœ¨å½“å‰çª—å£çš„è®­ç»ƒé›†ä¸Šè®­ç»ƒæ¨¡å‹
    3. åœ¨å½“å‰çª—å£çš„æµ‹è¯•é›†ä¸Šé¢„æµ‹
    4. ï¼ˆå¯é€‰ï¼‰å°†æ¨¡å‹æƒé‡ä¼ é€’ç»™ä¸‹ä¸€çª—å£
    5. åˆå¹¶æ‰€æœ‰çª—å£çš„é¢„æµ‹ç»“æœ
    
    Args:
        model_factory: æ¨¡å‹å·¥å‚å‡½æ•°ï¼Œæ¯æ¬¡è°ƒç”¨è¿”å›ä¸€ä¸ªæ–°çš„ nn.Module å®ä¾‹
        config: è®­ç»ƒé…ç½®
        device: è®¡ç®—è®¾å¤‡
        warm_start: æ˜¯å¦ç»§æ‰¿ä¸Šä¸€çª—å£çš„æ¨¡å‹æƒé‡
        save_each_window: æ˜¯å¦ä¿å­˜æ¯ä¸ªçª—å£çš„æ¨¡å‹
    """
    
    def __init__(
        self,
        model_factory: Callable[[], nn.Module],
        config: Optional[RollingTrainerConfig] = None,
        device: str = 'cuda',
        warm_start: bool = True,
        save_each_window: bool = True,
        **kwargs
    ):
        self.model_factory = model_factory
        self.config = config or RollingTrainerConfig(**kwargs)
        self.device = device if torch.cuda.is_available() else 'cpu'
        self.warm_start = warm_start
        self.save_each_window = save_each_window
        
        self.logger = logging.getLogger(self.__class__.__name__)
        
        # è®­ç»ƒçŠ¶æ€
        self.window_results: List[Dict[str, Any]] = []
        self.all_predictions: List[pd.DataFrame] = []
        self.current_model_state: Optional[Dict] = None
        
        self.logger.info("=" * 80)
        self.logger.info("ğŸ”„ åˆå§‹åŒ–æ»šåŠ¨çª—å£æ—¥æ‰¹æ¬¡è®­ç»ƒå™¨")
        self.logger.info("=" * 80)
        self.logger.info(f"  è®­ç»ƒç­–ç•¥: {'ç»§æ‰¿æƒé‡ (Warm Start)' if warm_start else 'ç‹¬ç«‹è®­ç»ƒ'}")
        self.logger.info(f"  è®¾å¤‡: {self.device}")
        self.logger.info(f"  æ¯çª—å£ä¿å­˜: {'æ˜¯' if save_each_window else 'å¦'}")
    
    def fit(
        self,
        rolling_loaders,
        n_epochs: Optional[int] = None,
        save_dir: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        è®­ç»ƒæ‰€æœ‰æ»šåŠ¨çª—å£
        
        Args:
            rolling_loaders: RollingDailyLoaderCollectionï¼Œç”± DataManager.create_rolling_daily_loaders è¿”å›
            n_epochs: æ¯ä¸ªçª—å£çš„è®­ç»ƒè½®æ•°
            save_dir: æ¨¡å‹ä¿å­˜ç›®å½•
            
        Returns:
            è®­ç»ƒç»“æœæ±‡æ€»å­—å…¸
        """
        n_epochs = n_epochs or self.config.n_epochs
        n_windows = len(rolling_loaders)
        
        self.logger.info("\n" + "=" * 80)
        self.logger.info("ğŸš€ å¼€å§‹æ»šåŠ¨çª—å£è®­ç»ƒ (Walk-Forward)")
        self.logger.info("=" * 80)
        self.logger.info(f"  æ€»çª—å£æ•°: {n_windows}")
        self.logger.info(f"  æ¯çª—å£è½®æ•°: {n_epochs}")
        
        if save_dir:
            save_dir = Path(save_dir)
            save_dir.mkdir(parents=True, exist_ok=True)
            self.logger.info(f"  ä¿å­˜ç›®å½•: {save_dir}")
        
        start_time = time.time()
        self.window_results = []
        self.all_predictions = []
        
        for window_idx, loaders in enumerate(rolling_loaders):
            self.logger.info("\n" + "-" * 60)
            self.logger.info(f"ğŸ“… çª—å£ {window_idx + 1}/{n_windows}")
            self.logger.info("-" * 60)
            
            # 1. åˆ›å»ºæˆ–ç»§æ‰¿æ¨¡å‹
            model = self._get_model_for_window(window_idx)
            
            # 2. åˆ›å»ºä¼˜åŒ–å™¨å’ŒæŸå¤±å‡½æ•°
            optimizer = torch.optim.Adam(
                model.parameters(),
                lr=self.config.learning_rate,
                weight_decay=self.config.weight_decay
            )
            criterion = nn.MSELoss()
            
            # 3. åˆ›å»ºå•çª—å£è®­ç»ƒå™¨ï¼ˆğŸ†• é€ä¼ è°ƒåº¦å™¨é…ç½®ï¼‰
            trainer_config = DynamicTrainerConfig(
                n_epochs=n_epochs,
                learning_rate=self.config.learning_rate,
                early_stop=self.config.early_stop,
                loss_fn=self.config.loss_fn,
                lambda_corr=self.config.lambda_corr,
                weight_decay=self.config.weight_decay,
                verbose=self.config.verbose,
                # ğŸ†• é€ä¼ å­¦ä¹ ç‡è°ƒåº¦å™¨é…ç½®
                use_scheduler=self.config.use_scheduler,
                scheduler_type=self.config.scheduler_type,
                scheduler_patience=self.config.scheduler_patience,
                scheduler_factor=self.config.scheduler_factor,
                scheduler_min_lr=self.config.scheduler_min_lr
            )
            
            window_trainer = DynamicGraphTrainer(
                model=model,
                config=trainer_config,
                device=self.device,
                optimizer=optimizer,
                criterion=criterion
            )
            
            # 4. è®­ç»ƒå½“å‰çª—å£
            save_path = None
            if save_dir and self.save_each_window:
                save_path = str(save_dir / f"window_{window_idx + 1}.pth")
            
            # æ£€æŸ¥æ¨¡å‹æ˜¯å¦å­˜åœ¨ï¼ˆæ–­ç‚¹ç»­è®­ï¼‰
            skip_training = False
            if save_path and Path(save_path).exists():
                self.logger.info(f"  âœ“ å‘ç°å·²è®­ç»ƒæ¨¡å‹: {save_path}ï¼Œè·³è¿‡è®­ç»ƒ")
                try:
                    checkpoint = torch.load(save_path, map_location=self.device)
                    model.load_state_dict(checkpoint['model_state_dict'])
                    skip_training = True
                    window_result = {
                        'best_epoch': checkpoint.get('best_epoch', 0),
                        'best_val_loss': checkpoint.get('best_val_loss', 0.0),
                        'skipped': True
                    }
                except Exception as e:
                    self.logger.warning(f"  âš ï¸ åŠ è½½å·²å­˜åœ¨æ¨¡å‹å¤±è´¥ï¼Œå°†é‡æ–°è®­ç»ƒ: {e}")
            
            if not skip_training:
                train_loader = loaders.train
                val_loader = loaders.val
                
                # æ£€æŸ¥è®­ç»ƒé›†æ˜¯å¦ä¸ºç©º
                if train_loader is None or len(train_loader) == 0:
                    self.logger.warning(f"  âš ï¸ çª—å£ {window_idx + 1} è®­ç»ƒé›†ä¸ºç©ºï¼Œè·³è¿‡è®­ç»ƒ")
                    continue
                
                self.logger.info(f"  è®­ç»ƒå¤©æ•°: {len(train_loader) if train_loader else 0}")
                self.logger.info(f"  éªŒè¯å¤©æ•°: {len(val_loader) if val_loader else 0}")
                
                window_result = window_trainer.fit(
                    train_loader=train_loader,
                    val_loader=val_loader,
                    n_epochs=n_epochs,
                    save_path=save_path
                )
            
            # 5. ä¿å­˜æ¨¡å‹çŠ¶æ€ï¼ˆç”¨äºä¸‹ä¸€çª—å£ç»§æ‰¿ï¼‰
            if self.warm_start:
                self.current_model_state = copy.deepcopy(model.state_dict())
            
            # 6. åœ¨æµ‹è¯•é›†ä¸Šé¢„æµ‹
            test_loader = loaders.test
            if test_loader and len(test_loader) > 0:
                self.logger.info(f"  æµ‹è¯•å¤©æ•°: {len(test_loader)}")
                pred_df, label_df = window_trainer.predict(
                    test_loader,
                    return_labels=True,
                    return_all_factors=True
                )
                
                # åˆå¹¶é¢„æµ‹å’Œæ ‡ç­¾
                pred_df['window_idx'] = window_idx + 1
                merged = pred_df.merge(
                    label_df.rename(columns={'label': 'y_true'}),
                    on=['trade_date', 'order_book_id'],
                    how='left'
                )
                self.all_predictions.append(merged)
                
                self.logger.info(f"  é¢„æµ‹æ ·æœ¬: {len(merged):,}")
            else:
                self.logger.warning(f"  âš ï¸ æ— æµ‹è¯•é›†")
            
            # 7. è®°å½•ç»“æœ
            window_result['window_idx'] = window_idx + 1
            window_result['save_path'] = save_path
            self.window_results.append(window_result)
            
            self.logger.info(f"âœ… çª—å£ {window_idx + 1} å®Œæˆ | "
                           f"best_epoch={window_result.get('best_epoch')} | "
                           f"best_val_loss={window_result.get('best_val_loss', 0):.6f}")
        
        # æ±‡æ€»ç»Ÿè®¡
        elapsed = time.time() - start_time
        
        summary = self._build_summary(elapsed)
        
        self.logger.info("\n" + "=" * 80)
        self.logger.info("ğŸ“Š æ»šåŠ¨çª—å£è®­ç»ƒæ±‡æ€»")
        self.logger.info("=" * 80)
        self.logger.info(f"  æ€»çª—å£: {summary['n_windows']}")
        self.logger.info(f"  æ€»è€—æ—¶: {elapsed:.1f}s ({elapsed/60:.1f} min)")
        self.logger.info(f"  å¹³å‡è®­ç»ƒæŸå¤±: {summary['avg_train_loss']:.6f}")
        self.logger.info(f"  å¹³å‡éªŒè¯æŸå¤±: {summary['avg_val_loss']:.6f}")
        self.logger.info(f"  å¹³å‡æœ€ä½³è½®æ•°: {summary['avg_best_epoch']:.1f}")
        self.logger.info(f"  æ€»é¢„æµ‹æ ·æœ¬: {summary['total_predictions']:,}")
        
        return summary
    
    def _get_model_for_window(self, window_idx: int) -> nn.Module:
        """è·å–å½“å‰çª—å£çš„æ¨¡å‹ï¼ˆæ–°å»ºæˆ–ç»§æ‰¿ï¼‰"""
        model = self.model_factory()
        model = model.to(self.device)
        
        if self.warm_start and self.current_model_state is not None and window_idx > 0:
            try:
                model.load_state_dict(self.current_model_state)
                self.logger.info(f"  ğŸ”— ç»§æ‰¿çª—å£ {window_idx} çš„æ¨¡å‹æƒé‡")
            except Exception as e:
                self.logger.warning(f"  âš ï¸ æ— æ³•åŠ è½½å‰ä¸€çª—å£æƒé‡: {e}")
        else:
            self.logger.info(f"  ğŸ†• ä½¿ç”¨å…¨æ–°æ¨¡å‹æƒé‡")
        
        return model
    
    def _build_summary(self, elapsed: float) -> Dict[str, Any]:
        """æ„å»ºè®­ç»ƒæ±‡æ€»"""
        train_losses = [r.get('train_losses', [])[-1] for r in self.window_results 
                       if r.get('train_losses')]
        val_losses = [r.get('best_val_loss') for r in self.window_results 
                     if r.get('best_val_loss') is not None]
        best_epochs = [r.get('best_epoch', 0) for r in self.window_results]
        
        total_preds = sum(len(df) for df in self.all_predictions) if self.all_predictions else 0
        
        return {
            'n_windows': len(self.window_results),
            'elapsed_time': elapsed,
            'avg_train_loss': float(np.mean(train_losses)) if train_losses else 0.0,
            'avg_val_loss': float(np.mean(val_losses)) if val_losses else 0.0,
            'avg_best_epoch': float(np.mean(best_epochs)) if best_epochs else 0.0,
            'total_predictions': total_preds,
            'window_results': self.window_results
        }
    
    def get_all_predictions(self) -> pd.DataFrame:
        """
        è·å–æ‰€æœ‰çª—å£çš„åˆå¹¶é¢„æµ‹ç»“æœ
        
        Returns:
            åˆå¹¶çš„ DataFrameï¼ŒåŒ…å«åˆ—:
            - trade_date: äº¤æ˜“æ—¥æœŸ
            - order_book_id: è‚¡ç¥¨ä»£ç 
            - pred: é¢„æµ‹å€¼
            - y_true: çœŸå®æ ‡ç­¾
            - window_idx: çª—å£ç´¢å¼•
            - pred_factor_0, pred_factor_1, ... (å¦‚æœæ˜¯å¤šå› å­)
        """
        if not self.all_predictions:
            return pd.DataFrame()
        
        combined = pd.concat(self.all_predictions, ignore_index=True)
        
        # å»é‡ï¼šå¦‚æœæœ‰é‡å æ—¥æœŸï¼Œä¿ç•™æœ€åä¸€ä¸ªçª—å£çš„é¢„æµ‹
        combined = combined.sort_values(['trade_date', 'order_book_id', 'window_idx'])
        combined = combined.drop_duplicates(
            subset=['trade_date', 'order_book_id'],
            keep='last'
        )
        
        return combined.sort_values(['trade_date', 'order_book_id']).reset_index(drop=True)
    
    def get_window_predictions(self, window_idx: int) -> pd.DataFrame:
        """è·å–æŒ‡å®šçª—å£çš„é¢„æµ‹ç»“æœ"""
        if window_idx < 0 or window_idx >= len(self.all_predictions):
            return pd.DataFrame()
        return self.all_predictions[window_idx]
    
    def get_summary(self) -> Dict[str, Any]:
        """è·å–è®­ç»ƒæ±‡æ€»ï¼ˆå…¼å®¹æ—§æ¥å£ï¼‰"""
        if not self.window_results:
            return {}
        return self._build_summary(0)


# ==================== å·¥å‚å‡½æ•° ====================

def create_rolling_trainer(
    model_class,
    model_config,
    d_feat: int,
    device: str = 'cuda',
    warm_start: bool = True,
    save_each_window: bool = True,
    **trainer_kwargs
) -> RollingDailyTrainer:
    """
    åˆ›å»ºæ»šåŠ¨çª—å£è®­ç»ƒå™¨çš„ä¾¿æ·å‡½æ•°
    
    Args:
        model_class: æ¨¡å‹ç±»ï¼ˆå¦‚ HybridGraphModelï¼‰
        model_config: æ¨¡å‹é…ç½®
        d_feat: ç‰¹å¾ç»´åº¦
        device: è®¾å¤‡
        warm_start: æ˜¯å¦ç»§æ‰¿æƒé‡
        save_each_window: æ˜¯å¦ä¿å­˜æ¯ä¸ªçª—å£æ¨¡å‹
        **trainer_kwargs: ä¼ é€’ç»™ RollingTrainerConfig çš„å‚æ•°
        
    Returns:
        RollingDailyTrainer å®ä¾‹
    """
    def model_factory():
        if hasattr(model_class, 'from_config'):
            wrapper = model_class.from_config(model_config, d_feat=d_feat)
            return wrapper.model  # è¿”å›åº•å±‚ nn.Module
        else:
            return model_class(d_feat=d_feat, **model_config.__dict__)
    
    config = RollingTrainerConfig(**trainer_kwargs)
    
    return RollingDailyTrainer(
        model_factory=model_factory,
        config=config,
        device=device,
        warm_start=warm_start,
        save_each_window=save_each_window
    )


# ==================== å•å…ƒæµ‹è¯• ====================

if __name__ == '__main__':
    import logging
    logging.basicConfig(level=logging.INFO)
    
    print("=" * 80)
    print("RollingDailyTrainer å•å…ƒæµ‹è¯•")
    print("=" * 80)
    
    # åˆ›å»ºç®€å•çš„æµ‹è¯•æ¨¡å‹
    class SimpleModel(nn.Module):
        def __init__(self, d_feat=6, hidden_size=32):
            super().__init__()
            self.rnn = nn.LSTM(d_feat, hidden_size, batch_first=True)
            self.fc = nn.Linear(hidden_size, 1)
        
        def forward(self, x, adj=None):
            out, _ = self.rnn(x)
            return self.fc(out[:, -1, :]).squeeze(-1)
    
    # æµ‹è¯•æ¨¡å‹å·¥å‚
    def model_factory():
        return SimpleModel(d_feat=6, hidden_size=32)
    
    # åˆ›å»ºè®­ç»ƒå™¨
    trainer = RollingDailyTrainer(
        model_factory=model_factory,
        device='cpu',
        warm_start=True,
        save_each_window=False
    )
    
    print("\nâœ… RollingDailyTrainer åˆ›å»ºæˆåŠŸ")
    print("\nåŠŸèƒ½:")
    print("  - æ”¯æŒçœŸæ­£çš„ Walk-Forward æ»šåŠ¨çª—å£è®­ç»ƒ")
    print("  - æ”¯æŒæ¨¡å‹æƒé‡ç»§æ‰¿ (warm_start)")
    print("  - æ”¯æŒæ¯ä¸ªçª—å£ç‹¬ç«‹ä¿å­˜æ¨¡å‹")
    print("  - è‡ªåŠ¨åˆå¹¶æ‰€æœ‰çª—å£çš„é¢„æµ‹ç»“æœ")
    print("  - å…¼å®¹ DynamicGraphTrainer çš„è®­ç»ƒæ¥å£")
    
    print("\n" + "=" * 80)
    print("âœ… RollingDailyTrainer æµ‹è¯•é€šè¿‡ï¼")
    print("=" * 80)
