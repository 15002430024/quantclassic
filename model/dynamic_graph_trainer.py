"""
dynamic_graph_trainer.py - åŠ¨æ€å›¾è®­ç»ƒå™¨

æ”¯æŒæ—¥æ‰¹æ¬¡è®­ç»ƒå’ŒåŠ¨æ€å›¾æ„å»ºçš„è®­ç»ƒå™¨ï¼Œä¸“ä¸º GNN æ¨¡å‹è®¾è®¡ã€‚

æ ¸å¿ƒç‰¹æ€§ï¼š
1. æ¯æ—¥æ„å»ºé‚»æ¥çŸ©é˜µï¼šåœ¨ collate_fn ä¸­è°ƒç”¨ GraphBuilder
2. æ—¥æ‰¹æ¬¡è®­ç»ƒï¼šæ¯ä¸ª batch æ˜¯ä¸€ä¸ªäº¤æ˜“æ—¥çš„æ‰€æœ‰è‚¡ç¥¨
3. å…¼å®¹ç°æœ‰æ¨¡å‹ï¼šå¯ä¸ HybridGraphModel æ— ç¼é›†æˆ

ä½¿ç”¨ç¤ºä¾‹ï¼š
    from quantclassic.model.dynamic_graph_trainer import DynamicGraphTrainer
    from quantclassic.data_processor.graph_builder import HybridGraphBuilder
    from quantclassic.model.hybrid_graph_models import HybridGraphModel
    
    # åˆ›å»ºå›¾æ„å»ºå™¨
    graph_builder = HybridGraphBuilder(alpha=0.7, top_k=10)
    
    # åˆ›å»ºè®­ç»ƒå™¨
    trainer = DynamicGraphTrainer(
        model=model,
        graph_builder=graph_builder,
        device='cuda'
    )
    
    # è®­ç»ƒ
    results = trainer.fit(
        train_loader=train_daily_loader,
        val_loader=val_daily_loader,
        n_epochs=20
    )
    
    # é¢„æµ‹
    predictions = trainer.predict(test_daily_loader)
"""

import torch
import torch.nn as nn
import numpy as np
import pandas as pd
from torch.utils.data import DataLoader
from typing import Optional, Dict, Any, List, Tuple, Union, Callable
from dataclasses import dataclass, field
from pathlib import Path
import logging
from tqdm import tqdm
import time


@dataclass
class DynamicTrainerConfig:
    """åŠ¨æ€å›¾è®­ç»ƒå™¨é…ç½®"""
    # è®­ç»ƒé…ç½®
    n_epochs: int = 20
    learning_rate: float = 0.001
    early_stop: int = 5
    optimizer: str = 'adam'
    weight_decay: float = 0.0
    
    # å­¦ä¹ ç‡è°ƒåº¦
    use_scheduler: bool = True
    scheduler_type: str = 'plateau'
    scheduler_patience: int = 3
    scheduler_factor: float = 0.5
    scheduler_min_lr: float = 1e-6
    
    # æŸå¤±å‡½æ•°
    loss_fn: str = 'mse'
    lambda_corr: float = 0.01  # ç›¸å…³æ€§æ­£åˆ™åŒ–æƒé‡
    
    # è®¾å¤‡
    device: str = 'cuda'
    
    # æ—¥å¿—
    verbose: bool = True
    log_interval: int = 10  # æ¯ N ä¸ª batch æ‰“å°ä¸€æ¬¡


class DynamicGraphTrainer:
    """
    åŠ¨æ€å›¾è®­ç»ƒå™¨
    
    ä¸“ä¸ºæ—¥æ‰¹æ¬¡ + åŠ¨æ€å›¾æ„å»ºè®¾è®¡çš„è®­ç»ƒå™¨ã€‚
    
    ä¸ä¼ ç»Ÿè®­ç»ƒå™¨çš„åŒºåˆ«ï¼š
    - è¾“å…¥æ˜¯ DailyGraphDataLoaderï¼Œæ¯ä¸ª batch æ˜¯ä¸€å¤©çš„æ‰€æœ‰è‚¡ç¥¨
    - é‚»æ¥çŸ©é˜µåœ¨ DataLoader ä¸­åŠ¨æ€æ„å»º
    - æ”¯æŒæˆªé¢ IC ä½œä¸ºè¯„ä¼°æŒ‡æ ‡
    
    Args:
        model: PyTorch æ¨¡å‹ï¼ˆéœ€è¦æ”¯æŒ forward(X, adj) æ¥å£ï¼‰
        graph_builder: GraphBuilder å®ä¾‹ï¼ˆå¯é€‰ï¼Œå¦‚æœ loader å·²åŒ…å«åˆ™ä¸éœ€è¦ï¼‰
        config: DynamicTrainerConfig é…ç½®
        device: è®¡ç®—è®¾å¤‡
    """
    
    def __init__(
        self,
        model: nn.Module,
        graph_builder: Optional[Any] = None,
        config: Optional[DynamicTrainerConfig] = None,
        device: str = 'cuda',
        optimizer: Optional[torch.optim.Optimizer] = None,
        scheduler: Optional[Any] = None,
        criterion: Optional[nn.Module] = None,
        **kwargs
    ):
        """åˆå§‹åŒ–åŠ¨æ€å›¾è®­ç»ƒå™¨ã€‚

        Args:
            model: éœ€è¦è®­ç»ƒçš„ PyTorch æ¨¡å‹ (nn.Module)
            graph_builder: å›¾æ„å»ºå™¨ (å¯é€‰)
            config: è®­ç»ƒé…ç½®ï¼Œè‹¥ä¸º None åˆ™ä½¿ç”¨ kwargs åˆå§‹åŒ– DynamicTrainerConfig
            device: è®­ç»ƒè®¾å¤‡å­—ç¬¦ä¸²
            optimizer: å¤–éƒ¨ä¼ å…¥çš„ä¼˜åŒ–å™¨ (å¯é€‰)
            scheduler: å¤–éƒ¨ä¼ å…¥çš„å­¦ä¹ ç‡è°ƒåº¦å™¨ (å¯é€‰)
            criterion: å¤–éƒ¨ä¼ å…¥çš„æŸå¤±å‡½æ•° (å¯é€‰)
            **kwargs: å½“ config ä¸º None æ—¶ç”¨äºåˆå§‹åŒ– DynamicTrainerConfig çš„å…³é”®å­—å‚æ•°
        """

        self.config = config or DynamicTrainerConfig(**kwargs)
        self.device = device if torch.cuda.is_available() else 'cpu'
        self.model = model.to(self.device)
        self.graph_builder = graph_builder
        
        self.logger = logging.getLogger(self.__class__.__name__)
        
        # åˆ›å»º / æ¥å—ä¼˜åŒ–å™¨
        self.optimizer = optimizer or self._create_optimizer()
        
        # åˆ›å»º / æ¥å—å­¦ä¹ ç‡è°ƒåº¦å™¨
        self.scheduler = scheduler or self._create_scheduler()
        
        # åˆ›å»º / æ¥å—æŸå¤±å‡½æ•°
        self.criterion = criterion or self._create_criterion()
        
        # è®­ç»ƒçŠ¶æ€
        self.best_val_loss = float('inf')
        self.best_epoch = 0
        self.patience_counter = 0
        self.train_losses = []
        self.val_losses = []
        self.val_ics = []
    
    def _create_optimizer(self) -> torch.optim.Optimizer:
        """åˆ›å»ºä¼˜åŒ–å™¨"""
        if self.config.optimizer.lower() == 'adam':
            return torch.optim.Adam(
                self.model.parameters(),
                lr=self.config.learning_rate,
                weight_decay=self.config.weight_decay
            )
        elif self.config.optimizer.lower() == 'sgd':
            return torch.optim.SGD(
                self.model.parameters(),
                lr=self.config.learning_rate,
                weight_decay=self.config.weight_decay,
                momentum=0.9
            )
        else:
            return torch.optim.Adam(
                self.model.parameters(),
                lr=self.config.learning_rate
            )
    
    def _create_scheduler(self):
        """åˆ›å»ºå­¦ä¹ ç‡è°ƒåº¦å™¨"""
        if not self.config.use_scheduler:
            return None
        
        if self.config.scheduler_type == 'plateau':
            return torch.optim.lr_scheduler.ReduceLROnPlateau(
                self.optimizer,
                mode='min',
                factor=self.config.scheduler_factor,
                patience=self.config.scheduler_patience,
                min_lr=self.config.scheduler_min_lr
            )
        elif self.config.scheduler_type == 'cosine':
            return torch.optim.lr_scheduler.CosineAnnealingLR(
                self.optimizer,
                T_max=self.config.n_epochs
            )
        else:
            return None
    
    def _create_criterion(self) -> nn.Module:
        """åˆ›å»ºæŸå¤±å‡½æ•°"""
        if self.config.loss_fn.lower() == 'mse':
            return nn.MSELoss()
        elif self.config.loss_fn.lower() == 'mae':
            return nn.L1Loss()
        elif self.config.loss_fn.lower() == 'huber':
            return nn.HuberLoss()
        else:
            return nn.MSELoss()
    
    def _compute_ic(self, pred: torch.Tensor, label: torch.Tensor) -> float:
        """
        è®¡ç®—æˆªé¢ IC (Information Coefficient)
        
        IC = Pearson(pred_ranks, label_ranks)
        """
        if len(pred) < 2:
            return 0.0
        
        pred_np = pred.detach().cpu().numpy().flatten()
        label_np = label.detach().cpu().numpy().flatten()
        
        # å¤„ç†å¤šå› å­è¾“å‡ºï¼šå–å¹³å‡
        if len(pred_np.shape) > 1:
            pred_np = pred_np.mean(axis=-1)
        
        # ç§»é™¤ NaN
        mask = ~(np.isnan(pred_np) | np.isnan(label_np))
        if mask.sum() < 2:
            return 0.0
        
        pred_np = pred_np[mask]
        label_np = label_np[mask]
        
        # è®¡ç®—ç›¸å…³ç³»æ•°
        corr = np.corrcoef(pred_np, label_np)[0, 1]
        return corr if not np.isnan(corr) else 0.0
    
    def fit(
        self,
        train_loader: DataLoader,
        val_loader: Optional[DataLoader] = None,
        n_epochs: Optional[int] = None,
        save_path: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        è®­ç»ƒæ¨¡å‹
        
        Args:
            train_loader: è®­ç»ƒæ•°æ®åŠ è½½å™¨ï¼ˆDailyGraphDataLoaderï¼‰
            val_loader: éªŒè¯æ•°æ®åŠ è½½å™¨ï¼ˆå¯é€‰ï¼‰
            n_epochs: è®­ç»ƒè½®æ•°ï¼ˆè¦†ç›–é…ç½®ï¼‰
            save_path: æ¨¡å‹ä¿å­˜è·¯å¾„
            
        Returns:
            è®­ç»ƒç»“æœå­—å…¸
        """
        n_epochs = n_epochs or self.config.n_epochs
        
        self.logger.info(f"å¼€å§‹è®­ç»ƒ (æ—¥æ‰¹æ¬¡æ¨¡å¼)")
        self.logger.info(f"  è®­ç»ƒå¤©æ•°: {len(train_loader)}")
        if val_loader:
            self.logger.info(f"  éªŒè¯å¤©æ•°: {len(val_loader)}")
        self.logger.info(f"  è®­ç»ƒè½®æ•°: {n_epochs}")
        self.logger.info(f"  è®¾å¤‡: {self.device}")
        
        start_time = time.time()
        
        for epoch in range(n_epochs):
            # è®­ç»ƒ
            train_loss, train_ic, train_mse, train_reg = self._train_epoch(train_loader, epoch)
            self.train_losses.append(train_loss)
            
            # éªŒè¯
            val_loss, val_ic = 0.0, 0.0
            if val_loader:
                val_loss, val_ic = self._validate_epoch(val_loader)
                self.val_losses.append(val_loss)
                self.val_ics.append(val_ic)
            
            # å­¦ä¹ ç‡è°ƒåº¦
            if self.scheduler:
                if isinstance(self.scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):
                    self.scheduler.step(val_loss if val_loader else train_loss)
                else:
                    self.scheduler.step()
            
            # æ‰“å°è¿›åº¦
            if self.config.verbose:
                lr = self.optimizer.param_groups[0]['lr']
                msg = f"Epoch {epoch+1}/{n_epochs} | "
                msg += f"Train Loss: {train_loss:.6f} (MSE: {train_mse:.6f}, Reg: {train_reg:.6f}) | Train IC: {train_ic:.4f}"
                if val_loader:
                    msg += f" | Val Loss: {val_loss:.6f} | Val IC: {val_ic:.4f}"
                msg += f" | LR: {lr:.2e}"
                self.logger.info(msg)
            
            # æ—©åœ
            current_loss = val_loss if val_loader else train_loss
            if current_loss < self.best_val_loss:
                self.best_val_loss = current_loss
                self.best_epoch = epoch + 1
                self.patience_counter = 0
                
                # ä¿å­˜æœ€ä½³æ¨¡å‹
                if save_path:
                    self._save_model(save_path)
            else:
                self.patience_counter += 1
                if self.patience_counter >= self.config.early_stop:
                    self.logger.info(f"æ—©åœè§¦å‘äº Epoch {epoch+1}")
                    break
        
        elapsed = time.time() - start_time
        
        # åŠ è½½æœ€ä½³æ¨¡å‹
        if save_path and Path(save_path).exists():
            self._load_model(save_path)
        
        results = {
            'best_epoch': self.best_epoch,
            'best_val_loss': self.best_val_loss,
            'train_losses': self.train_losses,
            'val_losses': self.val_losses,
            'val_ics': self.val_ics,
            'elapsed_time': elapsed
        }
        
        self.logger.info(f"è®­ç»ƒå®Œæˆ! æœ€ä½³ Epoch: {self.best_epoch}, "
                        f"æœ€ä½³éªŒè¯æŸå¤±: {self.best_val_loss:.6f}, "
                        f"è€—æ—¶: {elapsed:.1f}s")
        
        return results
    
    def _train_epoch(
        self, 
        loader: DataLoader,
        epoch: int
    ) -> Tuple[float, float, float, float]:
        """è®­ç»ƒä¸€ä¸ª epoch"""
        self.model.train()
        total_loss = 0.0
        total_mse = 0.0
        total_reg = 0.0
        total_ic = 0.0
        n_batches = 0
        
        pbar = tqdm(loader, desc=f"Epoch {epoch+1}", disable=not self.config.verbose)
        
        # é¦– batch é‚»æ¥çŸ©é˜µç»Ÿè®¡ï¼ˆä»…ç¬¬ä¸€ä¸ª epoch ç¬¬ä¸€ä¸ª batch æ‰“å°ï¼‰
        first_batch_logged = (epoch > 0)
        
        for batch_idx, batch in enumerate(pbar):
            # è§£åŒ… batch
            X, y, adj, stock_ids, date = batch
            
            # è·³è¿‡ç©º batch
            if len(y) == 0:
                continue
            
            # ç§»åŠ¨åˆ°è®¾å¤‡
            X = X.to(self.device)
            y = y.to(self.device)
            if adj is not None:
                adj = adj.to(self.device)
            
            # ğŸ†• é¦– batch é‚»æ¥çŸ©é˜µæ—¥å¿—ï¼ˆä»…é¦–ä¸ª epoch é¦–ä¸ª batchï¼‰
            if not first_batch_logged:
                first_batch_logged = True
                if adj is not None:
                    adj_cpu = adj.detach().cpu()
                    n = adj_cpu.shape[0]
                    diag_sum = int(adj_cpu.diag().sum().item())
                    nonzero = int((adj_cpu > 0).sum().item())
                    off_diag = nonzero - diag_sum
                    self.logger.info(f"âœ… åŠ¨æ€é‚»æ¥çŸ©é˜µå·²ä¼ å…¥æ¨¡å‹ | æ—¥æœŸ={date} | "
                                     f"N={n} | è¾¹æ•°={nonzero} | è·¨è‚¡ç¥¨è¾¹={off_diag}")
                else:
                    self.logger.warning("âš ï¸ é‚»æ¥çŸ©é˜µ adj=Noneï¼Œæ¨¡å‹å°†ä½¿ç”¨è‡ªç¯ï¼ˆå•ä½é˜µï¼‰")
            
            # å‰å‘ä¼ æ’­
            self.optimizer.zero_grad()
            
            # æ¨¡å‹æ¨ç†
            pred = self.model(X, adj)
            
            # å¤„ç†å¤šå› å­è¾“å‡º
            if len(pred.shape) > 1 and pred.shape[-1] > 1:
                # å¤šå› å­å–å¹³å‡
                pred_for_loss = pred.mean(dim=-1)
            else:
                pred_for_loss = pred.squeeze()
            
            # è®¡ç®—æŸå¤±
            mse_loss = self.criterion(pred_for_loss, y)
            
            # ç›¸å…³æ€§æ­£åˆ™åŒ–
            reg_loss = torch.tensor(0.0, device=self.device)
            if self.config.lambda_corr > 0 and len(pred.shape) > 1:
                reg_loss = self._correlation_regularization(pred)
                loss = mse_loss + self.config.lambda_corr * reg_loss
            else:
                loss = mse_loss
            
            # åå‘ä¼ æ’­
            loss.backward()
            
            # æ¢¯åº¦è£å‰ª
            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
            
            self.optimizer.step()
            
            # ç»Ÿè®¡
            total_loss += loss.item()
            total_mse += mse_loss.item()
            total_reg += reg_loss.item()
            total_ic += self._compute_ic(pred_for_loss, y)
            n_batches += 1
            
            # æ›´æ–°è¿›åº¦æ¡
            pbar.set_postfix({
                'loss': f"{loss.item():.4f}",
                'mse': f"{mse_loss.item():.4f}",
                'reg': f"{reg_loss.item():.4f}",
                'ic': f"{self._compute_ic(pred_for_loss, y):.4f}"
            })
        
        avg_loss = total_loss / max(n_batches, 1)
        avg_ic = total_ic / max(n_batches, 1)
        avg_mse = total_mse / max(n_batches, 1)
        avg_reg = total_reg / max(n_batches, 1)
        
        return avg_loss, avg_ic, avg_mse, avg_reg
    
    def _validate_epoch(self, loader: DataLoader) -> Tuple[float, float]:
        """éªŒè¯ä¸€ä¸ª epoch"""
        self.model.eval()
        total_loss = 0.0
        total_ic = 0.0
        n_batches = 0
        
        with torch.no_grad():
            for batch in loader:
                X, y, adj, stock_ids, date = batch
                
                if len(y) == 0:
                    continue
                
                X = X.to(self.device)
                y = y.to(self.device)
                if adj is not None:
                    adj = adj.to(self.device)
                
                pred = self.model(X, adj)
                
                if len(pred.shape) > 1 and pred.shape[-1] > 1:
                    pred_for_loss = pred.mean(dim=-1)
                else:
                    pred_for_loss = pred.squeeze()
                
                loss = self.criterion(pred_for_loss, y)
                
                total_loss += loss.item()
                total_ic += self._compute_ic(pred_for_loss, y)
                n_batches += 1
        
        avg_loss = total_loss / max(n_batches, 1)
        avg_ic = total_ic / max(n_batches, 1)
        
        return avg_loss, avg_ic
    
    def _correlation_regularization(self, pred: torch.Tensor) -> torch.Tensor:
        """
        è®¡ç®—å¤šå› å­ä¹‹é—´çš„ç›¸å…³æ€§æ­£åˆ™åŒ–é¡¹
        
        é¼“åŠ±ä¸åŒå› å­ä¹‹é—´æ­£äº¤ï¼ˆä½ç›¸å…³æ€§ï¼‰
        """
        if len(pred.shape) < 2 or pred.shape[-1] <= 1:
            return torch.tensor(0.0, device=pred.device)
        
        # pred: [N, F] å¤šå› å­è¾“å‡º
        # è®¡ç®—å› å­ä¹‹é—´çš„ç›¸å…³ç³»æ•°çŸ©é˜µ
        pred_centered = pred - pred.mean(dim=0, keepdim=True)
        pred_std = pred.std(dim=0, keepdim=True) + 1e-8
        pred_normalized = pred_centered / pred_std
        
        corr_matrix = torch.mm(pred_normalized.T, pred_normalized) / pred.shape[0]
        
        # æ­£åˆ™åŒ–é¡¹ï¼šéå¯¹è§’å…ƒç´ çš„å¹³æ–¹å’Œ
        mask = 1 - torch.eye(corr_matrix.shape[0], device=pred.device)
        reg = (corr_matrix * mask).pow(2).sum()
        
        return reg
    
    def predict(
        self, 
        loader: DataLoader,
        return_labels: bool = False,
        return_all_factors: bool = False
    ) -> Union[pd.DataFrame, Tuple[pd.DataFrame, pd.DataFrame]]:
        """
        é¢„æµ‹
        
        Args:
            loader: æµ‹è¯•æ•°æ®åŠ è½½å™¨
            return_labels: æ˜¯å¦è¿”å›æ ‡ç­¾
            return_all_factors: æ˜¯å¦è¿”å›æ‰€æœ‰å› å­åˆ—ï¼ˆå¤šå› å­è¾“å‡ºæ—¶ï¼‰
            
        Returns:
            é¢„æµ‹ç»“æœ DataFrameï¼ˆåŒ…å«æ—¥æœŸã€è‚¡ç¥¨IDã€é¢„æµ‹å€¼ï¼‰
            å¦‚æœ return_all_factors=Trueï¼Œåˆ™åŒ…å« pred_factor_0, pred_factor_1, ... åˆ—
        """
        self.model.eval()
        
        all_preds = []           # å­˜å‚¨å¹³å‡åçš„é¢„æµ‹å€¼
        all_factor_preds = []    # å­˜å‚¨æ‰€æœ‰å› å­çš„é¢„æµ‹å€¼ (å¤šå› å­æ—¶)
        all_labels = []
        all_stocks = []
        all_dates = []
        n_factors = None
        
        with torch.no_grad():
            for batch in tqdm(loader, desc="é¢„æµ‹ä¸­", disable=not self.config.verbose):
                X, y, adj, stock_ids, date = batch
                
                if len(y) == 0:
                    continue
                
                X = X.to(self.device)
                if adj is not None:
                    adj = adj.to(self.device)
                
                pred = self.model(X, adj)
                
                # å¤„ç†å¤šå› å­è¾“å‡º
                pred_np = pred.detach().cpu().numpy()
                if len(pred_np.shape) > 1 and pred_np.shape[-1] > 1:
                    # å¤šå› å­æ¨¡å¼ï¼š[N, F]
                    n_factors = pred_np.shape[-1]
                    if return_all_factors:
                        all_factor_preds.append(pred_np)  # [N, F]
                    # å–å¹³å‡ä½œä¸ºä¸»é¢„æµ‹å€¼
                    pred_mean = pred_np.mean(axis=-1)  # [N]
                    all_preds.append(pred_mean)
                else:
                    # å•å› å­æ¨¡å¼
                    all_preds.append(pred_np.flatten())
                
                all_labels.append(y.cpu().numpy().flatten())
                all_stocks.extend(stock_ids)
                all_dates.extend([date] * len(stock_ids))
        
        # æ„å»ºç»“æœ DataFrame
        pred_values = np.concatenate(all_preds) if all_preds else np.array([])
        
        pred_df = pd.DataFrame({
            'trade_date': all_dates,
            'order_book_id': all_stocks,
            'pred': pred_values
        })
        
        # å¦‚æœéœ€è¦è¿”å›æ‰€æœ‰å› å­
        if return_all_factors and all_factor_preds and n_factors:
            all_factors_np = np.concatenate(all_factor_preds, axis=0)  # [total_N, F]
            for f_idx in range(n_factors):
                pred_df[f'pred_factor_{f_idx}'] = all_factors_np[:, f_idx]
        
        if return_labels:
            label_df = pd.DataFrame({
                'trade_date': all_dates,
                'order_book_id': all_stocks,
                'label': np.concatenate(all_labels) if all_labels else []
            })
            return pred_df, label_df
        
        return pred_df
    
    def _save_model(self, path: str):
        """ä¿å­˜æ¨¡å‹"""
        Path(path).parent.mkdir(parents=True, exist_ok=True)
        torch.save({
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'best_epoch': self.best_epoch,
            'best_val_loss': self.best_val_loss,
        }, path)
        self.logger.info(f"æ¨¡å‹å·²ä¿å­˜: {path}")
    
    def _load_model(self, path: str):
        """åŠ è½½æ¨¡å‹"""
        checkpoint = torch.load(path, map_location=self.device)
        self.model.load_state_dict(checkpoint['model_state_dict'])
        self.logger.info(f"æ¨¡å‹å·²åŠ è½½: {path}")


# ==================== å·¥å‚å‡½æ•° ====================

def create_dynamic_trainer(
    model: nn.Module,
    graph_builder_config: Optional[Dict] = None,
    trainer_config: Optional[Dict] = None,
    device: str = 'cuda'
) -> DynamicGraphTrainer:
    """
    åˆ›å»ºåŠ¨æ€å›¾è®­ç»ƒå™¨çš„ä¾¿æ·å‡½æ•°
    
    Args:
        model: æ¨¡å‹
        graph_builder_config: å›¾æ„å»ºå™¨é…ç½®
        trainer_config: è®­ç»ƒå™¨é…ç½®
        device: è®¾å¤‡
        
    Returns:
        DynamicGraphTrainer å®ä¾‹
    """
    # åˆ›å»ºå›¾æ„å»ºå™¨
    graph_builder = None
    if graph_builder_config:
        from quantclassic.data_processor.graph_builder import GraphBuilderFactory
        graph_builder = GraphBuilderFactory.create(graph_builder_config)
    
    # åˆ›å»ºè®­ç»ƒå™¨é…ç½®
    config = DynamicTrainerConfig(**(trainer_config or {}))
    
    return DynamicGraphTrainer(
        model=model,
        graph_builder=graph_builder,
        config=config,
        device=device
    )


# ==================== å•å…ƒæµ‹è¯• ====================

if __name__ == '__main__':
    import logging
    logging.basicConfig(level=logging.INFO)
    
    print("=" * 80)
    print("DynamicGraphTrainer å•å…ƒæµ‹è¯•")
    print("=" * 80)
    
    # åˆ›å»ºç®€å•çš„æµ‹è¯•æ¨¡å‹
    class SimpleGNNModel(nn.Module):
        def __init__(self, d_feat, hidden_size, output_dim=1):
            super().__init__()
            self.rnn = nn.LSTM(d_feat, hidden_size, batch_first=True)
            self.fc = nn.Linear(hidden_size, output_dim)
        
        def forward(self, x, adj=None):
            # x: [N, T, F]
            out, _ = self.rnn(x)
            return self.fc(out[:, -1, :]).squeeze(-1)
    
    # åˆ›å»ºæ¨¡å‹
    model = SimpleGNNModel(d_feat=6, hidden_size=32, output_dim=1)
    
    # åˆ›å»ºè®­ç»ƒå™¨
    config = DynamicTrainerConfig(
        n_epochs=2,
        learning_rate=0.001,
        verbose=True
    )
    trainer = DynamicGraphTrainer(model=model, config=config, device='cpu')
    
    print("\nâœ… DynamicGraphTrainer åˆ›å»ºæˆåŠŸ")
    
    # åˆ›å»ºæ¨¡æ‹Ÿæ•°æ®
    print("\nã€æµ‹è¯•è®­ç»ƒæµç¨‹ã€‘")
    
    # æ¨¡æ‹Ÿ DailyGraphDataLoader çš„è¾“å‡º
    class MockDailyLoader:
        def __init__(self, n_days=5, n_stocks=10):
            self.n_days = n_days
            self.n_stocks = n_stocks
        
        def __len__(self):
            return self.n_days
        
        def __iter__(self):
            for i in range(self.n_days):
                X = torch.randn(self.n_stocks, 20, 6)  # [N, T, F]
                y = torch.randn(self.n_stocks)  # [N]
                adj = torch.eye(self.n_stocks)  # [N, N]
                stock_ids = [f'stock_{j}' for j in range(self.n_stocks)]
                date = f'2024-01-{i+1:02d}'
                yield X, y, adj, stock_ids, date
    
    mock_loader = MockDailyLoader(n_days=5, n_stocks=10)
    
    # æµ‹è¯•è®­ç»ƒ
    results = trainer.fit(
        train_loader=mock_loader,
        val_loader=MockDailyLoader(n_days=2, n_stocks=10),
        n_epochs=2
    )
    
    print(f"\nè®­ç»ƒç»“æœ:")
    print(f"  æœ€ä½³ Epoch: {results['best_epoch']}")
    print(f"  æœ€ä½³éªŒè¯æŸå¤±: {results['best_val_loss']:.6f}")
    print(f"  è€—æ—¶: {results['elapsed_time']:.2f}s")
    
    # æµ‹è¯•é¢„æµ‹
    print("\nã€æµ‹è¯•é¢„æµ‹æµç¨‹ã€‘")
    pred_df = trainer.predict(MockDailyLoader(n_days=3, n_stocks=10))
    print(f"  é¢„æµ‹ç»“æœå½¢çŠ¶: {pred_df.shape}")
    print(f"  åˆ—å: {list(pred_df.columns)}")
    
    print("\n" + "=" * 80)
    print("âœ… æ‰€æœ‰ DynamicGraphTrainer æµ‹è¯•é€šè¿‡ï¼")
    print("=" * 80)
