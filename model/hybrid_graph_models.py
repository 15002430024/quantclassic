"""
Hybrid Graph Models - æ··åˆå›¾ç¥ç»ç½‘ç»œæ¨¡å‹

å®ç° RNN+Attention+GAT çš„æ··åˆæ¶æ„ï¼Œç»“åˆæ—¶åºç‰¹å¾æå–å’Œæˆªé¢ä¿¡æ¯äº¤äº’ã€‚

æ¨¡å‹æ¶æ„:
1. æ—¶åºæå–å™¨ (TemporalBlock): RNN + Self-Attention
   - å¤„ç†å•åªè‚¡ç¥¨çš„æ—¶é—´åºåˆ—æ•°æ®
   - æ•æ‰é•¿æœŸä¾èµ–å’Œå…³é”®æ—¶é—´ç‚¹
   
2. æˆªé¢äº¤äº’å™¨ (GraphBlock): GAT
   - åŸºäºè¡Œä¸šæˆ–ç›¸å…³æ€§æ„å»ºé‚»æ¥çŸ©é˜µ
   - é€šè¿‡æ³¨æ„åŠ›æœºåˆ¶èšåˆé‚»å±…ä¿¡æ¯
   
3. èåˆé¢„æµ‹å™¨ (FusionBlock): MLP
   - èåˆæ—¶åºç‰¹å¾å’Œå›¾ç‰¹å¾
   - è¾“å‡ºæœ€ç»ˆé¢„æµ‹ç»“æœ
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from typing import Optional, Tuple, Union, Dict, List
from pathlib import Path
from tqdm import tqdm
from collections import OrderedDict

from .base_model import PyTorchModel
from .model_factory import register_model


# ==================== Graph Inference Mode Constants ====================
GRAPH_MODE_BATCH = 'batch'                    # åŸå§‹æ¨¡å¼ï¼šç›´æ¥åˆ‡ç‰‡é‚»æ¥çŸ©é˜µ
GRAPH_MODE_CROSS_SECTIONAL = 'cross_sectional'  # æˆªé¢æ¨¡å¼ï¼šæŒ‰æ—¥æœŸåˆ†ç»„
GRAPH_MODE_NEIGHBOR_SAMPLING = 'neighbor_sampling'  # é‚»å±…é‡‡æ ·æ¨¡å¼


# ==================== å­æ¨¡å— 1: æ—¶åºç‰¹å¾æå– ====================

class TemporalBlock(nn.Module):
    """
    æ—¶åºç‰¹å¾æå–æ¨¡å— (RNN + Self-Attention + Residual)
    
    è´Ÿè´£ä»å•åªè‚¡ç¥¨çš„æ—¶é—´åºåˆ—ä¸­æå–æ—¶åºç‰¹å¾ã€‚
    ğŸ†• æ”¯æŒæ®‹å·®è¿æ¥ï¼Œå¸®åŠ©æ¢¯åº¦ä¼ æ’­ï¼Œé˜²æ­¢æ·±å±‚ç½‘ç»œä¸­çš„æ¢¯åº¦æ¶ˆå¤±ã€‚
    
    Args:
        d_feat (int): è¾“å…¥ç‰¹å¾ç»´åº¦
        hidden_size (int): RNNéšè—å±‚å¤§å°
        num_layers (int): RNNå±‚æ•°
        rnn_type (str): RNNç±»å‹ ('lstm' æˆ– 'gru')
        dropout (float): Dropoutæ¦‚ç‡
        use_attention (bool): æ˜¯å¦ä½¿ç”¨Self-Attention
        use_residual (bool): ğŸ†• æ˜¯å¦ä½¿ç”¨æ®‹å·®è¿æ¥
    """
    
    def __init__(
        self,
        d_feat: int,
        hidden_size: int,
        num_layers: int,
        rnn_type: str = 'lstm',
        dropout: float = 0.3,
        use_attention: bool = True,
        use_residual: bool = True  # ğŸ†• æ®‹å·®è¿æ¥å¼€å…³
    ):
        super().__init__()
        
        self.use_attention = use_attention
        self.use_residual = use_residual
        self.hidden_size = hidden_size
        
        # ğŸ†• è¾“å…¥æŠ•å½±å±‚ï¼šå°† d_feat æ˜ å°„åˆ° hidden_sizeï¼Œç”¨äºæ®‹å·®è¿æ¥
        # åªæœ‰å½“ d_feat != hidden_size æ—¶æ‰éœ€è¦
        if use_residual:
            self.input_proj = nn.Linear(d_feat, hidden_size) if d_feat != hidden_size else nn.Identity()
            self.layer_norm = nn.LayerNorm(hidden_size)
        
        # RNNå±‚
        if rnn_type == 'lstm':
            self.rnn = nn.LSTM(
                input_size=d_feat,
                hidden_size=hidden_size,
                num_layers=num_layers,
                batch_first=True,
                dropout=dropout if num_layers > 1 else 0
            )
        elif rnn_type == 'gru':
            self.rnn = nn.GRU(
                input_size=d_feat,
                hidden_size=hidden_size,
                num_layers=num_layers,
                batch_first=True,
                dropout=dropout if num_layers > 1 else 0
            )
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„RNNç±»å‹: {rnn_type}")
        
        # Self-Attentionå±‚
        if use_attention:
            self.attention = nn.Sequential(
                nn.Linear(hidden_size, hidden_size // 2),
                nn.Tanh(),
                nn.Linear(hidden_size // 2, 1)
            )
        
        self.dropout = nn.Dropout(dropout)
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        å‰å‘ä¼ æ’­
        
        Args:
            x: [batch_size, seq_len, d_feat]
            
        Returns:
            time_feat: [batch_size, hidden_size] - æ—¶åºç‰¹å¾å‘é‡
        """
        # ğŸ†• ä¿å­˜è¾“å…¥ç”¨äºæ®‹å·®è¿æ¥ï¼ˆå–æœ€åä¸€ä¸ªæ—¶é—´æ­¥å¹¶æŠ•å½±ï¼‰
        if self.use_residual:
            # å°†æœ€åä¸€ä¸ªæ—¶é—´æ­¥çš„è¾“å…¥æŠ•å½±åˆ° hidden_size
            residual = self.input_proj(x[:, -1, :])  # [batch, hidden_size]
        
        # RNNç¼–ç 
        rnn_out, _ = self.rnn(x)  # [batch, seq_len, hidden]
        
        if self.use_attention:
            # Self-AttentionåŠ æƒ
            scores = self.attention(rnn_out)  # [batch, seq_len, 1]
            weights = F.softmax(scores, dim=1)  # å½’ä¸€åŒ–æ³¨æ„åŠ›æƒé‡
            
            # åŠ æƒæ±‚å’Œï¼Œå‹ç¼©æ—¶é—´ç»´åº¦
            context = torch.sum(rnn_out * weights, dim=1)  # [batch, hidden]
        else:
            # ç›´æ¥å–æœ€åä¸€ä¸ªæ—¶é—´æ­¥
            context = rnn_out[:, -1, :]  # [batch, hidden]
        
        # ğŸ†• æ®‹å·®è¿æ¥ + LayerNorm
        if self.use_residual:
            context = self.layer_norm(context + residual)
        
        return self.dropout(context)


# ==================== å­æ¨¡å— 2: æˆªé¢ç‰¹å¾æå– ====================

class GraphBlock(nn.Module):
    """
    æˆªé¢ç‰¹å¾æå–æ¨¡å— (GAT - Graph Attention Network + Residual)
    
    é€šè¿‡å›¾æ³¨æ„åŠ›æœºåˆ¶æ•æ‰è‚¡ç¥¨é—´çš„æˆªé¢å…³è”ã€‚
    ğŸ†• æ”¯æŒæ®‹å·®è¿æ¥ï¼Œå¢å¼ºæ¨¡å‹ç¨³å®šæ€§ã€‚
    
    Args:
        in_dim (int): è¾“å…¥ç‰¹å¾ç»´åº¦
        out_dim (int): è¾“å‡ºç‰¹å¾ç»´åº¦
        heads (int): æ³¨æ„åŠ›å¤´æ•°
        dropout (float): Dropoutæ¦‚ç‡
        concat (bool): æ˜¯å¦æ‹¼æ¥å¤šå¤´è¾“å‡ºï¼ˆFalseæ—¶å–å¹³å‡ï¼‰
        use_residual (bool): ğŸ†• æ˜¯å¦ä½¿ç”¨æ®‹å·®è¿æ¥
    """
    
    def __init__(
        self,
        in_dim: int,
        out_dim: int,
        heads: int = 4,
        dropout: float = 0.3,
        concat: bool = True,
        use_residual: bool = True  # ğŸ†• æ®‹å·®è¿æ¥å¼€å…³
    ):
        super().__init__()
        
        assert out_dim % heads == 0, "out_dim å¿…é¡»èƒ½è¢« heads æ•´é™¤"
        
        self.heads = heads
        self.head_dim = out_dim // heads if concat else out_dim
        self.concat = concat
        self.dropout = dropout
        self.use_residual = use_residual
        self.out_dim = out_dim
        
        # ğŸ†• æ®‹å·®è¿æ¥ï¼šæŠ•å½±å±‚ + LayerNorm
        if use_residual:
            self.residual_proj = nn.Linear(in_dim, out_dim) if in_dim != out_dim else nn.Identity()
            self.layer_norm = nn.LayerNorm(out_dim)
        
        # å¤šå¤´æ³¨æ„åŠ›å‚æ•°
        self.W = nn.ModuleList([
            nn.Linear(in_dim, self.head_dim, bias=False)
            for _ in range(heads)
        ])
        
        # æ³¨æ„åŠ›ç³»æ•°è®¡ç®—
        self.a = nn.ModuleList([
            nn.Linear(2 * self.head_dim, 1, bias=False)
            for _ in range(heads)
        ])
        
        self.leakyrelu = nn.LeakyReLU(0.2)
        self.dropout_layer = nn.Dropout(dropout)
    
    def forward(
        self,
        x: torch.Tensor,
        adj: torch.Tensor
    ) -> torch.Tensor:
        """
        å‰å‘ä¼ æ’­
        
        Args:
            x: [num_stocks, in_dim] - èŠ‚ç‚¹ç‰¹å¾
            adj: [num_stocks, num_stocks] - é‚»æ¥çŸ©é˜µï¼ˆ0/1æˆ–æƒé‡ï¼‰
            
        Returns:
            graph_feat: [num_stocks, out_dim] - å›¾ç‰¹å¾
        """
        N = x.size(0)  # è‚¡ç¥¨æ•°é‡
        
        # å¤šå¤´æ³¨æ„åŠ›è®¡ç®—
        multi_head_outputs = []
        
        for i in range(self.heads):
            # çº¿æ€§å˜æ¢
            h = self.W[i](x)  # [N, head_dim]
            
            # è®¡ç®—æ³¨æ„åŠ›ç³»æ•°
            # æ„é€  [N, N, 2*head_dim] çš„æ‹¼æ¥çŸ©é˜µ
            h_repeat = h.repeat(N, 1, 1)  # [N, N, head_dim]
            h_repeat_transpose = h.repeat(1, N).view(N, N, -1)  # [N, N, head_dim]
            h_concat = torch.cat([h_repeat, h_repeat_transpose], dim=-1)  # [N, N, 2*head_dim]
            
            # è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°
            e = self.leakyrelu(self.a[i](h_concat).squeeze(-1))  # [N, N]
            
            # ä»…ä¿ç•™é‚»æ¥çŸ©é˜µä¸­çš„è¾¹ï¼ˆmaskæ‰éé‚»å±…ï¼‰
            zero_vec = -9e15 * torch.ones_like(e)
            attention = torch.where(adj > 0, e, zero_vec)
            
            # Softmaxå½’ä¸€åŒ–
            attention = F.softmax(attention, dim=1)
            attention = self.dropout_layer(attention)
            
            # åŠ æƒèšåˆé‚»å±…ç‰¹å¾
            h_prime = torch.matmul(attention, h)  # [N, head_dim]
            multi_head_outputs.append(h_prime)
        
        # å¤šå¤´èåˆ
        if self.concat:
            output = torch.cat(multi_head_outputs, dim=-1)  # [N, heads*head_dim=out_dim]
        else:
            output = torch.mean(torch.stack(multi_head_outputs), dim=0)  # [N, out_dim]
        
        # ğŸ†• æ®‹å·®è¿æ¥ + LayerNorm
        if self.use_residual:
            residual = self.residual_proj(x)  # [N, out_dim]
            output = self.layer_norm(F.elu(output) + residual)
            return output
        else:
            return F.elu(output)


# ==================== å­æ¨¡å— 3: èåˆé¢„æµ‹å™¨ ====================

class FusionBlock(nn.Module):
    """
    èåˆé¢„æµ‹æ¨¡å— (MLP)
    
    å°†æ—¶åºç‰¹å¾å’Œå›¾ç‰¹å¾èåˆåè¿›è¡Œé¢„æµ‹ã€‚
    ğŸ†• æ”¯æŒå¤šå› å­è¾“å‡ºï¼šå½“ output_dim > 1 æ—¶ï¼Œè¾“å‡º NÃ—F çŸ©é˜µï¼ˆç ”æŠ¥ baselineï¼‰
    ğŸ†• æ”¯æŒæ®‹å·®è¿æ¥ï¼šå¢å¼ºæ·±å±‚ MLP çš„æ¢¯åº¦ä¼ æ’­
    
    Args:
        input_dim (int): è¾“å…¥ç»´åº¦
        hidden_sizes (List[int]): éšè—å±‚å°ºå¯¸åˆ—è¡¨
        output_dim (int): è¾“å‡ºç»´åº¦ï¼ˆå› å­æ•°é‡ Fï¼‰ï¼Œé»˜è®¤ 1
        dropout (float): Dropoutæ¦‚ç‡
        use_residual (bool): ğŸ†• æ˜¯å¦ä½¿ç”¨æ®‹å·®è¿æ¥
    """
    
    def __init__(
        self,
        input_dim: int,
        hidden_sizes: list,
        output_dim: int = 1,
        dropout: float = 0.3,
        use_residual: bool = False  # ğŸ†• æ®‹å·®è¿æ¥å¼€å…³
    ):
        super().__init__()
        
        self.output_dim = output_dim  # ğŸ†• ä¿å­˜è¾“å‡ºç»´åº¦
        self.use_residual = use_residual  # ğŸ†• æ®‹å·®è¿æ¥å¼€å…³
        
        layers = []
        prev_dim = input_dim
        
        for hidden_dim in hidden_sizes:
            layers.extend([
                nn.Linear(prev_dim, hidden_dim),
                nn.ReLU(),
                nn.Dropout(dropout)
            ])
            prev_dim = hidden_dim
        
        # è¾“å‡ºå±‚
        layers.append(nn.Linear(prev_dim, output_dim))
        
        self.mlp = nn.Sequential(*layers)
        
        # ğŸ†• æ®‹å·®è¿æ¥ï¼šæŠ•å½±å±‚ + LayerNorm
        if use_residual:
            # æŠ•å½±å±‚ï¼šå°†è¾“å…¥ç»´åº¦å¯¹é½åˆ°è¾“å‡ºç»´åº¦
            self.residual_proj = nn.Linear(input_dim, output_dim) if input_dim != output_dim else nn.Identity()
            self.layer_norm = nn.LayerNorm(output_dim)
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        å‰å‘ä¼ æ’­
        
        Args:
            x: [batch_size, input_dim] - èåˆç‰¹å¾
            
        Returns:
            output: 
                - å½“ output_dim=1 æ—¶: [batch_size] - å•å› å­é¢„æµ‹
                - å½“ output_dim>1 æ—¶: [batch_size, output_dim] - å¤šå› å­çŸ©é˜µ (NÃ—F)
        """
        out = self.mlp(x)
        
        # ğŸ†• æ®‹å·®è¿æ¥ + LayerNorm
        if self.use_residual:
            residual = self.residual_proj(x)
            out = self.layer_norm(out + residual)
        
        # ğŸ†• åªæœ‰å•å› å­æ—¶æ‰ squeezeï¼Œå¤šå› å­ä¿æŒ NÃ—F æ ¼å¼
        if self.output_dim == 1:
            return out.squeeze(-1)
        return out  # [batch_size, F]


# ==================== ä¸»æ¨¡å‹: HybridNet ====================

class HybridNet(nn.Module):
    """
    æ··åˆå›¾ç¥ç»ç½‘ç»œ (RNN+Attention+GAT+MLP)
    
    ç»„åˆæ—¶åºæ¨¡å—ã€å›¾æ¨¡å—å’Œèåˆæ¨¡å—ã€‚
    ğŸ†• æ”¯æŒæ®‹å·®è¿æ¥ï¼Œå¢å¼ºæ¨¡å‹è®­ç»ƒç¨³å®šæ€§ã€‚
    
    å·¥ä½œæµç¨‹:
    1. æ—¶åºæå–: å¯¹æ¯åªè‚¡ç¥¨çš„æ—¶é—´åºåˆ—æå–æ—¶åºç‰¹å¾
    2. ç‰¹å¾èåˆ: (å¯é€‰) æ‹¼æ¥åŸºæœ¬é¢æ•°æ®
    3. æˆªé¢äº¤äº’: é€šè¿‡GATè¿›è¡Œè·¨è‚¡ç¥¨çš„ä¿¡æ¯èšåˆ
    4. èåˆé¢„æµ‹: MLPè¾“å‡ºæœ€ç»ˆé¢„æµ‹
    """
    
    def __init__(
        self,
        d_feat: int,
        rnn_hidden: int,
        rnn_layers: int,
        rnn_type: str = 'lstm',
        use_attention: bool = True,
        use_graph: bool = True,
        gat_hidden: int = 32,
        gat_heads: int = 4,
        mlp_hidden_sizes: list = None,
        funda_dim: Optional[int] = None,
        dropout: float = 0.3,
        output_dim: int = 1,
        use_residual: bool = True  # ğŸ†• æ®‹å·®è¿æ¥å¼€å…³
    ):
        super().__init__()
        
        self.use_graph = use_graph
        self.funda_dim = funda_dim
        self.use_residual = use_residual
        
        # 1. æ—¶åºæ¨¡å— (ğŸ†• æ”¯æŒæ®‹å·®è¿æ¥)
        self.temporal = TemporalBlock(
            d_feat=d_feat,
            hidden_size=rnn_hidden,
            num_layers=rnn_layers,
            rnn_type=rnn_type,
            dropout=dropout,
            use_attention=use_attention,
            use_residual=use_residual  # ğŸ†• ä¼ é€’æ®‹å·®è¿æ¥å‚æ•°
        )
        
        # è®¡ç®—è¿›å…¥GATå‰çš„ç‰¹å¾ç»´åº¦
        graph_input_dim = rnn_hidden
        if funda_dim is not None:
            graph_input_dim += funda_dim
        
        # 2. å›¾æ¨¡å— (ğŸ†• æ”¯æŒæ®‹å·®è¿æ¥)
        if use_graph:
            self.gat = GraphBlock(
                in_dim=graph_input_dim,
                out_dim=gat_hidden,
                heads=gat_heads,
                dropout=dropout,
                concat=True,
                use_residual=use_residual  # ğŸ†• ä¼ é€’æ®‹å·®è¿æ¥å‚æ•°
            )
            mlp_input_dim = rnn_hidden + gat_hidden  # æ—¶åºç‰¹å¾ + å›¾ç‰¹å¾
        else:
            mlp_input_dim = rnn_hidden
        
        # å¦‚æœæœ‰åŸºæœ¬é¢æ•°æ®ï¼Œæœ€ç»ˆé¢„æµ‹æ—¶ä¹Ÿè¦æ‹¼æ¥
        if funda_dim is not None:
            mlp_input_dim += funda_dim
        
        # 3. èåˆæ¨¡å— (ğŸ†• æ”¯æŒæ®‹å·®è¿æ¥)
        if mlp_hidden_sizes is None:
            mlp_hidden_sizes = [64]
        
        self.fusion = FusionBlock(
            input_dim=mlp_input_dim,
            hidden_sizes=mlp_hidden_sizes,
            output_dim=output_dim,
            dropout=dropout,
            use_residual=use_residual  # ğŸ†• ä¼ é€’æ®‹å·®è¿æ¥å‚æ•°
        )
    
    def forward(
        self,
        x: torch.Tensor,
        adj: Optional[torch.Tensor] = None,
        funda: Optional[torch.Tensor] = None,
        return_hidden: bool = False
    ) -> Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor, torch.Tensor]]:
        """
        å‰å‘ä¼ æ’­
        
        Args:
            x: [batch_size, seq_len, d_feat] - æ—¶åºé‡ä»·æ•°æ®
            adj: [batch_size, batch_size] - é‚»æ¥çŸ©é˜µ (å¯é€‰)
            funda: [batch_size, funda_dim] - åŸºæœ¬é¢æ•°æ® (å¯é€‰)
            return_hidden: æ˜¯å¦è¿”å›éšè—ç‰¹å¾ï¼ˆç”¨äºç›¸å…³æ€§æ­£åˆ™åŒ–ï¼‰
            
        Returns:
            å¦‚æœ return_hidden=False:
                pred: [batch_size] - é¢„æµ‹ç»“æœ
            å¦‚æœ return_hidden=True:
                (pred, time_feat, combined) - é¢„æµ‹ç»“æœã€æ—¶åºç‰¹å¾ã€èåˆç‰¹å¾
        """
        # Step 1: æå–æ—¶åºç‰¹å¾
        time_feat = self.temporal(x)  # [batch, rnn_hidden]
        
        # Step 2: ç‰¹å¾èåˆ (æ‹¼æ¥åŸºæœ¬é¢)
        if self.use_graph:
            # å¤„ç† adj ä¸º None çš„æƒ…å†µ
            current_adj = adj
            if current_adj is None:
                # ä½¿ç”¨å•ä½çŸ©é˜µï¼ˆè‡ªç¯ï¼‰ï¼Œç¡®ä¿ç»´åº¦åŒ¹é…
                # è¿™æ ·å³ä½¿æ²¡æœ‰é‚»æ¥çŸ©é˜µï¼Œæ¨¡å‹ä¹Ÿèƒ½è¿è¡Œï¼ˆé€€åŒ–ä¸ºæ— å›¾äº¤äº’ï¼‰
                current_adj = torch.eye(x.size(0), device=x.device)

            if funda is not None:
                # æ‹¼æ¥åŸºæœ¬é¢æ•°æ®ä½œä¸ºå›¾çš„è¾“å…¥
                graph_input = torch.cat([time_feat, funda], dim=-1)
            else:
                graph_input = time_feat
            
            # Step 3: å›¾ç‰¹å¾æå–
            graph_feat = self.gat(graph_input, current_adj)  # [batch, gat_hidden]
            
            # Step 4: æ‹¼æ¥æ—¶åºç‰¹å¾å’Œå›¾ç‰¹å¾
            if funda is not None:
                combined = torch.cat([time_feat, graph_feat, funda], dim=-1)
            else:
                combined = torch.cat([time_feat, graph_feat], dim=-1)
        else:
            # ä¸ä½¿ç”¨å›¾ç½‘ç»œï¼Œç›´æ¥ä½¿ç”¨æ—¶åºç‰¹å¾
            if funda is not None:
                combined = torch.cat([time_feat, funda], dim=-1)
            else:
                combined = time_feat
        
        # Step 5: èåˆé¢„æµ‹
        pred = self.fusion(combined)
        
        if return_hidden:
            return pred, time_feat, combined
        return pred


# ==================== PyTorchæ¨¡å‹å°è£… ====================

@register_model('hybrid_graph')
@register_model('HybridGraph')
class HybridGraphModel(PyTorchModel):
    """
    æ··åˆå›¾ç¥ç»ç½‘ç»œæ¨¡å‹ (RNN+Attention+GAT+MLP)
    
    ç»“åˆæ—¶åºç‰¹å¾æå–å’Œæˆªé¢ä¿¡æ¯äº¤äº’çš„æ··åˆæ¶æ„ï¼Œé€‚ç”¨äºè‚¡ç¥¨æ”¶ç›Šé¢„æµ‹ã€‚
    
    ç‰¹ç‚¹:
    - RNN+Self-Attention: æ•æ‰å•åªè‚¡ç¥¨çš„æ—¶åºè§„å¾‹
    - GAT: å­¦ä¹ è‚¡ç¥¨é—´çš„æˆªé¢å…³è”ï¼ˆè¡Œä¸šè”åŠ¨/ç›¸å…³æ€§ï¼‰
    - çµæ´»çš„èåˆæœºåˆ¶: æ”¯æŒæ‹¼æ¥åŸºæœ¬é¢æ•°æ®
    
    Example:
        # åˆ›å»ºæ¨¡å‹
        model = HybridGraphModel(
            d_feat=20,
            rnn_hidden=64,
            rnn_layers=2,
            use_graph=True,
            gat_hidden=32,
            gat_heads=4,
            adj_matrix_path='adj_matrix.pt',
            n_epochs=100
        )
        
        # è®­ç»ƒ
        model.fit(train_loader, valid_loader)
        
        # é¢„æµ‹
        predictions = model.predict(test_loader)
        
        # ğŸ†• ä½¿ç”¨é‚»å±…é‡‡æ ·æ¨¡å¼é¢„æµ‹ï¼ˆä¿æŒå›¾ç»“æ„å®Œæ•´æ€§ï¼‰
        predictions = model.predict(test_loader, graph_inference_mode='neighbor_sampling')
    
    Graph Inference Modes:
        - 'batch' (é»˜è®¤): ç›´æ¥æŒ‰ batch åˆ‡ç‰‡é‚»æ¥çŸ©é˜µï¼Œå¯èƒ½ä¸¢å¤±é‚»å±…ä¿¡æ¯
        - 'cross_sectional': æŒ‰æ—¥æœŸåˆ†ç»„ï¼ŒåŒä¸€å¤©æ‰€æœ‰è‚¡ç¥¨ä¸€èµ·æ¨ç†
        - 'neighbor_sampling': é‚»å±…é‡‡æ ·æ¨¡å¼ï¼Œä¸»åŠ¨æ‹‰å–ç¼ºå¤±é‚»å±…çš„ç¼“å­˜ç‰¹å¾
    """
    
    def __init__(
        self,
        d_feat: int = 20,
        rnn_hidden: int = 64,
        rnn_layers: int = 2,
        rnn_type: str = 'lstm',
        use_attention: bool = True,
        use_graph: bool = True,
        gat_type: str = 'standard',
        gat_hidden: int = 32,
        gat_heads: int = 4,
        top_k_neighbors: int = 10,
        mlp_hidden_sizes: list = None,
        funda_dim: Optional[int] = None,
        dropout: float = 0.3,
        adj_matrix_path: Optional[str] = None,
        use_residual: bool = True,  # ğŸ†• æ®‹å·®è¿æ¥å¼€å…³
        # ğŸ†• å¤šå› å­è¾“å‡ºå‚æ•°
        output_dim: int = 1,  # ğŸ†• è¾“å‡ºç»´åº¦ï¼ˆå› å­æ•°é‡ Fï¼‰ï¼Œé»˜è®¤ 1ï¼Œè®¾ç½®ä¸º F>1 æ—¶è¾“å‡º NÃ—F çŸ©é˜µ
        # ğŸ†• Graph-Aware Inference å‚æ•°
        graph_inference_mode: str = 'batch',  # 'batch', 'cross_sectional', 'neighbor_sampling'
        max_neighbors: int = 10,  # é‚»å±…é‡‡æ ·æ—¶çš„æœ€å¤§é‚»å±…æ•°
        cache_size: int = 5000,   # èŠ‚ç‚¹çŠ¶æ€ç¼“å­˜å¤§å°
        # ğŸ†• æ•°æ®æ ¼å¼å‚æ•°
        stock_idx_position: Optional[int] = None,  # è‚¡ç¥¨ç´¢å¼•åœ¨ batch ä¸­çš„ä½ç½®ï¼ˆ0-indexedï¼‰
        funda_position: Optional[int] = None,      # åŸºæœ¬é¢æ•°æ®åœ¨ batch ä¸­çš„ä½ç½®ï¼ˆ0-indexedï¼‰
        # ğŸ†• ç›¸å…³æ€§æ­£åˆ™åŒ–å‚æ•°
        lambda_corr: float = 0.0,  # ç›¸å…³æ€§æ­£åˆ™åŒ–æƒé‡ï¼Œ0è¡¨ç¤ºä¸ä½¿ç”¨
        **kwargs
    ):
        """
        Args:
            d_feat: è¾“å…¥ç‰¹å¾ç»´åº¦
            rnn_hidden: RNNéšè—å±‚å¤§å°
            rnn_layers: RNNå±‚æ•°
            rnn_type: RNNç±»å‹ ('lstm' æˆ– 'gru')
            use_attention: æ˜¯å¦ä½¿ç”¨Self-Attention
            use_graph: æ˜¯å¦ä½¿ç”¨å›¾ç¥ç»ç½‘ç»œ
            gat_type: GATç±»å‹ ('standard' æˆ– 'correlation')
            gat_hidden: GATéšè—å±‚ç»´åº¦
            gat_heads: GATæ³¨æ„åŠ›å¤´æ•°
            top_k_neighbors: ç›¸å…³æ€§GATçš„é‚»å±…æ•°
            mlp_hidden_sizes: MLPéšè—å±‚å°ºå¯¸åˆ—è¡¨
            funda_dim: åŸºæœ¬é¢æ•°æ®ç»´åº¦
            dropout: Dropoutæ¦‚ç‡
            adj_matrix_path: é‚»æ¥çŸ©é˜µè·¯å¾„
            use_residual: ğŸ†• æ˜¯å¦ä½¿ç”¨æ®‹å·®è¿æ¥ (LayerNorm + Skip Connection)
            output_dim: ğŸ†• è¾“å‡ºç»´åº¦ï¼ˆå› å­æ•°é‡ Fï¼‰ï¼Œé»˜è®¤ 1ï¼Œè®¾ç½®ä¸º F>1 æ—¶è¾“å‡º NÃ—F çŸ©é˜µ
            lambda_corr: ğŸ†• ç›¸å…³æ€§æ­£åˆ™åŒ–æƒé‡ï¼Œ0è¡¨ç¤ºä¸ä½¿ç”¨
        """
        # ğŸ†• å°† lambda_corr ä¼ é€’ç»™åŸºç±»
        super().__init__(lambda_corr=lambda_corr, **kwargs)
        
        # ä¿å­˜é…ç½®
        self.d_feat = d_feat
        self.rnn_hidden = rnn_hidden
        self.rnn_layers = rnn_layers
        self.rnn_type = rnn_type
        self.use_attention = use_attention
        self.use_graph = use_graph
        self.gat_type = gat_type
        self.gat_hidden = gat_hidden
        self.gat_heads = gat_heads
        self.top_k_neighbors = top_k_neighbors
        self.mlp_hidden_sizes = mlp_hidden_sizes or [64]
        self.funda_dim = funda_dim
        self.dropout_rate = dropout
        self.adj_matrix_path = adj_matrix_path
        self.use_residual = use_residual  # ğŸ†• ä¿å­˜æ®‹å·®è¿æ¥é…ç½®
        self.output_dim = output_dim  # ğŸ†• ä¿å­˜è¾“å‡ºç»´åº¦ï¼ˆå¤šå› å­ï¼‰
        
        # ğŸ†• Graph-Aware Inference é…ç½®
        self.graph_inference_mode = graph_inference_mode
        self.max_neighbors = max_neighbors
        self.cache_size = cache_size
        
        # ğŸ†• æ•°æ®æ ¼å¼é…ç½®ï¼ˆæ˜ç¡®æŒ‡å®šä½ç½®ï¼Œé¿å…å¯å‘å¼çŒœæµ‹ï¼‰
        self.stock_idx_position = stock_idx_position
        self.funda_position = funda_position
        
        # ğŸ†• èŠ‚ç‚¹çŠ¶æ€ç¼“å­˜ (LRU Cache)
        # Key: stock_idx (int), Value: temporal_feature (Tensor)
        self._node_state_cache: OrderedDict[int, torch.Tensor] = OrderedDict()
        
        # ğŸ†• é¢„è®¡ç®—é‚»å±…ç´¢å¼•ç¼“å­˜ï¼ˆé¿å…é‡å¤è®¡ç®— Top-Kï¼‰
        self._neighbor_cache: Optional[Dict[int, torch.Tensor]] = None
        
        # åˆ›å»ºæ¨¡å‹ (ğŸ†• æ”¯æŒæ®‹å·®è¿æ¥ + å¤šå› å­è¾“å‡º)
        self.model = HybridNet(
            d_feat=d_feat,
            rnn_hidden=rnn_hidden,
            rnn_layers=rnn_layers,
            rnn_type=rnn_type,
            use_attention=use_attention,
            use_graph=use_graph,
            gat_hidden=gat_hidden,
            gat_heads=gat_heads,
            mlp_hidden_sizes=self.mlp_hidden_sizes,
            funda_dim=funda_dim,
            dropout=dropout,
            output_dim=output_dim,  # ğŸ†• ä¼ é€’è¾“å‡ºç»´åº¦
            use_residual=use_residual  # ğŸ†• ä¼ é€’æ®‹å·®è¿æ¥å‚æ•°
        ).to(self.device)
        
        # åˆ›å»ºä¼˜åŒ–å™¨å’ŒæŸå¤±å‡½æ•°
        self.optimizer = self._get_optimizer()
        self.criterion = self._get_loss_fn()
        
        # åŠ è½½é‚»æ¥çŸ©é˜µï¼›åŠ¨æ€å›¾æ¨¡å¼ä¸‹å…è®¸ä¸ºç©ºï¼ˆæŒ‰ batch æä¾› adjï¼‰
        self.adj_matrix = self._load_adj_matrix(adj_matrix_path)

        # ä»…åœ¨éœ€è¦é™æ€é‚»æ¥ä½†æœªåŠ è½½æ—¶ç»™å‡ºè­¦å‘Šï¼›
        # å¦‚æœç”¨æˆ·èµ°æ—¥æ‰¹æ¬¡åŠ¨æ€å›¾ï¼ˆæ¯ä¸ª batch ä¼ å…¥ adjï¼‰ï¼Œä¸å†è¯¯æŠ¥ã€‚
        expects_static_adj = self.use_graph and adj_matrix_path is not None
        using_dynamic_adj = self.use_graph and adj_matrix_path is None and self.graph_inference_mode == GRAPH_MODE_BATCH

        if expects_static_adj and self.adj_matrix is None:
            self.logger.warning("âš ï¸ å¯ç”¨äº†å›¾ç½‘ç»œ (use_graph=True) ä½†æœªåŠ è½½é‚»æ¥çŸ©é˜µã€‚")
            self.logger.warning("   æ¨¡å‹å°†ä½¿ç”¨å•ä½çŸ©é˜µï¼ˆä»…è‡ªç¯ï¼‰ï¼Œè¿™æ„å‘³ç€æ²¡æœ‰è·¨è‚¡ç¥¨çš„ä¿¡æ¯äº¤äº’ã€‚")
            self.logger.warning("   è‹¥éœ€ä½¿ç”¨å›¾ç‰¹å¾ï¼Œè¯·æä¾› adj_matrix_pathã€‚")
        elif using_dynamic_adj and self.adj_matrix is None:
            self.logger.info("ğŸŸ¢ åŠ¨æ€å›¾æ¨¡å¼ï¼šæœªåŠ è½½é™æ€é‚»æ¥çŸ©é˜µï¼Œè®­ç»ƒ/æ¨ç†å°†ä½¿ç”¨ batch å†…æä¾›çš„ adjã€‚")
        
        self.logger.info(f"HybridGraph æ¨¡å‹å‚æ•°:")
        self.logger.info(f"  ç‰¹å¾ç»´åº¦: {d_feat}")
        self.logger.info(f"  RNNç±»å‹: {rnn_type}, éšè—å±‚: {rnn_hidden}, å±‚æ•°: {rnn_layers}")
        self.logger.info(f"  ä½¿ç”¨Attention: {use_attention}")
        self.logger.info(f"  ä½¿ç”¨å›¾ç½‘ç»œ: {use_graph}")
        self.logger.info(f"  ğŸ†• æ®‹å·®è¿æ¥: {use_residual}")  # ğŸ†• æ˜¾ç¤ºæ®‹å·®è¿æ¥çŠ¶æ€
        if use_graph:
            self.logger.info(f"  GATç±»å‹: {gat_type}, éšè—å±‚: {gat_hidden}, å¤´æ•°: {gat_heads}")
        if funda_dim:
            self.logger.info(f"  åŸºæœ¬é¢ç»´åº¦: {funda_dim}")
        self.logger.info(f"  MLPéšè—å±‚: {self.mlp_hidden_sizes}")
        
        # ğŸ†• æ˜¾ç¤º Graph-Aware Inference é…ç½®
        self.logger.info(f"  ğŸ†• å›¾æ¨ç†æ¨¡å¼: {graph_inference_mode}")
        if graph_inference_mode == GRAPH_MODE_NEIGHBOR_SAMPLING:
            self.logger.info(f"     æœ€å¤§é‚»å±…æ•°: {max_neighbors}")
            self.logger.info(f"     ç¼“å­˜å¤§å°: {cache_size}")
        
        # ğŸ†• æ˜¾ç¤ºæ•°æ®æ ¼å¼é…ç½®
        if stock_idx_position is not None:
            self.logger.info(f"  ğŸ†• è‚¡ç¥¨ç´¢å¼•ä½ç½®: batch[{stock_idx_position}]")
        if funda_position is not None:
            self.logger.info(f"  ğŸ†• åŸºæœ¬é¢æ•°æ®ä½ç½®: batch[{funda_position}]")

    def to(self, device):
        """å°†åº•å±‚ nn.Module è¿ç§»åˆ°ç›®æ ‡è®¾å¤‡å¹¶åŒæ­¥æ›´æ–°è‡ªèº« device."""
        target_device = torch.device(device) if not isinstance(device, torch.device) else device
        self.device = target_device
        self.model = self.model.to(self.device)
        if self.adj_matrix is not None:
            self.adj_matrix = self.adj_matrix.to(self.device)
        return self
    
    def parameters(self):
        """å§”æ‰˜ç»™å†…éƒ¨ nn.Module çš„ parameters æ–¹æ³•ï¼Œä¾›å¤–éƒ¨ä¼˜åŒ–å™¨ä½¿ç”¨"""
        return self.model.parameters()
    
    def named_parameters(self):
        """å§”æ‰˜ç»™å†…éƒ¨ nn.Module çš„ named_parameters æ–¹æ³•"""
        return self.model.named_parameters()
    
    def state_dict(self):
        """å§”æ‰˜ç»™å†…éƒ¨ nn.Module çš„ state_dict æ–¹æ³•"""
        return self.model.state_dict()
    
    def load_state_dict(self, state_dict, strict=True):
        """å§”æ‰˜ç»™å†…éƒ¨ nn.Module çš„ load_state_dict æ–¹æ³•"""
        return self.model.load_state_dict(state_dict, strict=strict)
    
    def train(self, mode=True):
        """å§”æ‰˜ç»™å†…éƒ¨ nn.Module çš„ train æ–¹æ³•ï¼Œè®¾ç½®è®­ç»ƒæ¨¡å¼"""
        self.model.train(mode)
        return self
    
    def eval(self):
        """å§”æ‰˜ç»™å†…éƒ¨ nn.Module çš„ eval æ–¹æ³•ï¼Œè®¾ç½®è¯„ä¼°æ¨¡å¼"""
        self.model.eval()
        return self
    
    def __call__(self, *args, **kwargs):
        """ä½¿å°è£…ç±»å¯ä»¥åƒ nn.Module ä¸€æ ·è¢«è°ƒç”¨"""
        return self.forward(*args, **kwargs)
    
    def forward(self, x, adj=None, funda=None, return_hidden=False):
        """å§”æ‰˜ç»™å†…éƒ¨ nn.Module çš„ forward æ–¹æ³•"""
        return self.model(x, adj, funda, return_hidden)
    
    # ==================== ğŸ†• Node State Cache Methods ====================
    
    def _update_node_cache(self, stock_idx: torch.Tensor, temporal_features: torch.Tensor):
        """
        æ›´æ–°èŠ‚ç‚¹çŠ¶æ€ç¼“å­˜
        
        åœ¨æ¯æ¬¡ forward åè°ƒç”¨ï¼Œå°†è®¡ç®—å‡ºçš„æ—¶åºç‰¹å¾å­˜å…¥ç¼“å­˜ï¼Œ
        ä¾›åç»­ batch çš„é‚»å±…é‡‡æ ·ä½¿ç”¨ã€‚
        
        Args:
            stock_idx: [batch_size] è‚¡ç¥¨å…¨å±€ç´¢å¼•
            temporal_features: [batch_size, hidden_dim] æ—¶åºç‰¹å¾
        """
        if stock_idx is None or temporal_features is None:
            return
        
        # è½¬æ¢ä¸º CPU ä»¥èŠ‚çœ GPU æ˜¾å­˜
        features_cpu = temporal_features.detach().cpu()
        indices = stock_idx.cpu().tolist()
        
        for idx, feat in zip(indices, features_cpu):
            # å¦‚æœå·²å­˜åœ¨ï¼Œå…ˆåˆ é™¤å†æ·»åŠ ï¼ˆç§»åˆ°æœ«å°¾ï¼ŒLRUï¼‰
            if idx in self._node_state_cache:
                del self._node_state_cache[idx]
            self._node_state_cache[idx] = feat
            
            # è¶…å‡ºå®¹é‡æ—¶ï¼Œåˆ é™¤æœ€æ—§çš„æ¡ç›®
            if len(self._node_state_cache) > self.cache_size:
                self._node_state_cache.popitem(last=False)
    
    def _fetch_cached_features(
        self, 
        neighbor_idx: torch.Tensor, 
        fallback_feature: Optional[torch.Tensor] = None
    ) -> torch.Tensor:
        """
        ä»ç¼“å­˜ä¸­è·å–é‚»å±…ç‰¹å¾
        
        Args:
            neighbor_idx: [num_neighbors] éœ€è¦è·å–çš„é‚»å±…ç´¢å¼•
            fallback_feature: ç¼“å­˜æœªå‘½ä¸­æ—¶çš„åå¤‡ç‰¹å¾ï¼ˆå¦‚å½“å‰ batch çš„å‡å€¼ï¼‰
            
        Returns:
            [num_neighbors, hidden_dim] é‚»å±…ç‰¹å¾
        """
        features = []
        indices = neighbor_idx.cpu().tolist()
        
        for idx in indices:
            if idx in self._node_state_cache:
                features.append(self._node_state_cache[idx])
            elif fallback_feature is not None:
                # ç¡®ä¿ fallback_feature åœ¨ CPU ä¸Šï¼Œä»¥ä¾¿ä¸ç¼“å­˜ä¸­çš„ç‰¹å¾ï¼ˆä¹Ÿåœ¨ CPU ä¸Šï¼‰ä¸€è‡´
                features.append(fallback_feature.cpu())
            else:
                # ä½¿ç”¨é›¶å‘é‡ä½œä¸ºæœ€ç»ˆåå¤‡
                features.append(torch.zeros(self.rnn_hidden))
        
        return torch.stack(features).to(self.device)
    
    def clear_node_cache(self):
        """æ¸…ç©ºèŠ‚ç‚¹çŠ¶æ€ç¼“å­˜"""
        self._node_state_cache.clear()
        self.logger.debug("èŠ‚ç‚¹çŠ¶æ€ç¼“å­˜å·²æ¸…ç©º")
    
    def _precompute_neighbors(self):
        """
        é¢„è®¡ç®—æ¯ä¸ªèŠ‚ç‚¹çš„ Top-K é‚»å±…
        
        é¿å…åœ¨æ¯æ¬¡ forward æ—¶é‡å¤è®¡ç®—ï¼Œæå‡æ•ˆç‡ã€‚
        """
        if self.adj_matrix is None:
            return
        
        if self._neighbor_cache is not None:
            return  # å·²ç»è®¡ç®—è¿‡
        
        self.logger.info(f"é¢„è®¡ç®— Top-{self.max_neighbors} é‚»å±…ç´¢å¼•...")
        self._neighbor_cache = {}
        
        N = self.adj_matrix.size(0)
        for i in range(N):
            # è·å–ç¬¬ i ä¸ªèŠ‚ç‚¹çš„æ‰€æœ‰è¾¹æƒé‡
            weights = self.adj_matrix[i]
            # æ’é™¤è‡ªç¯
            weights[i] = 0
            # Top-K
            k = min(self.max_neighbors, (weights > 0).sum().item())
            if k > 0:
                _, topk_idx = torch.topk(weights, k)
                self._neighbor_cache[i] = topk_idx
            else:
                self._neighbor_cache[i] = torch.tensor([], dtype=torch.long, device=self.device)
        
        self.logger.info(f"é‚»å±…ç´¢å¼•é¢„è®¡ç®—å®Œæˆï¼Œå…± {N} ä¸ªèŠ‚ç‚¹")
    
    # ==================== ğŸ†• Unified Forward Step ====================
    
    def _forward_step(
        self,
        batch_x: torch.Tensor,
        stock_idx: Optional[torch.Tensor],
        batch_funda: Optional[torch.Tensor],
        mode: Optional[str] = None,
        update_cache: bool = True
    ) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        ç»Ÿä¸€çš„å‰å‘ä¼ æ’­æ­¥éª¤
        
        å°† temporal -> graph -> fusion çš„æµç¨‹å°è£…ï¼Œä¾›è®­ç»ƒã€éªŒè¯ã€é¢„æµ‹å…±ç”¨ã€‚
        è§£å†³è®­ç»ƒä¸æ¨ç†è®¡ç®—å›¾ä¸ä¸€è‡´çš„é—®é¢˜ã€‚
        
        Args:
            batch_x: [batch_size, seq_len, d_feat] è¾“å…¥ç‰¹å¾
            stock_idx: [batch_size] è‚¡ç¥¨å…¨å±€ç´¢å¼•ï¼ˆå¯é€‰ï¼‰
            batch_funda: [batch_size, funda_dim] åŸºæœ¬é¢æ•°æ®ï¼ˆå¯é€‰ï¼‰
            mode: å›¾æ¨ç†æ¨¡å¼ï¼ŒNone æ—¶ä½¿ç”¨é»˜è®¤é…ç½®
            update_cache: æ˜¯å¦æ›´æ–°èŠ‚ç‚¹ç¼“å­˜
            
        Returns:
            (pred, time_feat)
            - pred: [batch_size] é¢„æµ‹ç»“æœ
            - time_feat: [batch_size, hidden] æ—¶åºç‰¹å¾ï¼ˆç”¨äºç¼“å­˜ï¼‰
        """
        mode = mode or self.graph_inference_mode
        batch_size = batch_x.size(0)
        
        # Step 1: æå–æ—¶åºç‰¹å¾
        time_feat = self.model.temporal(batch_x)  # [batch, hidden]
        
        # Step 2: æ›´æ–°èŠ‚ç‚¹ç¼“å­˜ï¼ˆç”¨äºåç»­ batch çš„é‚»å±…é‡‡æ ·ï¼‰
        if update_cache and stock_idx is not None:
            self._update_node_cache(stock_idx, time_feat.detach())
        
        # Step 3: å‡†å¤‡å›¾ä¸Šä¸‹æ–‡å¹¶è¿è¡Œ GAT
        if self.use_graph:
            augmented_x, augmented_adj, original_batch_size = self._prepare_graph_context(
                time_feat, stock_idx, mode
            )
            
            # ğŸ†• å¤„ç†åŸºæœ¬é¢æ•°æ®ç»´åº¦é—®é¢˜ï¼šå¯¹å¤–éƒ¨é‚»å±…å¡«å……é›¶
            if batch_funda is not None:
                num_external = augmented_x.size(0) - original_batch_size
                if num_external > 0:
                    # ä¸ºå¤–éƒ¨é‚»å±…åˆ›å»ºé›¶å¡«å……çš„åŸºæœ¬é¢æ•°æ®
                    external_funda = torch.zeros(
                        num_external, batch_funda.size(-1),
                        device=batch_funda.device, dtype=batch_funda.dtype
                    )
                    augmented_funda = torch.cat([batch_funda, external_funda], dim=0)
                    graph_input = torch.cat([augmented_x, augmented_funda], dim=-1)
                else:
                    graph_input = torch.cat([augmented_x, batch_funda], dim=-1)
            else:
                graph_input = augmented_x
            
            # è¿è¡Œ GAT
            graph_feat = self.model.gat(graph_input, augmented_adj)
            
            # åªå–åŸå§‹ batch çš„ GAT è¾“å‡º
            graph_feat = graph_feat[:original_batch_size]
            time_feat_for_fusion = time_feat[:original_batch_size]
            
            # Step 4: èåˆç‰¹å¾
            if batch_funda is not None:
                combined = torch.cat([time_feat_for_fusion, graph_feat, batch_funda], dim=-1)
            else:
                combined = torch.cat([time_feat_for_fusion, graph_feat], dim=-1)
        else:
            # ä¸ä½¿ç”¨å›¾ç½‘ç»œ
            if batch_funda is not None:
                combined = torch.cat([time_feat, batch_funda], dim=-1)
            else:
                combined = time_feat
        
        # Step 5: MLP é¢„æµ‹
        pred = self.model.fusion(combined)
        
        # ğŸ†• å¤„ç†é¢„æµ‹ç»“æœç»´åº¦
        # - å¤šå› å­æ¨¡å¼ (output_dim > 1): ä¿æŒ [batch, F] å½¢çŠ¶
        # - å•å› å­æ¨¡å¼ (output_dim == 1): squeeze ä¸º [batch]
        if pred.dim() == 2 and pred.size(-1) == 1:
            # å•å› å­ï¼Œsqueeze æœ€åä¸€ç»´
            pred = pred.squeeze(-1)
        elif pred.dim() == 0:
            pred = pred.unsqueeze(0)
        # å¤šå› å­æ—¶ä¿æŒ [batch, F] ä¸å˜
        
        # ğŸ†• è¿”å›èåˆç‰¹å¾ç”¨äºç›¸å…³æ€§æ­£åˆ™åŒ–
        # combined æ˜¯è¿›å…¥æœ€ç»ˆè¾“å‡ºå±‚ä¹‹å‰çš„ç‰¹å¾å‘é‡
        return pred, time_feat, combined
    
    # ==================== ğŸ†• Graph Context Preparation ====================
    
    def _prepare_graph_context(
        self,
        x_temporal: torch.Tensor,
        stock_idx: Optional[torch.Tensor],
        mode: Optional[str] = None
    ) -> Tuple[torch.Tensor, torch.Tensor, int]:
        """
        å‡†å¤‡å›¾ä¸Šä¸‹æ–‡ï¼ˆç‰¹å¾ + é‚»æ¥çŸ©é˜µï¼‰
        
        æ ¹æ®ä¸åŒçš„æ¨ç†æ¨¡å¼ï¼Œæ„å»ºé€‚åˆ GAT çš„è¾“å…¥ï¼š
        - 'batch': ç›´æ¥åˆ‡ç‰‡ï¼ˆå¯èƒ½ä¸¢å¤±é‚»å±…ä¿¡æ¯ï¼‰
        - 'neighbor_sampling': ä¸»åŠ¨é‡‡æ ·é‚»å±…ï¼Œä»ç¼“å­˜è¡¥å…¨ç‰¹å¾
        
        Args:
            x_temporal: [batch_size, hidden_dim] å½“å‰ batch çš„æ—¶åºç‰¹å¾
            stock_idx: [batch_size] è‚¡ç¥¨å…¨å±€ç´¢å¼•
            mode: æ¨ç†æ¨¡å¼ï¼ˆNone æ—¶ä½¿ç”¨é»˜è®¤é…ç½®ï¼‰
            
        Returns:
            (augmented_x, augmented_adj, batch_size)
            - augmented_x: [batch_size + num_external, hidden_dim]
            - augmented_adj: [batch_size + num_external, batch_size + num_external]
            - batch_size: åŸå§‹ batch å¤§å°ï¼ˆç”¨äºåç»­åˆ‡ç‰‡ï¼‰
        """
        mode = mode or self.graph_inference_mode
        batch_size = x_temporal.size(0)
        
        # å¦‚æœæ²¡æœ‰é‚»æ¥çŸ©é˜µæˆ–æ²¡æœ‰è‚¡ç¥¨ç´¢å¼•ï¼Œç›´æ¥è¿”å›å•ä½çŸ©é˜µ
        if self.adj_matrix is None or stock_idx is None:
            return x_temporal, torch.eye(batch_size, device=self.device), batch_size
        
        stock_idx = stock_idx.to(self.device)
        
        # ========== Mode: batch (åŸå§‹æ¨¡å¼) ==========
        if mode == GRAPH_MODE_BATCH:
            # ç›´æ¥åˆ‡ç‰‡é‚»æ¥çŸ©é˜µ
            current_adj = self.adj_matrix[stock_idx][:, stock_idx]
            return x_temporal, current_adj, batch_size
        
        # ========== Mode: neighbor_sampling ==========
        elif mode == GRAPH_MODE_NEIGHBOR_SAMPLING:
            # ç¡®ä¿å·²é¢„è®¡ç®—é‚»å±…
            if self._neighbor_cache is None:
                self._precompute_neighbors()
            
            # 1. æ”¶é›†æ‰€æœ‰éœ€è¦çš„é‚»å±…
            batch_set = set(stock_idx.cpu().tolist())
            external_neighbors = set()
            
            for idx in stock_idx.cpu().tolist():
                if idx in self._neighbor_cache:
                    neighbors = self._neighbor_cache[idx].cpu().tolist()
                    for n in neighbors:
                        if n not in batch_set:
                            external_neighbors.add(n)
            
            # 2. å¦‚æœæ²¡æœ‰å¤–éƒ¨é‚»å±…ï¼Œé€€åŒ–ä¸º batch æ¨¡å¼
            if len(external_neighbors) == 0:
                current_adj = self.adj_matrix[stock_idx][:, stock_idx]
                return x_temporal, current_adj, batch_size
            
            # 3. è·å–å¤–éƒ¨é‚»å±…çš„ç‰¹å¾
            external_idx = torch.tensor(list(external_neighbors), dtype=torch.long, device=self.device)
            
            # è®¡ç®— fallback ç‰¹å¾ï¼ˆå½“å‰ batch çš„å‡å€¼ï¼‰
            fallback_feat = x_temporal.mean(dim=0)
            external_features = self._fetch_cached_features(external_idx, fallback_feat)
            
            # 4. æ‹¼æ¥ç‰¹å¾: [batch, hidden] + [external, hidden] -> [batch+external, hidden]
            augmented_x = torch.cat([x_temporal, external_features], dim=0)
            
            # 5. æ„å»ºæ‰©å±•é‚»æ¥çŸ©é˜µ
            # ç´¢å¼•é¡ºåº: [batch_nodes..., external_nodes...]
            all_idx = torch.cat([stock_idx, external_idx], dim=0)
            augmented_adj = self.adj_matrix[all_idx][:, all_idx]
            
            return augmented_x, augmented_adj, batch_size
        
        # ========== Mode: cross_sectional ==========
        elif mode == GRAPH_MODE_CROSS_SECTIONAL:
            # æˆªé¢æ¨¡å¼ï¼šå‡è®¾ batch å·²ç»æ˜¯åŒä¸€å¤©çš„æ‰€æœ‰è‚¡ç¥¨
            # å¦‚æœ batch_size æ¥è¿‘æ€»è‚¡ç¥¨æ•°ï¼Œç›´æ¥ç”¨å…¨å›¾ï¼›å¦åˆ™åˆ‡ç‰‡
            if batch_size > self.adj_matrix.size(0) * 0.8:
                # æ¥è¿‘å…¨é‡ï¼Œä½¿ç”¨å…¨å›¾
                return x_temporal, self.adj_matrix[:batch_size, :batch_size], batch_size
            else:
                # å¦åˆ™åˆ‡ç‰‡
                current_adj = self.adj_matrix[stock_idx][:, stock_idx]
                return x_temporal, current_adj, batch_size
        
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„å›¾æ¨ç†æ¨¡å¼: {mode}")
    
    def _load_adj_matrix(
        self,
        path: Optional[str]
    ) -> Optional[torch.Tensor]:
        """
        åŠ è½½é‚»æ¥çŸ©é˜µ
        
        ğŸ†• æ”¯æŒä¸¤ç§ä¿å­˜æ ¼å¼ï¼š
        1. çº¯ Tensorï¼šç›´æ¥åŠ è½½
        2. åŒ…å«å…ƒæ•°æ®çš„å­—å…¸ï¼šæå– 'adj_matrix' é”®
        
        Args:
            path: é‚»æ¥çŸ©é˜µæ–‡ä»¶è·¯å¾„ (.pt, .pth, æˆ– .npy)
            
        Returns:
            é‚»æ¥çŸ©é˜µ Tensor æˆ– None
        """
        if path is None or not self.use_graph:
            return None
        
        path = Path(path)
        if not path.exists():
            self.logger.warning(f"é‚»æ¥çŸ©é˜µæ–‡ä»¶ä¸å­˜åœ¨: {path}")
            return None
        
        try:
            if path.suffix in ['.pt', '.pth']:
                data = torch.load(path, map_location=self.device)
                # ğŸ†• æ”¯æŒå­—å…¸æ ¼å¼ï¼ˆbuild_industry_adj.py ä¿å­˜çš„æ ¼å¼ï¼‰
                if isinstance(data, dict):
                    adj = data['adj_matrix']
                    # ä¿å­˜å…ƒæ•°æ®ä¾›åç»­ä½¿ç”¨
                    self._adj_stock_list = data.get('stock_list', None)
                    self._adj_stock_to_idx = data.get('stock_to_idx', None)
                    n_industries = data.get('n_industries', 'N/A')
                    self.logger.info(f"  é‚»æ¥çŸ©é˜µå…ƒæ•°æ®: {len(self._adj_stock_list) if self._adj_stock_list else 0} åªè‚¡ç¥¨, {n_industries} ä¸ªè¡Œä¸š")
                else:
                    # å…¼å®¹çº¯ Tensor æ ¼å¼
                    adj = data
                # ç¡®ä¿åœ¨æ­£ç¡®è®¾å¤‡ä¸Š
                adj = adj.to(self.device)
            elif path.suffix == '.npy':
                adj = torch.from_numpy(np.load(path)).float().to(self.device)
            else:
                raise ValueError(f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {path.suffix}")
            
            self.logger.info(f"å·²åŠ è½½é‚»æ¥çŸ©é˜µ: {path}, shape={adj.shape}")
            return adj
        
        except Exception as e:
            self.logger.error(f"åŠ è½½é‚»æ¥çŸ©é˜µå¤±è´¥: {e}")
            return None
    
    def _train_epoch(self, train_loader):
        """
        è®­ç»ƒä¸€ä¸ª epoch (ğŸ†• ä½¿ç”¨ç»Ÿä¸€çš„å‰å‘ä¼ æ’­æµç¨‹)
        
        Args:
            train_loader: è®­ç»ƒæ•°æ®åŠ è½½å™¨
            
        Returns:
            å¹³å‡è®­ç»ƒæŸå¤±
            
        Note:
            ğŸ†• ç°åœ¨ä½¿ç”¨ _forward_step ç»Ÿä¸€å‰å‘ä¼ æ’­ï¼Œç¡®ä¿è®­ç»ƒå’Œæ¨ç†
            ä½¿ç”¨ç›¸åŒçš„è®¡ç®—å›¾ï¼ˆåŒ…æ‹¬é‚»å±…é‡‡æ ·é€»è¾‘ï¼‰ï¼Œé¿å…åˆ†å¸ƒåç§»ã€‚
        """
        self.model.train()
        total_loss = 0
        n_batches = 0
        
        for batch_data in tqdm(train_loader, desc="è®­ç»ƒ", leave=False):
            # è§£æ batch æ•°æ®
            batch_x, stock_idx, batch_funda = self._parse_batch_data(batch_data)
            batch_y = batch_data[1]
            
            batch_x = batch_x.to(self.device)
            batch_y = batch_y.to(self.device)
            if batch_funda is not None:
                batch_funda = batch_funda.to(self.device)

            # ğŸ†• ä½¿ç”¨ç»Ÿä¸€çš„å‰å‘ä¼ æ’­ï¼ˆè®­ç»ƒæ—¶ä¹Ÿä½¿ç”¨é‚»å±…é‡‡æ ·ï¼Œä¿æŒä¸€è‡´æ€§ï¼‰
            self.optimizer.zero_grad()
            pred, _, hidden_features = self._forward_step(
                batch_x, stock_idx, batch_funda,
                mode=self.graph_inference_mode,
                update_cache=True  # è®­ç»ƒæ—¶æ›´æ–°ç¼“å­˜
            )
            
            # ğŸ†• å¤šå› å­è¾“å‡ºæ”¯æŒï¼šæ ‡ç­¾å¹¿æ’­ç­–ç•¥
            # å¦‚æœ pred æ˜¯ [batch, F]ï¼Œåˆ™å°† batch_y å¹¿æ’­ä¸º [batch, F]
            # è®©æ¯ä¸ªå› å­ç‹¬ç«‹æ‹ŸåˆåŒä¸€ä¸ª alpha æ ‡ç­¾
            is_multifactor = pred.dim() == 2 and pred.size(1) > 1
            if is_multifactor:
                batch_y_expanded = batch_y.unsqueeze(1).expand_as(pred)
                target_for_loss = batch_y_expanded
            else:
                target_for_loss = batch_y
            
            # ğŸ†• æ ¹æ®æ ‡å¿—ä½å†³å®šæ˜¯å¦ä¼ é€’ hidden_features
            # ğŸ”§ å¤šå› å­æ¨¡å¼ä¸‹å¯¹è¾“å‡ºå› å­åšæ­£äº¤åŒ–ï¼Œå•å› å­æ¨¡å¼ä¸‹å¯¹éšè—ç‰¹å¾åšæ­£äº¤åŒ–
            if self._use_corr_loss:
                corr_target = pred if is_multifactor else hidden_features
                loss = self.criterion(pred, target_for_loss, corr_target)
            else:
                loss = self.criterion(pred, target_for_loss)
            
            # åå‘ä¼ æ’­
            loss.backward()
            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
            self.optimizer.step()
            
            total_loss += loss.item()
            n_batches += 1
        
        return total_loss / n_batches if n_batches > 0 else 0
    
    def _valid_epoch(self, valid_loader):
        """
        éªŒè¯ä¸€ä¸ª epoch (ğŸ†• ä½¿ç”¨ç»Ÿä¸€çš„å‰å‘ä¼ æ’­æµç¨‹)
        
        Args:
            valid_loader: éªŒè¯æ•°æ®åŠ è½½å™¨
            
        Returns:
            å¹³å‡éªŒè¯æŸå¤±
        """
        self.model.eval()
        total_loss = 0
        n_batches = 0
        
        with torch.no_grad():
            for batch_data in valid_loader:
                # è§£æ batch æ•°æ®
                batch_x, stock_idx, batch_funda = self._parse_batch_data(batch_data)
                batch_y = batch_data[1]
                
                batch_x = batch_x.to(self.device)
                batch_y = batch_y.to(self.device)
                if batch_funda is not None:
                    batch_funda = batch_funda.to(self.device)
                
                # ğŸ†• ä½¿ç”¨ç»Ÿä¸€çš„å‰å‘ä¼ æ’­
                pred, _, hidden_features = self._forward_step(
                    batch_x, stock_idx, batch_funda,
                    mode=self.graph_inference_mode,
                    update_cache=True  # éªŒè¯æ—¶ä¹Ÿæ›´æ–°ç¼“å­˜ï¼Œä¸ºæ¨ç†é¢„çƒ­
                )
                
                # ğŸ†• å¤šå› å­è¾“å‡ºæ”¯æŒï¼šæ ‡ç­¾å¹¿æ’­ç­–ç•¥
                is_multifactor = pred.dim() == 2 and pred.size(1) > 1
                if is_multifactor:
                    batch_y_expanded = batch_y.unsqueeze(1).expand_as(pred)
                    target_for_loss = batch_y_expanded
                else:
                    target_for_loss = batch_y
                
                # ğŸ†• æ ¹æ®æ ‡å¿—ä½å†³å®šæ˜¯å¦ä¼ é€’ hidden_features
                # ğŸ”§ å¤šå› å­æ¨¡å¼ä¸‹å¯¹è¾“å‡ºå› å­åšæ­£äº¤åŒ–ï¼Œå•å› å­æ¨¡å¼ä¸‹å¯¹éšè—ç‰¹å¾åšæ­£äº¤åŒ–
                if self._use_corr_loss:
                    corr_target = pred if is_multifactor else hidden_features
                    loss = self.criterion(pred, target_for_loss, corr_target)
                else:
                    loss = self.criterion(pred, target_for_loss)
                    
                total_loss += loss.item()
                n_batches += 1
        
        return total_loss / n_batches if n_batches > 0 else 0
    
    def fit(self, train_loader, valid_loader=None, save_path: Optional[str] = None):
        """
        è®­ç»ƒæ¨¡å‹
        
        Args:
            train_loader: è®­ç»ƒæ•°æ®åŠ è½½å™¨
            valid_loader: éªŒè¯æ•°æ®åŠ è½½å™¨
            save_path: æ¨¡å‹ä¿å­˜è·¯å¾„
        """
        self.logger.info("å¼€å§‹è®­ç»ƒ HybridGraph æ¨¡å‹...")
        
        # ğŸ†• åˆ›å»ºå­¦ä¹ ç‡è°ƒåº¦å™¨
        self.scheduler = self._get_scheduler()
        
        best_valid_loss = float('inf')
        patience_counter = 0
        
        for epoch in range(self.n_epochs):
            train_loss = self._train_epoch(train_loader)
            self.train_losses.append(train_loss)
            
            if valid_loader is not None:
                valid_loss = self._valid_epoch(valid_loader)
                self.valid_losses.append(valid_loss)
                
                # ğŸ†• è·å–å½“å‰å­¦ä¹ ç‡ç”¨äºæ˜¾ç¤º
                current_lr = self.optimizer.param_groups[0]['lr']
                
                self.logger.info(
                    f"Epoch {epoch+1}/{self.n_epochs} - "
                    f"Train Loss: {train_loss:.6f}, Valid Loss: {valid_loss:.6f}, "
                    f"LR: {current_lr:.2e}"
                )
                
                # ğŸ†• æ›´æ–°å­¦ä¹ ç‡è°ƒåº¦å™¨
                self._step_scheduler(valid_loss)
                
                if valid_loss < best_valid_loss:
                    best_valid_loss = valid_loss
                    self.best_score = -valid_loss
                    self.best_epoch = epoch
                    patience_counter = 0
                    
                    if save_path:
                        self.save_model(save_path)
                else:
                    patience_counter += 1
                    if patience_counter >= self.early_stop:
                        self.logger.info(f"æ—©åœè§¦å‘ at epoch {epoch+1}")
                        break
            else:
                self.logger.info(f"Epoch {epoch+1}/{self.n_epochs} - Train Loss: {train_loss:.6f}")
                # ğŸ†• å³ä½¿æ²¡æœ‰éªŒè¯é›†ï¼Œä¹Ÿæ›´æ–°è°ƒåº¦å™¨ï¼ˆä½¿ç”¨è®­ç»ƒæŸå¤±ï¼‰
                self._step_scheduler(train_loss)
        
        self.fitted = True
        self.logger.info(f"è®­ç»ƒå®Œæˆ! æœ€ä½³ epoch: {self.best_epoch+1}")
        
        # ğŸ†• æ˜¾ç¤ºå­¦ä¹ ç‡å˜åŒ–ç»Ÿè®¡
        if self.lr_history:
            self.logger.info(f"  åˆå§‹å­¦ä¹ ç‡: {self.lr_history[0]:.2e}")
            self.logger.info(f"  æœ€ç»ˆå­¦ä¹ ç‡: {self.lr_history[-1]:.2e}")
            if self.lr_history[0] != self.lr_history[-1]:
                self.logger.info(f"  å­¦ä¹ ç‡è°ƒæ•´æ¬¡æ•°: {sum(1 for i in range(1, len(self.lr_history)) if self.lr_history[i] != self.lr_history[i-1])}")
    
    def predict(
        self, 
        test_loader, 
        return_numpy: bool = True,
        graph_inference_mode: Optional[str] = None,
        warm_up_cache: bool = False
    ):
        """
        é¢„æµ‹ï¼ˆğŸ†• æ”¯æŒ Graph-Aware Inferenceï¼‰
        
        Args:
            test_loader: æµ‹è¯•æ•°æ®åŠ è½½å™¨
            return_numpy: æ˜¯å¦è¿”å› numpy æ•°ç»„
            graph_inference_mode: å›¾æ¨ç†æ¨¡å¼ï¼Œå¯è¦†ç›–é»˜è®¤é…ç½®
                - 'batch': ç›´æ¥æŒ‰ batch åˆ‡ç‰‡é‚»æ¥çŸ©é˜µï¼ˆé»˜è®¤ï¼‰
                - 'cross_sectional': å‡è®¾æ¯ä¸ª batch æ˜¯åŒä¸€å¤©çš„æ‰€æœ‰è‚¡ç¥¨
                - 'neighbor_sampling': ä¸»åŠ¨é‡‡æ ·é‚»å±…ï¼Œä»ç¼“å­˜è¡¥å…¨ç‰¹å¾
            warm_up_cache: æ˜¯å¦åœ¨é¢„æµ‹å‰é¢„çƒ­ç¼“å­˜ï¼ˆè·‘ä¸€é loader ä½†ä¸äº§å‡ºç»“æœï¼‰
            
        Returns:
            é¢„æµ‹ç»“æœ
            
        Note:
            ä½¿ç”¨ 'neighbor_sampling' æ¨¡å¼æ—¶ï¼Œå»ºè®®åœ¨è®­ç»ƒåä¿æŒç¼“å­˜ï¼Œ
            æˆ–åœ¨é¢„æµ‹å‰è°ƒç”¨ warm_up_cache=True è¿›è¡Œé¢„çƒ­ã€‚
        """
        if not self.fitted:
            raise ValueError("æ¨¡å‹æœªè®­ç»ƒ,è¯·å…ˆè°ƒç”¨ fit()")
        
        # ç¡®å®šæ¨ç†æ¨¡å¼
        mode = graph_inference_mode or self.graph_inference_mode
        
        # ğŸ†• é‚»å±…é‡‡æ ·æ¨¡å¼é¢„çƒ­
        if warm_up_cache and mode == GRAPH_MODE_NEIGHBOR_SAMPLING:
            self.logger.info("ğŸ”¥ é¢„çƒ­èŠ‚ç‚¹çŠ¶æ€ç¼“å­˜...")
            self._warm_up_cache(test_loader)
        
        self.model.eval()
        predictions = []
        
        with torch.no_grad():
            for batch_data in test_loader:
                # è§£æ batch æ•°æ®
                batch_x, stock_idx, batch_funda = self._parse_batch_data(batch_data)
                batch_x = batch_x.to(self.device)
                if batch_funda is not None:
                    batch_funda = batch_funda.to(self.device)
                
                # ğŸ†• ä½¿ç”¨ç»Ÿä¸€çš„å‰å‘ä¼ æ’­
                pred, _, _ = self._forward_step(
                    batch_x, stock_idx, batch_funda,
                    mode=mode,
                    update_cache=True  # é¢„æµ‹æ—¶ä¹Ÿæ›´æ–°ç¼“å­˜ï¼Œæ”¯æŒæµå¼æ¨ç†
                )
                
                predictions.append(pred.cpu())
        
        # å¤„ç†ç©ºé¢„æµ‹åˆ—è¡¨
        if len(predictions) == 0:
            return np.array([]) if return_numpy else torch.tensor([])
        
        # ğŸ†• å¤šå› å­è¾“å‡ºæ”¯æŒï¼šä¿ç•™ [N, F] ç»“æ„
        # æ£€æŸ¥ç¬¬ä¸€ä¸ª batch çš„ç»´åº¦æ¥ç¡®å®šè¾“å‡ºæ ¼å¼
        first_pred = predictions[0]
        is_multi_factor = first_pred.dim() == 2 and first_pred.size(-1) > 1
        
        if is_multi_factor:
            # å¤šå› å­æ¨¡å¼ï¼šæ‹¼æ¥ä¸º [N, F] çŸ©é˜µ
            predictions = torch.cat(predictions, dim=0)  # [N, F]
            if return_numpy:
                return predictions.numpy()  # è¿”å› (N, F) çš„ 2D æ•°ç»„
            return predictions
        else:
            # å•å› å­æ¨¡å¼ï¼šä¿æŒåŸæœ‰è¡Œä¸ºï¼Œè¿”å› 1D æ•°ç»„
            predictions = [p.flatten() if p.dim() > 1 else p for p in predictions]
            predictions = torch.cat(predictions, dim=0)
            if return_numpy:
                return predictions.numpy().flatten()
            return predictions
    
    def _parse_batch_data(self, batch_data) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[torch.Tensor]]:
        """
        è§£æ batch æ•°æ®
        
        Args:
            batch_data: DataLoader è¿”å›çš„ batchï¼ˆå…ƒç»„æˆ–å­—å…¸ï¼‰
            
        Returns:
            (batch_x, stock_idx, batch_funda)
            
        Batch æ ¼å¼æ”¯æŒ:
            1. å…ƒç»„æ ¼å¼ï¼ˆæ¨èä½¿ç”¨ stock_idx_position æ˜ç¡®æŒ‡å®šï¼‰:
               - (x, y): æ— ç´¢å¼•
               - (x, y, stock_idx): stock_idx_position=2
               - (x, y, date_idx, stock_idx): stock_idx_position=3
               - (x, y, stock_idx, funda): stock_idx_position=2, funda_position=3
               
            2. å­—å…¸æ ¼å¼ï¼ˆè‡ªåŠ¨è¯†åˆ« keyï¼‰:
               - batch['x'], batch['stock_idx'], batch['funda']
               
        Note:
            ğŸ†• ä¸å†ä½¿ç”¨æ•°å€¼èŒƒå›´å¯å‘å¼çŒœæµ‹ï¼Œè€Œæ˜¯é€šè¿‡æ˜ç¡®çš„ä½ç½®å‚æ•°
            æˆ–å­—å…¸ key æ¥è¯†åˆ«è‚¡ç¥¨ç´¢å¼•ï¼Œé¿å… date_idx è¢«è¯¯è®¤ä¸º stock_idxã€‚
        """
        # ========== å­—å…¸æ ¼å¼ ==========
        if isinstance(batch_data, dict):
            batch_x = batch_data.get('x') or batch_data.get('features') or batch_data.get('input')
            if batch_x is None:
                raise ValueError("å­—å…¸æ ¼å¼çš„ batch å¿…é¡»åŒ…å« 'x', 'features' æˆ– 'input' é”®")
            
            # æŸ¥æ‰¾è‚¡ç¥¨ç´¢å¼•ï¼ˆæ”¯æŒå¤šç§å‘½åï¼‰
            stock_idx = (
                batch_data.get('stock_idx') or 
                batch_data.get('instrument') or 
                batch_data.get('code_idx') or
                batch_data.get('symbol_idx')
            )
            
            # æŸ¥æ‰¾åŸºæœ¬é¢æ•°æ®
            batch_funda = (
                batch_data.get('funda') or 
                batch_data.get('fundamental') or
                batch_data.get('static_features')
            )
            
            return batch_x, stock_idx, batch_funda
        
        # ========== å…ƒç»„æ ¼å¼ ==========
        if not isinstance(batch_data, (tuple, list)):
            raise ValueError(f"ä¸æ”¯æŒçš„ batch ç±»å‹: {type(batch_data)}")
        
        if len(batch_data) < 2:
            raise ValueError(f"batch å…ƒç´ æ•°é‡ä¸è¶³: {len(batch_data)}ï¼Œè‡³å°‘éœ€è¦ (x, y)")
        
        batch_x = batch_data[0]
        batch_funda = None
        stock_idx = None
        
        # æ–¹å¼ 1: ç”¨æˆ·æ˜ç¡®æŒ‡å®šäº†ä½ç½®
        if self.stock_idx_position is not None:
            if self.stock_idx_position < len(batch_data):
                candidate = batch_data[self.stock_idx_position]
                if torch.is_tensor(candidate) and candidate.dtype == torch.long:
                    stock_idx = self._validate_stock_idx_range(candidate)
                else:
                    self.logger.warning(
                        f"batch[{self.stock_idx_position}] ä¸æ˜¯ long ç±»å‹å¼ é‡ï¼Œè·³è¿‡"
                    )
        
        if self.funda_position is not None:
            if self.funda_position < len(batch_data):
                candidate = batch_data[self.funda_position]
                if torch.is_tensor(candidate) and candidate.dtype in [torch.float, torch.float32, torch.float64]:
                    batch_funda = candidate
        
        # æ–¹å¼ 2: ç”¨æˆ·æœªæŒ‡å®šï¼Œä½¿ç”¨ä¿å®ˆçš„é»˜è®¤è§„åˆ™
        # åªæœ‰å½“ stock_idx_position æœªè®¾ç½®æ—¶æ‰å°è¯•è‡ªåŠ¨æ¨æ–­
        if self.stock_idx_position is None and stock_idx is None:
            # ä¿å®ˆç­–ç•¥ï¼šåªåœ¨ batch é•¿åº¦ >= 4 æ—¶ï¼Œå‡è®¾ç¬¬ 4 ä¸ªå…ƒç´ æ˜¯ stock_idx
            # è¿™æ˜¯å› ä¸ºå¸¸è§æ ¼å¼ä¸º (x, y, date_idx, stock_idx)
            if len(batch_data) >= 4:
                candidate = batch_data[3]
                if torch.is_tensor(candidate) and candidate.dtype == torch.long:
                    stock_idx = self._validate_stock_idx_range(candidate)
                    if stock_idx is not None:
                        self.logger.debug(
                            f"è‡ªåŠ¨æ£€æµ‹åˆ° stock_idx åœ¨ batch[3]ï¼Œå»ºè®®æ˜¾å¼è®¾ç½® stock_idx_position=3"
                        )
        
        return batch_x, stock_idx, batch_funda
    
    def _validate_stock_idx_range(self, idx_tensor: torch.Tensor) -> Optional[torch.Tensor]:
        """
        éªŒè¯è‚¡ç¥¨ç´¢å¼•æ˜¯å¦åœ¨é‚»æ¥çŸ©é˜µèŒƒå›´å†…
        
        è¿™æ˜¯ä¸€ä¸ªè¾…åŠ©éªŒè¯ï¼Œç”¨äºåœ¨ç”¨æˆ·æ˜ç¡®æŒ‡å®š stock_idx_position åï¼Œ
        è¿›ä¸€æ­¥æ£€æŸ¥ç´¢å¼•å€¼æ˜¯å¦åˆç†ã€‚
        
        Args:
            idx_tensor: å¾…éªŒè¯çš„ç´¢å¼•å¼ é‡
            
        Returns:
            æœ‰æ•ˆçš„è‚¡ç¥¨ç´¢å¼•ï¼Œæˆ– Noneï¼ˆå¦‚æœè¶…å‡ºèŒƒå›´ï¼‰
            
        Note:
            å¦‚æœç”¨æˆ·é€šè¿‡ stock_idx_position æ˜ç¡®æŒ‡å®šäº†ä½ç½®ï¼Œè¿™é‡ŒåªåšèŒƒå›´æ£€æŸ¥ï¼Œ
            ä¸ä¼šå› ä¸ºå€¼å°äº num_stocks å°±æ‹’ç»ï¼ˆé‚£æ˜¯æ—§çš„å¯å‘å¼é€»è¾‘çš„é—®é¢˜ï¼‰ã€‚
        """
        if idx_tensor is None:
            return None
        
        # å¦‚æœæ²¡æœ‰é‚»æ¥çŸ©é˜µï¼Œæ— æ³•éªŒè¯èŒƒå›´ï¼Œç›´æ¥è¿”å›
        if self.adj_matrix is None:
            return idx_tensor
        
        # æ£€æŸ¥ç´¢å¼•æ˜¯å¦è¶…å‡ºé‚»æ¥çŸ©é˜µèŒƒå›´
        max_idx = idx_tensor.max().item()
        min_idx = idx_tensor.min().item()
        num_stocks = self.adj_matrix.size(0)
        
        if max_idx >= num_stocks:
            self.logger.warning(
                f"è‚¡ç¥¨ç´¢å¼•è¶…å‡ºé‚»æ¥çŸ©é˜µèŒƒå›´: max_idx={max_idx} >= num_stocks={num_stocks}ï¼Œ"
                f"å°†å¿½ç•¥è¯¥ç´¢å¼•ã€‚è¯·æ£€æŸ¥ stock_idx_position é…ç½®æ˜¯å¦æ­£ç¡®ã€‚"
            )
            return None
        
        if min_idx < 0:
            self.logger.warning(
                f"è‚¡ç¥¨ç´¢å¼•åŒ…å«è´Ÿå€¼: min_idx={min_idx}ï¼Œå°†å¿½ç•¥è¯¥ç´¢å¼•ã€‚"
            )
            return None
        
        return idx_tensor
    
    def _warm_up_cache(self, data_loader):
        """
        é¢„çƒ­èŠ‚ç‚¹çŠ¶æ€ç¼“å­˜
        
        åœ¨æ­£å¼é¢„æµ‹å‰è·‘ä¸€éæ•°æ®ï¼Œå¡«å……ç¼“å­˜ã€‚
        
        Args:
            data_loader: æ•°æ®åŠ è½½å™¨
        """
        self.model.eval()
        with torch.no_grad():
            for batch_data in tqdm(data_loader, desc="é¢„çƒ­ç¼“å­˜", leave=False):
                batch_x, stock_idx, _ = self._parse_batch_data(batch_data)
                if stock_idx is None:
                    continue
                
                batch_x = batch_x.to(self.device)
                time_feat = self.model.temporal(batch_x)
                self._update_node_cache(stock_idx, time_feat)
        
        self.logger.info(f"ç¼“å­˜é¢„çƒ­å®Œæˆï¼Œå½“å‰ç¼“å­˜å¤§å°: {len(self._node_state_cache)}")
    
    @classmethod
    def from_config(cls, config, d_feat: int = None):
        """
        ä» CompositeModelConfig åˆ›å»ºæ¨¡å‹å®ä¾‹
        
        Args:
            config: CompositeModelConfig å¯¹è±¡
            d_feat: è¾“å…¥ç‰¹å¾ç»´åº¦ï¼ˆå¯é€‰ï¼Œè¦†ç›–é…ç½®ä¸­çš„å€¼ï¼‰
            
        Returns:
            HybridGraphModel å®ä¾‹
        """
        # ä¼˜å…ˆä½¿ç”¨ä¼ å…¥çš„ d_featï¼Œå¦åˆ™ä½¿ç”¨é…ç½®ä¸­çš„
        input_dim = d_feat if d_feat is not None else config.d_feat
        
        # æå–æ—¶åºæ¨¡å—é…ç½®
        rnn_type = 'lstm'
        rnn_hidden = 64
        rnn_layers = 2
        use_attention = True
        dropout = 0.3
        
        if config.temporal:
            rnn_type = config.temporal.rnn_type
            rnn_hidden = config.temporal.hidden_size
            rnn_layers = config.temporal.num_layers
            use_attention = config.temporal.use_attention
            dropout = config.temporal.dropout
            
        # æå–å›¾æ¨¡å—é…ç½®
        use_graph = False
        gat_type = 'standard'
        gat_hidden = 32
        gat_heads = 4
        top_k_neighbors = 10
        
        if config.graph and config.graph.enabled:
            use_graph = True
            gat_type = config.graph.gat_type
            gat_hidden = config.graph.hidden_dim
            gat_heads = config.graph.heads
            top_k_neighbors = config.graph.top_k_neighbors
            
        # æå–èåˆæ¨¡å—é…ç½®
        mlp_hidden_sizes = [64]
        output_dim = 1  # é»˜è®¤å•å› å­è¾“å‡º
        if config.fusion:
            mlp_hidden_sizes = config.fusion.hidden_sizes
            output_dim = getattr(config.fusion, 'output_dim', 1)  # ğŸ†• æå–å¤šå› å­è¾“å‡ºç»´åº¦
            
        # ğŸ†• æå–æ®‹å·®è¿æ¥é…ç½®
        use_residual = getattr(config, 'use_residual', True)
        
        # ğŸ†• æå–å­¦ä¹ ç‡è°ƒåº¦å™¨é…ç½®
        use_scheduler = getattr(config, 'use_scheduler', True)
        scheduler_type = getattr(config, 'scheduler_type', 'plateau')
        scheduler_patience = getattr(config, 'scheduler_patience', 5)
        scheduler_factor = getattr(config, 'scheduler_factor', 0.5)
        scheduler_min_lr = getattr(config, 'scheduler_min_lr', 1e-6)
        
        # ğŸ†• æå– Graph-Aware Inference é…ç½®
        graph_inference_mode = getattr(config, 'graph_inference_mode', 'batch')
        max_neighbors = getattr(config, 'max_neighbors', 10)
        cache_size = getattr(config, 'cache_size', 5000)
        
        # ğŸ†• æå–ç›¸å…³æ€§æ­£åˆ™åŒ–é…ç½®
        lambda_corr = getattr(config, 'lambda_corr', 0.0)
        
        return cls(
            d_feat=input_dim,
            rnn_hidden=rnn_hidden,
            rnn_layers=rnn_layers,
            rnn_type=rnn_type,
            use_attention=use_attention,
            use_graph=use_graph,
            gat_type=gat_type,
            gat_hidden=gat_hidden,
            gat_heads=gat_heads,
            top_k_neighbors=top_k_neighbors,
            mlp_hidden_sizes=mlp_hidden_sizes,
            funda_dim=config.funda_dim,
            dropout=dropout,
            adj_matrix_path=config.adj_matrix_path,
            output_dim=output_dim,  # ğŸ†• å¤šå› å­è¾“å‡ºç»´åº¦
            use_residual=use_residual,  # ğŸ†• æ®‹å·®è¿æ¥
            # è®­ç»ƒå‚æ•°
            n_epochs=config.n_epochs,
            batch_size=config.batch_size,
            lr=config.learning_rate,  # ğŸ†• ä¿®æ­£å‚æ•°å
            early_stop=config.early_stop,
            optimizer=config.optimizer,
            device=config.device,
            # ğŸ†• å­¦ä¹ ç‡è°ƒåº¦å™¨å‚æ•°
            use_scheduler=use_scheduler,
            scheduler_type=scheduler_type,
            scheduler_patience=scheduler_patience,
            scheduler_factor=scheduler_factor,
            scheduler_min_lr=scheduler_min_lr,
            # ğŸ†• Graph-Aware Inference å‚æ•°
            graph_inference_mode=graph_inference_mode,
            max_neighbors=max_neighbors,
            cache_size=cache_size,
            # ğŸ†• æ•°æ®æ ¼å¼å‚æ•°
            stock_idx_position=getattr(config, 'stock_idx_position', None),
            funda_position=getattr(config, 'funda_position', None),
            # ğŸ†• ç›¸å…³æ€§æ­£åˆ™åŒ–å‚æ•°
            lambda_corr=lambda_corr
        )


if __name__ == '__main__':
    print("=" * 80)
    print("Hybrid Graph Models æµ‹è¯•")
    print("=" * 80)
    
    # æµ‹è¯•å­æ¨¡å—
    print("\n1. æµ‹è¯• TemporalBlock:")
    temporal = TemporalBlock(d_feat=20, hidden_size=64, num_layers=2)
    x = torch.randn(32, 40, 20)  # [batch=32, seq=40, feat=20]
    time_feat = temporal(x)
    print(f"  è¾“å…¥: {x.shape} -> è¾“å‡º: {time_feat.shape}")
    
    print("\n2. æµ‹è¯• GraphBlock:")
    graph = GraphBlock(in_dim=64, out_dim=32, heads=4)
    node_feat = torch.randn(100, 64)  # [stocks=100, feat=64]
    adj = torch.randint(0, 2, (100, 100)).float()  # éšæœºé‚»æ¥çŸ©é˜µ
    graph_feat = graph(node_feat, adj)
    print(f"  è¾“å…¥: {node_feat.shape} + adj {adj.shape} -> è¾“å‡º: {graph_feat.shape}")
    
    print("\n3. æµ‹è¯• FusionBlock:")
    fusion = FusionBlock(input_dim=96, hidden_sizes=[64], output_dim=1)
    combined = torch.randn(32, 96)
    pred = fusion(combined)
    print(f"  è¾“å…¥: {combined.shape} -> è¾“å‡º: {pred.shape}")
    
    print("\n4. æµ‹è¯• HybridNet:")
    model = HybridNet(
        d_feat=20,
        rnn_hidden=64,
        rnn_layers=2,
        use_graph=True,
        gat_hidden=32,
        gat_heads=4,
        mlp_hidden_sizes=[64]
    )
    x = torch.randn(32, 40, 20)
    adj = torch.randint(0, 2, (32, 32)).float()
    pred = model(x, adj)
    print(f"  è¾“å…¥: {x.shape} + adj {adj.shape} -> è¾“å‡º: {pred.shape}")
    
    print("\n5. æµ‹è¯• HybridGraphModel åˆ›å»º:")
    hybrid_model = HybridGraphModel(
        d_feat=20,
        rnn_hidden=64,
        rnn_layers=2,
        use_graph=True,
        gat_hidden=32,
        gat_heads=4,
        n_epochs=10
    )
    print(f"âœ… HybridGraphModel åˆ›å»ºæˆåŠŸ")
    
    print("\n" + "=" * 80)
    print("âœ… Hybrid Graph Models æµ‹è¯•å®Œæˆ")
    print("=" * 80)
