
# QuantClassic Model Module - æ¨¡å‹æ¨¡å—

æ ‡å‡†åŒ–çš„é‡åŒ–æ¨¡å‹æ¥å£å’Œå®ç°ï¼Œå‚ç…§ Qlib è®¾è®¡ã€‚

## ğŸ“¦ æ ¸å¿ƒç»„ä»¶

```
model/
â”œâ”€â”€ base_model.py           # æ¨¡å‹åŸºç±»
â”œâ”€â”€ model_factory.py        # æ¨¡å‹å·¥å‚å’Œæ³¨å†Œæœºåˆ¶
â”œâ”€â”€ pytorch_models.py       # PyTorch æ¨¡å‹å®ç°
â”œâ”€â”€ example_usage.py        # å®Œæ•´ä½¿ç”¨ç¤ºä¾‹
â””â”€â”€ README.md              # æœ¬æ–‡ä»¶
```

## âœ¨ æ ¸å¿ƒç‰¹æ€§

### ğŸ¯ ç»Ÿä¸€æ¥å£
- **æ ‡å‡†åŒ–**: æ‰€æœ‰æ¨¡å‹ç»§æ‰¿è‡ª `Model` åŸºç±»
- **ä¸€è‡´æ€§**: ç»Ÿä¸€çš„ `fit()` å’Œ `predict()` æ¥å£
- **å…¼å®¹æ€§**: ä¸ Qlib è®¾è®¡ç†å¿µä¸€è‡´

### ğŸ­ å·¥å‚æ¨¡å¼
- **åŠ¨æ€åˆ›å»º**: é€šè¿‡é…ç½®å­—å…¸åˆ›å»ºæ¨¡å‹
- **æ³¨å†Œæœºåˆ¶**: ä½¿ç”¨è£…é¥°å™¨æ³¨å†Œæ¨¡å‹
- **çµæ´»é…ç½®**: æ”¯æŒ YAML é…ç½®æ–‡ä»¶

### ğŸš€ è‡ªåŠ¨åŒ–åŠŸèƒ½
- **GPU ç®¡ç†**: è‡ªåŠ¨æ£€æµ‹å’Œä½¿ç”¨ GPU
- **æ—©åœæœºåˆ¶**: å†…ç½®æ—©åœé¿å…è¿‡æ‹Ÿåˆ
- **æ¨¡å‹ä¿å­˜**: è‡ªåŠ¨ä¿å­˜æœ€ä½³æ¨¡å‹
- **æ—¥å¿—è®°å½•**: å®Œæ•´çš„è®­ç»ƒæ—¥å¿—

### ğŸ”§ PyTorch ä¼˜åŒ–
- **æ¢¯åº¦è£å‰ª**: é˜²æ­¢æ¢¯åº¦çˆ†ç‚¸
- **å­¦ä¹ ç‡è°ƒåº¦**: æ”¯æŒå¤šç§ä¼˜åŒ–å™¨
- **æ‰¹é‡è®­ç»ƒ**: é«˜æ•ˆçš„æ•°æ®åŠ è½½

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. åŸºç¡€ä½¿ç”¨

```python
from model import LSTMModel
from data_manager import DataManager, DataConfig

# å‡†å¤‡æ•°æ®
config = DataConfig(base_dir='rq_data_parquet')
manager = DataManager(config)
loaders = manager.run_full_pipeline()

# åˆ›å»ºæ¨¡å‹
model = LSTMModel(
    d_feat=20,
    hidden_size=64,
    num_layers=2,
    n_epochs=100,
    lr=0.001
)

# è®­ç»ƒ
model.fit(loaders.train, loaders.val, save_path='output/model.pth')

# é¢„æµ‹
predictions = model.predict(loaders.test)
```

### 2. é…ç½®é©±åŠ¨

```python
from model import ModelFactory

# æ¨¡å‹é…ç½®
config = {
    'class': 'LSTM',
    'kwargs': {
        'd_feat': 20,
        'hidden_size': 128,
        'num_layers': 3,
        'dropout': 0.2,
        'n_epochs': 200,
        'lr': 0.0005
    }
}

# åˆ›å»ºæ¨¡å‹
model = ModelFactory.create_model(config)
model.fit(train_loader, valid_loader)
```

### 3. æ¨¡å‹å¯¹æ¯”

```python
from model import LSTMModel, GRUModel, TransformerModel, VAEModel

models = {
    'LSTM': LSTMModel(d_feat=20, hidden_size=64),
    'GRU': GRUModel(d_feat=20, hidden_size=64),
    'Transformer': TransformerModel(d_feat=20, d_model=64),
    'VAE': VAEModel(d_feat=20, hidden_dim=128, latent_dim=16)
}

results = {}
for name, model in models.items():
    model.fit(train_loader, valid_loader)
    predictions = model.predict(test_loader)
    results[name] = evaluate(predictions, labels)
```

## ğŸ“š ç±»ç»§æ‰¿å…³ç³»

```
BaseModel (æŠ½è±¡åŸºç±»)
    â”œâ”€â”€ predict() - æŠ½è±¡æ–¹æ³•
    â””â”€â”€ __call__() - è°ƒç”¨ predict()
    
Model (ç»§æ‰¿ BaseModel)
    â”œâ”€â”€ fit() - æŠ½è±¡æ–¹æ³•
    â””â”€â”€ predict() - æŠ½è±¡æ–¹æ³•
    
PyTorchModel (ç»§æ‰¿ Model)
    â”œâ”€â”€ è‡ªåŠ¨ GPU ç®¡ç†
    â”œâ”€â”€ å†…ç½®æ—©åœæœºåˆ¶
    â”œâ”€â”€ æ¨¡å‹ä¿å­˜/åŠ è½½
    â””â”€â”€ è®­ç»ƒå¾ªç¯å°è£…
    
LSTMModel / GRUModel / TransformerModel
    â””â”€â”€ ç»§æ‰¿ PyTorchModelï¼Œå®ç°å…·ä½“æ¨¡å‹
```

## ğŸ”¨ åˆ›å»ºè‡ªå®šä¹‰æ¨¡å‹

### æ–¹æ³• 1: ç»§æ‰¿ PyTorchModel

```python
import torch.nn as nn
from model import PyTorchModel, register_model

class MyNet(nn.Module):
    """è‡ªå®šä¹‰ç¥ç»ç½‘ç»œ"""
    def __init__(self, d_feat, hidden_size):
        super().__init__()
        self.fc1 = nn.Linear(d_feat, hidden_size)
        self.fc2 = nn.Linear(hidden_size, 1)
        self.relu = nn.ReLU()
    
    def forward(self, x):
        x = x[:, -1, :]  # å–æœ€åæ—¶é—´æ­¥
        x = self.relu(self.fc1(x))
        return self.fc2(x).squeeze(-1)


@register_model('my_model')
class MyModel(PyTorchModel):
    """è‡ªå®šä¹‰æ¨¡å‹"""
    
    def __init__(self, d_feat=20, hidden_size=64, **kwargs):
        super().__init__(**kwargs)
        self.d_feat = d_feat
        self.hidden_size = hidden_size
        
        # åˆ›å»ºç½‘ç»œ
        self.model = MyNet(d_feat, hidden_size).to(self.device)
        self.optimizer = self._get_optimizer()
        self.criterion = self._get_loss_fn()
    
    def fit(self, train_loader, valid_loader=None, save_path=None):
        """è®­ç»ƒæ¨¡å‹"""
        for epoch in range(self.n_epochs):
            train_loss = self._train_epoch(train_loader)
            
            if valid_loader:
                valid_loss = self._valid_epoch(valid_loader)
                self.logger.info(
                    f"Epoch {epoch+1}: "
                    f"Train={train_loss:.6f}, Valid={valid_loss:.6f}"
                )
        
        self.fitted = True
    
    def predict(self, test_loader, return_numpy=True):
        """é¢„æµ‹"""
        self.model.eval()
        predictions = []
        
        with torch.no_grad():
            for batch_x, _ in test_loader:
                batch_x = batch_x.to(self.device)
                pred = self.model(batch_x)
                predictions.append(pred.cpu())
        
        predictions = torch.cat(predictions)
        return predictions.numpy() if return_numpy else predictions
```

### æ–¹æ³• 2: ç»§æ‰¿ Model (ä¸ä½¿ç”¨ PyTorch)

```python
from model import Model, register_model
import lightgbm as lgb

@register_model('lgb')
class LightGBMModel(Model):
    """LightGBM æ¨¡å‹"""
    
    def __init__(self, num_leaves=31, learning_rate=0.05, n_estimators=100):
        super().__init__()
        self.params = {
            'num_leaves': num_leaves,
            'learning_rate': learning_rate,
            'n_estimators': n_estimators
        }
        self.model = None
    
    def fit(self, X_train, y_train, X_valid=None, y_valid=None):
        """è®­ç»ƒ"""
        train_data = lgb.Dataset(X_train, label=y_train)
        
        if X_valid is not None:
            valid_data = lgb.Dataset(X_valid, label=y_valid)
            self.model = lgb.train(
                self.params,
                train_data,
                valid_sets=[valid_data],
                callbacks=[lgb.early_stopping(20)]
            )
        else:
            self.model = lgb.train(self.params, train_data)
        
        self.fitted = True
    
    def predict(self, X_test):
        """é¢„æµ‹"""
        if not self.fitted:
            raise ValueError("Model not fitted")
        return self.model.predict(X_test)
```

## ğŸ¨ å·²å®ç°çš„æ¨¡å‹

| æ¨¡å‹ | ç±»å | æ³¨å†Œå | ç‰¹ç‚¹ |
|------|------|--------|------|
| LSTM | `LSTMModel` | `'lstm'`, `'LSTM'` | é•¿çŸ­æœŸè®°å¿†ç½‘ç»œï¼Œé€‚åˆæ—¶åº |
| GRU | `GRUModel` | `'gru'`, `'GRU'` | å‚æ•°æ›´å°‘ï¼Œè®­ç»ƒæ›´å¿« |
| Transformer | `TransformerModel` | `'transformer'`, `'Transformer'` | è‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼Œæ•æ‰é•¿æœŸä¾èµ– |
| VAE | `VAEModel` | `'vae'`, `'VAE'` | å˜åˆ†è‡ªç¼–ç å™¨ï¼Œå› å­æå–ã€å¼‚å¸¸æ£€æµ‹ âœ¨ |

## ğŸ“‹ æ¨¡å‹å‚æ•°è¯´æ˜

### LSTMModel / GRUModel

```python
model = LSTMModel(
    # æ¨¡å‹ç»“æ„
    d_feat=20,           # ç‰¹å¾ç»´åº¦
    hidden_size=64,      # éšè—å±‚å¤§å°
    num_layers=2,        # RNN å±‚æ•°
    dropout=0.1,         # Dropout æ¦‚ç‡
    
    # è®­ç»ƒå‚æ•°
    n_epochs=100,        # è®­ç»ƒè½®æ•°
    batch_size=256,      # æ‰¹é‡å¤§å°
    lr=0.001,            # å­¦ä¹ ç‡
    early_stop=20,       # æ—©åœè€å¿ƒå€¼
    
    # ä¼˜åŒ–å™¨å’ŒæŸå¤±
    optimizer='adam',    # 'adam', 'sgd', 'adamw'
    loss_fn='mse',      # 'mse', 'mae', 'huber'
    
    # è®¾å¤‡
    device=None         # None(è‡ªåŠ¨), 'cuda', 'cpu'
)
```

### TransformerModel

```python
model = TransformerModel(
    d_feat=20,          # ç‰¹å¾ç»´åº¦
    d_model=64,         # Transformer éšè—ç»´åº¦
    nhead=4,            # æ³¨æ„åŠ›å¤´æ•°
    num_layers=2,       # Transformer å±‚æ•°
    dropout=0.1,        # Dropout æ¦‚ç‡
    # ... å…¶ä»–å‚æ•°åŒä¸Š
)
```

## ğŸ’¾ æ¨¡å‹ä¿å­˜å’ŒåŠ è½½

```python
# è®­ç»ƒæ—¶è‡ªåŠ¨ä¿å­˜
model.fit(train_loader, valid_loader, save_path='output/best_model.pth')

# æ‰‹åŠ¨ä¿å­˜
model.save_model('output/my_model.pth')

# åŠ è½½æ¨¡å‹
new_model = LSTMModel(d_feat=20, hidden_size=64)
new_model.load_model('output/best_model.pth')

# ç»§ç»­è®­ç»ƒ
new_model.fit(train_loader, valid_loader)
```

## ğŸ”— ä¸å…¶ä»–æ¨¡å—é›†æˆ

### ä¸ DataManager é›†æˆ

```python
from data_manager import DataManager, DataConfig
from model import LSTMModel

# 1. æ•°æ®å‡†å¤‡
config = DataConfig(base_dir='rq_data_parquet')
manager = DataManager(config)
loaders = manager.run_full_pipeline()

# 2. æ¨¡å‹è®­ç»ƒ
model = LSTMModel(d_feat=len(manager.feature_cols))
model.fit(loaders.train, loaders.val)

# 3. é¢„æµ‹
predictions = model.predict(loaders.test)
```

### ä¸ Factorsystem é›†æˆ

```python
from model import LSTMModel
from Factorsystem import FactorBacktestSystem, BacktestConfig

# 1. è®­ç»ƒæ¨¡å‹
model = LSTMModel(d_feat=20)
model.fit(train_loader, valid_loader)

# 2. ç”Ÿæˆå› å­
predictions = model.predict(test_loader)

# 3. æ·»åŠ åˆ°æ•°æ®æ¡†
df['factor'] = predictions

# 4. å›æµ‹
backtest_config = BacktestConfig()
system = FactorBacktestSystem(backtest_config)
results = system.run_backtest(df)
```

## ğŸ“Š å®Œæ•´å·¥ä½œæµç¤ºä¾‹

```python
"""å®Œæ•´çš„é‡åŒ–ç ”ç©¶æµç¨‹"""

# 1. æ•°æ®å‡†å¤‡
from data_manager import DataManager, DataConfig
config = DataConfig(
    base_dir='rq_data_parquet',
    window_size=20,
    split_strategy='time_series'
)
manager = DataManager(config)
loaders = manager.run_full_pipeline()

# 2. æ¨¡å‹è®­ç»ƒ
from model import ModelFactory
model_config = {
    'class': 'LSTM',
    'kwargs': {
        'd_feat': len(manager.feature_cols),
        'hidden_size': 128,
        'num_layers': 3,
        'n_epochs': 200,
        'lr': 0.0005,
        'early_stop': 20
    }
}
model = ModelFactory.create_model(model_config)
model.fit(
    loaders.train,
    loaders.val,
    save_path='output/best_model.pth'
)

# 3. ç”Ÿæˆé¢„æµ‹
predictions = model.predict(loaders.test)

# 4. å›æµ‹åˆ†æ
from Factorsystem import FactorBacktestSystem, BacktestConfig
backtest_config = BacktestConfig(
    output_dir='output/backtest',
    save_plots=True
)
system = FactorBacktestSystem(backtest_config)

# å‡†å¤‡å›æµ‹æ•°æ®
test_df = manager.split_data[2]  # æµ‹è¯•é›†
test_df['factor'] = predictions

# è¿è¡Œå›æµ‹
results = system.run_backtest(test_df)

# 5. æŸ¥çœ‹ç»“æœ
print(f"ICå‡å€¼: {results['ic_stats']['ic_mean']:.4f}")
print(f"å¤æ™®æ¯”ç‡: {results['performance_metrics']['long_short']['sharpe_ratio']:.4f}")
print(f"å¹´åŒ–æ”¶ç›Š: {results['performance_metrics']['long_short']['annual_return']:.2%}")
```

## ğŸŒŸ VAE æ¨¡å‹è¯¦è§£

### VAE (Variational Autoencoder) ç‰¹æ€§

VAE æ˜¯ä¸€ç§ç”Ÿæˆæ¨¡å‹ï¼Œåœ¨é‡åŒ–é‡‘èä¸­ç‰¹åˆ«é€‚åˆï¼š
- **å› å­æå–**: ä»é«˜ç»´ç‰¹å¾ä¸­æå–ä½ç»´æ½œåœ¨å› å­
- **å¼‚å¸¸æ£€æµ‹**: é€šè¿‡é‡æ„è¯¯å·®æ£€æµ‹å¼‚å¸¸äº¤æ˜“æ¨¡å¼
- **ç‰¹å¾å­¦ä¹ **: å­¦ä¹ æ•°æ®çš„éšå«ç»“æ„

### VAE æ¨¡å‹æ¶æ„

```
è¾“å…¥åºåˆ— [batch, window, features]
    â†“
ç¼–ç å™¨ (GRU) â†’ æ½œåœ¨ç©ºé—´ (Î¼, Ïƒ)
    â†“
é‡å‚æ•°åŒ– z = Î¼ + ÎµÂ·Ïƒ
    â†“
    â”œâ†’ è§£ç å™¨ â†’ é‡æ„åºåˆ—
    â””â†’ é¢„æµ‹å¤´ â†’ æ”¶ç›Šé¢„æµ‹
```

### VAE ä½¿ç”¨ç¤ºä¾‹

```python
from model import VAEModel

# åˆ›å»º VAE æ¨¡å‹
vae_model = VAEModel(
    d_feat=20,              # è¾“å…¥ç‰¹å¾ç»´åº¦
    hidden_dim=128,         # GRUéšè—å±‚ç»´åº¦
    latent_dim=16,          # æ½œåœ¨ç©ºé—´ç»´åº¦
    window_size=40,         # æ—¶é—´çª—å£
    dropout=0.3,
    
    # VAE æŸå¤±æƒé‡
    alpha_recon=0.1,        # é‡æ„æŸå¤±æƒé‡
    beta_kl=0.001,          # KLæ•£åº¦æƒé‡
    gamma_pred=1.0,         # é¢„æµ‹æŸå¤±æƒé‡
    
    n_epochs=50,
    lr=0.001
)

# è®­ç»ƒ
vae_model.fit(train_loader, valid_loader, save_path='output/vae_model.pth')

# é¢„æµ‹ + æå–æ½œåœ¨ç‰¹å¾
predictions, latent_features = vae_model.predict(
    test_loader, 
    return_latent=True
)

# æˆ–å•ç‹¬æå–æ½œåœ¨ç‰¹å¾ï¼ˆç”¨äºå› å­ç”Ÿæˆï¼‰
mu, z = vae_model.extract_latent(test_loader)
```

### VAE æŸå¤±å‡½æ•°

VAE ä½¿ç”¨ä¸‰ä¸ªæŸå¤±çš„åŠ æƒç»„åˆï¼š

1. **é‡æ„æŸå¤±** (Reconstruction Loss): ç¡®ä¿è§£ç å™¨èƒ½é‡æ„è¾“å…¥
   ```python
   L_recon = MSE(x_recon, x_true)
   ```

2. **KL æ•£åº¦** (KL Divergence): æ­£åˆ™åŒ–æ½œåœ¨ç©ºé—´ï¼Œä½¿å…¶æ¥è¿‘æ ‡å‡†æ­£æ€åˆ†å¸ƒ
   ```python
   L_kl = -0.5 * mean(1 + log(ÏƒÂ²) - Î¼Â² - ÏƒÂ²)
   ```

3. **é¢„æµ‹æŸå¤±** (Prediction Loss): ç›‘ç£å­¦ä¹ æ”¶ç›Šé¢„æµ‹
   ```python
   L_pred = MSE(y_pred, y_true)
   ```

æ€»æŸå¤±:
```python
L_total = Î±Â·L_recon + Î²Â·L_kl + Î³Â·L_pred
```

### VAE å‚æ•°è°ƒä¼˜å»ºè®®

| å‚æ•° | æ¨èèŒƒå›´ | è¯´æ˜ |
|------|---------|------|
| `hidden_dim` | 64-256 | ç¼–ç å™¨éšè—å±‚å¤§å° |
| `latent_dim` | 8-32 | æ½œåœ¨ç©ºé—´ç»´åº¦ï¼ˆå› å­æ•°é‡ï¼‰ |
| `alpha_recon` | 0.05-0.2 | é‡æ„æŸå¤±æƒé‡ï¼ˆè¾ƒå°ï¼‰ |
| `beta_kl` | 0.0001-0.01 | KLæ•£åº¦æƒé‡ï¼ˆå¾ˆå°ï¼‰ |
| `gamma_pred` | 0.5-2.0 | é¢„æµ‹æŸå¤±æƒé‡ï¼ˆè¾ƒå¤§ï¼‰ |
| `dropout` | 0.2-0.4 | Dropoutç‡ |

### VAE æ½œåœ¨ç‰¹å¾å¯è§†åŒ–

```python
import matplotlib.pyplot as plt
from sklearn.manifold import TSNE

# æå–æ½œåœ¨ç‰¹å¾
mu, z = vae_model.extract_latent(test_loader)

# t-SNE é™ç»´åˆ° 2D
tsne = TSNE(n_components=2)
z_2d = tsne.fit_transform(z)

# å¯è§†åŒ–
plt.figure(figsize=(10, 8))
scatter = plt.scatter(z_2d[:, 0], z_2d[:, 1], c=labels, cmap='viridis', alpha=0.6)
plt.colorbar(scatter, label='Return')
plt.title('VAE Latent Space (t-SNE)')
plt.xlabel('Component 1')
plt.ylabel('Component 2')
plt.show()
```

### VAE ç”¨äºå› å­ç”Ÿæˆ

```python
# 1. è®­ç»ƒ VAE
vae_model.fit(train_loader, valid_loader)

# 2. æå–æ½œåœ¨ç‰¹å¾ä½œä¸ºå› å­
mu_features, z_features = vae_model.extract_latent(test_loader)

# 3. æ„å»ºå› å­DataFrame
import pandas as pd
factor_df = pd.DataFrame({
    'latent_mean': mu_features.mean(axis=1),
    'latent_std': mu_features.std(axis=1),
    **{f'latent_{i}': mu_features[:, i] for i in range(mu_features.shape[1])}
})

# 4. å› å­æ ‡å‡†åŒ–
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
factor_df_scaled = pd.DataFrame(
    scaler.fit_transform(factor_df),
    columns=factor_df.columns
)

# 5. å›æµ‹
from Factorsystem import FactorBacktestSystem
backtest_system = FactorBacktestSystem(backtest_config)
results = backtest_system.run_backtest(factor_df_scaled)
```

## ğŸ¯ ä¸‹ä¸€æ­¥è®¡åˆ’

- [ ] æ·»åŠ æ›´å¤šæ¨¡å‹ (TabNet, TCN, ALSTM ç­‰)
- [ ] å®ç°æ¨¡å‹é›†æˆ (Ensemble)
- [ ] æ·»åŠ è¶…å‚æ•°ä¼˜åŒ–
- [ ] å®ç°å¢é‡å­¦ä¹ 
- [ ] æ·»åŠ æ¨¡å‹è§£é‡Šæ€§å·¥å…·
- [x] âœ… æ·»åŠ  VAE æ¨¡å‹ï¼ˆå› å­æå–ã€å¼‚å¸¸æ£€æµ‹ï¼‰
- [ ] åˆ›å»ºå®éªŒç®¡ç†ç³»ç»Ÿ
- [ ] æ”¯æŒåˆ†å¸ƒå¼è®­ç»ƒ

## ğŸ“– å‚è€ƒ

- **Qlib**: https://github.com/microsoft/qlib
- **è®¾è®¡ç†å¿µ**: å‚ç…§ Qlib çš„æ¨¡å‹æ¥å£è®¾è®¡
- **VAE**: Kingma & Welling (2013) "Auto-Encoding Variational Bayes"

## ğŸ“ æ›´æ–°æ—¥å¿—

- **v1.1.0** (2025-11-19)
  - âœ¨ æ·»åŠ  VAE (Variational Autoencoder) æ¨¡å‹
  - âœ¨ æ”¯æŒæ½œåœ¨ç‰¹å¾æå–ç”¨äºå› å­ç”Ÿæˆ
  - âœ… å®Œå–„æ¨¡å‹æ–‡æ¡£å’Œä½¿ç”¨ç¤ºä¾‹

- **v1.0.0** (2025-11-19)
  - âœ… åˆ›å»ºæ¨¡å‹åŸºç±»ç³»ç»Ÿ
  - âœ… å®ç°æ¨¡å‹å·¥å‚å’Œæ³¨å†Œæœºåˆ¶
  - âœ… æ·»åŠ  LSTM/GRU/Transformer æ¨¡å‹
  - âœ… å®Œæ•´çš„ä½¿ç”¨ç¤ºä¾‹

---

**Author**: QuantClassic Team  
**License**: Internal Use
