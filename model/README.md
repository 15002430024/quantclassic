
# QuantClassic Model Module - æ¨¡å‹æ¨¡å—

æ ‡å‡†åŒ–çš„é‡åŒ–æ¨¡å‹æ¥å£å’Œå®ç°ï¼Œå‚ç…§ Qlib è®¾è®¡ã€‚

> **ğŸ†• ç‰ˆæœ¬: v2.0.0 (2026-01-11 é‡æ„)**
> - ç»Ÿä¸€è®­ç»ƒå¼•æ“ï¼Œ`fit()` ä»£ç†åˆ° `train/SimpleTrainer`
> - ç»Ÿä¸€ `predict()` æ–¹æ³•åˆ°åŸºç±»ï¼Œæ”¯æŒæ‰€æœ‰ batch æ ¼å¼
> - æ¨¡å—åŒ–é…ç½®ç³»ç»Ÿï¼Œæ”¯æŒçµæ´»ç»„åˆæ—¶åº/å›¾/èåˆæ¨¡å—
> - å›¾æ„å»ºç»Ÿä¸€åˆ° `data_processor/graph_builder.py`

## ğŸ“¦ æ¨¡å—ç»“æ„

```
model/
â”œâ”€â”€ base_model.py           # æ¨¡å‹åŸºç±» (BaseModel â†’ Model â†’ PyTorchModel)
â”œâ”€â”€ pytorch_models.py       # PyTorch æ¨¡å‹å®ç° (LSTM/GRU/Transformer/VAE)
â”œâ”€â”€ hybrid_graph_models.py  # æ··åˆå›¾æ¨¡å‹ (HybridGraphModel + TemporalBlock/GraphBlock/FusionBlock)
â”œâ”€â”€ model_factory.py        # æ¨¡å‹å·¥å‚å’Œæ³¨å†Œæœºåˆ¶
â”œâ”€â”€ model_config.py         # âš ï¸ æ—§é…ç½®ï¼ˆå·²åºŸå¼ƒï¼Œè¯·ç”¨ modular_config.pyï¼‰
â”œâ”€â”€ modular_config.py       # ğŸ†• æ¨¡å—åŒ–é…ç½®ç³»ç»Ÿ (CompositeModelConfig)
â”œâ”€â”€ loss.py                 # æŸå¤±å‡½æ•° (UnifiedLoss, ICLoss, CorrelationRegularizer)
â”œâ”€â”€ train/                  # ğŸ†• ç»Ÿä¸€è®­ç»ƒå¼•æ“
â”‚   â”œâ”€â”€ base_trainer.py     #   è®­ç»ƒåŸºç±» + TrainerConfig
â”‚   â”œâ”€â”€ simple_trainer.py   #   ç®€å•è®­ç»ƒå™¨ï¼ˆå•çª—å£ï¼‰
â”‚   â”œâ”€â”€ rolling_window_trainer.py  #  æ»šåŠ¨çª—å£è®­ç»ƒå™¨
â”‚   â””â”€â”€ rolling_daily_trainer.py   #  æ—¥çº§æ»šåŠ¨è®­ç»ƒå™¨
â”œâ”€â”€ example/                # ä½¿ç”¨ç¤ºä¾‹
â””â”€â”€ updatemd/               # è¯¦ç»†æ–‡æ¡£
```

## âœ¨ æ ¸å¿ƒç‰¹æ€§

### ğŸ¯ ç»Ÿä¸€æ¥å£
- **æ ‡å‡†åŒ–**: æ‰€æœ‰æ¨¡å‹ç»§æ‰¿è‡ª `PyTorchModel` åŸºç±»
- **ä¸€è‡´æ€§**: ç»Ÿä¸€çš„ `fit()` å’Œ `predict()` æ¥å£
- **ğŸ†• é€šç”¨ predict**: åŸºç±»å®ç°ç»Ÿä¸€é¢„æµ‹é€»è¾‘ï¼Œæ”¯æŒ `(x,y)` / `(x,y,adj,...)` / `dict` ç­‰å¤šç§ batch æ ¼å¼
- **ğŸ†• Trainer å¯¹é½**: `SimpleTrainer.predict` ä¼˜å…ˆå§”æ‰˜æ¨¡å‹çš„ `predict()`ï¼Œç¡®ä¿ä¸æ¨¡å‹çš„ batch è§£æä¿æŒä¸€è‡´ï¼›çº¯ `nn.Module` è‡ªåŠ¨å›é€€å†…ç½®å®ç°

### ğŸ­ å·¥å‚æ¨¡å¼
- **åŠ¨æ€åˆ›å»º**: é€šè¿‡é…ç½®å­—å…¸åˆ›å»ºæ¨¡å‹
- **æ³¨å†Œæœºåˆ¶**: ä½¿ç”¨ `@register_model` è£…é¥°å™¨æ³¨å†Œæ¨¡å‹
- **ğŸ†• æ¨¡å—åŒ–é…ç½®**: `CompositeModelConfig` æ”¯æŒæ—¶åº/å›¾/èåˆæ¨¡å—è‡ªç”±ç»„åˆ

### ğŸš€ è‡ªåŠ¨åŒ–åŠŸèƒ½
- **GPU ç®¡ç†**: è‡ªåŠ¨æ£€æµ‹å’Œä½¿ç”¨ GPU
- **æ—©åœæœºåˆ¶**: å†…ç½®æ—©åœé¿å…è¿‡æ‹Ÿåˆ
- **æ¨¡å‹ä¿å­˜**: è‡ªåŠ¨ä¿å­˜æœ€ä½³æ¨¡å‹
- **æ—¥å¿—è®°å½•**: å®Œæ•´çš„è®­ç»ƒæ—¥å¿—
- **ğŸ†• å­¦ä¹ ç‡è°ƒåº¦**: æ”¯æŒ ReduceLROnPlateau / Cosine / Step

### ğŸ”§ è®­ç»ƒå¼•æ“ (2026-01 é‡æ„)
- **è®­ç»ƒä»£ç†**: `Model.fit()` å†…éƒ¨ä½¿ç”¨ `SimpleTrainer`ï¼Œä¿æŒæ¥å£å…¼å®¹
- **æ»šåŠ¨è®­ç»ƒ**: `RollingWindowTrainer` æ”¯æŒæƒé‡ç»§æ‰¿ã€ä¼˜åŒ–å™¨çŠ¶æ€ä¿å­˜
- **ç›¸å…³æ€§æ­£åˆ™åŒ–**: æ”¯æŒ `lambda_corr` æŠ‘åˆ¶ç‰¹å¾å†—ä½™

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. åŸºç¡€ä½¿ç”¨

```python
from quantclassic.model import LSTMModel
from quantclassic.data_set import DataManager, DataConfig

# å‡†å¤‡æ•°æ®
config = DataConfig(base_dir='rq_data_parquet')
manager = DataManager(config)
loaders = manager.run_full_pipeline()

# åˆ›å»ºæ¨¡å‹
model = LSTMModel(
    d_feat=20,
    hidden_size=64,
    num_layers=2,
    n_epochs=100,
    lr=0.001
)

# è®­ç»ƒï¼ˆå†…éƒ¨ä½¿ç”¨ SimpleTrainerï¼‰
model.fit(loaders.train, loaders.val, save_path='output/model.pth')

# é¢„æµ‹ï¼ˆæ”¯æŒæ ‡å‡†/å›¾/æ—¥çº§ loaderï¼‰
predictions = model.predict(loaders.test)
```

### 2. æ¨¡å—åŒ–é…ç½®ï¼ˆæ¨èï¼‰

```python
from quantclassic.model.modular_config import ModelConfigBuilder, ConfigTemplates
from quantclassic.model import create_model_from_composite_config

# æ–¹å¼1: ä½¿ç”¨é¢„å®šä¹‰æ¨¡æ¿
config = ConfigTemplates.pure_temporal(d_feat=20, model_size='default')

# æ–¹å¼2: ä½¿ç”¨ Builder çµæ´»ç»„åˆ
config = ModelConfigBuilder() \
    .set_input(d_feat=20) \
    .add_temporal(rnn_type='lstm', hidden_size=128, use_attention=True) \
    .add_graph(gat_type='correlation', hidden_dim=64, heads=4) \
    .add_fusion(hidden_sizes=[128, 64]) \
    .build()

# åˆ›å»ºæ¨¡å‹
model = create_model_from_composite_config(config)
model.fit(train_loader, valid_loader)
```

### 3. ä½¿ç”¨è®­ç»ƒå¼•æ“

```python
from quantclassic.model.train import SimpleTrainer, TrainerConfig

# åˆ›å»ºè®­ç»ƒé…ç½®
config = TrainerConfig(
    n_epochs=100,
    lr=0.001,
    early_stop=20,
    loss_fn='mse',
    lambda_corr=0.01,  # ç›¸å…³æ€§æ­£åˆ™åŒ–
    use_scheduler=True,
    scheduler_type='plateau'
)

# åˆ›å»ºè®­ç»ƒå™¨ï¼ˆä¼ å…¥ nn.Moduleï¼‰
trainer = SimpleTrainer(model.model, config, device='cuda')
result = trainer.train(train_loader, valid_loader)
```

## ğŸ“š ç±»ç»§æ‰¿å…³ç³»

```
BaseModel (æŠ½è±¡åŸºç±»)
    â”œâ”€â”€ predict() - æŠ½è±¡æ–¹æ³•
    â””â”€â”€ __call__() - è°ƒç”¨ predict()
    
Model (ç»§æ‰¿ BaseModel)
    â”œâ”€â”€ fit() - æŠ½è±¡æ–¹æ³•
    â””â”€â”€ predict() - æŠ½è±¡æ–¹æ³•
    
PyTorchModel (ç»§æ‰¿ Model)
    â”œâ”€â”€ ğŸ†• é€šç”¨ predict() - æ”¯æŒæ‰€æœ‰ batch æ ¼å¼
    â”‚   â”œâ”€â”€ _parse_batch_data() - ç»Ÿä¸€è§£æ (x,y) / (x,y,adj,...) / dict
    â”‚   â”œâ”€â”€ _forward_for_predict() - å‰å‘ä¼ æ’­é’©å­ï¼ˆå¯è¦†å†™ï¼‰
    â”‚   â””â”€â”€ _post_process() - åå¤„ç†é’©å­ï¼ˆå¯è¦†å†™ï¼‰
    â”œâ”€â”€ fit() - ä»£ç†åˆ° SimpleTrainer
    â”œâ”€â”€ è‡ªåŠ¨ GPU ç®¡ç†
    â”œâ”€â”€ å­¦ä¹ ç‡è°ƒåº¦å™¨
    â””â”€â”€ ç›¸å…³æ€§æ­£åˆ™åŒ–æ”¯æŒ
    
LSTMModel / GRUModel / TransformerModel
    â””â”€â”€ ç»§æ‰¿ PyTorchModelï¼Œä½¿ç”¨åŸºç±» predict()
    
VAEModel
    â”œâ”€â”€ ç»§æ‰¿ PyTorchModel
    â”œâ”€â”€ è¦†å†™ predict() æ”¯æŒ return_latentï¼›åœ¨ return_latent=False æ—¶å¤ç”¨åŸºç±»é€šç”¨ predict
    â”œâ”€â”€ è¦†å†™ _forward_for_predict() è¿”å› y_pred
    â””â”€â”€ extract_latent() - æå–æ½œåœ¨ç‰¹å¾

HybridGraphModel
    â”œâ”€â”€ ç»§æ‰¿ PyTorchModel
    â”œâ”€â”€ è¦†å†™ _parse_batch_data() è§£æ funda/stock_idx
    â””â”€â”€ æ”¯æŒå›¾æ¨ç†æ¨¡å¼ (batch/cross_sectional/neighbor_sampling)
```

## ğŸ”¨ åˆ›å»ºè‡ªå®šä¹‰æ¨¡å‹

### æ–¹æ³• 1: ç»§æ‰¿ PyTorchModelï¼ˆæ¨èï¼‰

```python
import torch.nn as nn
from quantclassic.model import PyTorchModel, register_model

class MyNet(nn.Module):
    """è‡ªå®šä¹‰ç¥ç»ç½‘ç»œ"""
    def __init__(self, d_feat, hidden_size):
        super().__init__()
        self.fc1 = nn.Linear(d_feat, hidden_size)
        self.fc2 = nn.Linear(hidden_size, 1)
        self.relu = nn.ReLU()
    
    def forward(self, x, return_hidden=False):
        x = x[:, -1, :]  # å–æœ€åæ—¶é—´æ­¥
        hidden = self.relu(self.fc1(x))
        pred = self.fc2(hidden).squeeze(-1)
        if return_hidden:
            return pred, hidden  # æ”¯æŒç›¸å…³æ€§æ­£åˆ™åŒ–
        return pred


@register_model('my_model')
class MyModel(PyTorchModel):
    """è‡ªå®šä¹‰æ¨¡å‹"""
    
    def __init__(self, d_feat=20, hidden_size=64, **kwargs):
        super().__init__(**kwargs)
        self.d_feat = d_feat
        self.hidden_size = hidden_size
        
        # åˆ›å»ºç½‘ç»œ
        self.model = MyNet(d_feat, hidden_size).to(self.device)
        self.optimizer = self._get_optimizer()
        self.criterion = self._get_loss_fn()
    
    def fit(self, train_loader, valid_loader=None, save_path=None):
        """è®­ç»ƒæ¨¡å‹ - ä½¿ç”¨ SimpleTrainer"""
        from quantclassic.model.train import SimpleTrainer, TrainerConfig
        
        config = TrainerConfig(
            n_epochs=self.n_epochs, lr=self.lr, early_stop=self.early_stop,
            loss_fn=self.loss_fn_name, lambda_corr=self.lambda_corr
        )
        trainer = SimpleTrainer(self.model, config, str(self.device))
        result = trainer.train(train_loader, valid_loader, save_path=save_path)
        
        self.fitted = True
        return result
    
    # predict() ç»§æ‰¿è‡ª PyTorchModelï¼Œæ— éœ€å®ç°
    # å¦‚éœ€è‡ªå®šä¹‰ï¼Œå¯è¦†å†™ _forward_for_predict() é’©å­
```

### æ–¹æ³• 2: ç‰¹æ®Šè¾“å‡ºæ¨¡å‹ï¼ˆå¦‚ VAEï¼‰

```python
@register_model('my_vae')
class MyVAEModel(PyTorchModel):
    """VAE ç±»æ¨¡å‹ - éœ€è¦ç‰¹æ®Šçš„å‰å‘é€»è¾‘"""
    
    def _forward_for_predict(self, x, adj=None, idx=None):
        """è¦†å†™å‰å‘é’©å­ï¼Œåªè¿”å›é¢„æµ‹å€¼"""
        _, y_pred, _, _, _ = self.model(x)  # VAE è¿”å›å¤šä¸ªè¾“å‡º
        return y_pred
    
    def predict(self, test_loader, return_numpy=True, return_latent=False):
        """æ‰©å±• predict æ”¯æŒè¿”å›æ½œåœ¨ç‰¹å¾"""
        if not return_latent:
            return super().predict(test_loader, return_numpy)
        
        # è‡ªå®šä¹‰é€»è¾‘å¤„ç† return_latent
        ...
```

## ğŸ¨ å·²å®ç°çš„æ¨¡å‹

| æ¨¡å‹ | ç±»å | æ³¨å†Œå | ç‰¹ç‚¹ |
|------|------|--------|------|
| LSTM | `LSTMModel` | `'lstm'`, `'LSTM'` | é•¿çŸ­æœŸè®°å¿†ç½‘ç»œï¼Œé€‚åˆæ—¶åº |
| GRU | `GRUModel` | `'gru'`, `'GRU'` | å‚æ•°æ›´å°‘ï¼Œè®­ç»ƒæ›´å¿« |
| Transformer | `TransformerModel` | `'transformer'`, `'Transformer'` | è‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼Œæ•æ‰é•¿æœŸä¾èµ– |
| VAE | `VAEModel` | `'vae'`, `'VAE'` | å˜åˆ†è‡ªç¼–ç å™¨ï¼Œå› å­æå–ã€å¼‚å¸¸æ£€æµ‹ |
| HybridGraph | `HybridGraphModel` | `'hybrid_graph'` | ğŸ†• æ—¶åº+å›¾æ··åˆæ¨¡å‹ (RNN+Attention+GAT) |

## ğŸ§© æ··åˆå›¾æ¨¡å‹ (HybridGraphModel)

### æ¶æ„æ¦‚è¿°

```
è¾“å…¥: [batch, window, features]
           â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”
    â–¼             â–¼
TemporalBlock  GraphBlock
 (RNN+Attn)     (GAT)
    â”‚             â”‚
    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
           â–¼
      FusionBlock
        (MLP)
           â”‚
           â–¼
       é¢„æµ‹è¾“å‡º
```

### å­æ¨¡å—è¯´æ˜

- **TemporalBlock**: RNN (LSTM/GRU) + Self-Attention + æ®‹å·®è¿æ¥
- **GraphBlock**: GAT å›¾æ³¨æ„åŠ›ç½‘ç»œï¼Œæ”¯æŒè¡Œä¸šå›¾/ç›¸å…³æ€§å›¾/æ··åˆå›¾
- **FusionBlock**: å¤šå±‚ MLP + BatchNorm + æ®‹å·®è¿æ¥

### ä½¿ç”¨ç¤ºä¾‹

```python
from quantclassic.model import HybridGraphModel
from quantclassic.model.modular_config import ConfigTemplates

# ä½¿ç”¨é¢„å®šä¹‰æ¨¡æ¿
config = ConfigTemplates.temporal_with_graph(
    d_feat=20, gat_type='correlation', model_size='default'
)

model = HybridGraphModel(config)
model.fit(train_loader, val_loader)  # loader éœ€è¿”å› (x, y, adj, ...)
predictions = model.predict(test_loader)
```

## ğŸ“‹ è®­ç»ƒé…ç½®å‚æ•°

### TrainerConfig (train/base_trainer.py)

```python
from quantclassic.model.train import TrainerConfig

config = TrainerConfig(
    # åŸºç¡€è®­ç»ƒå‚æ•°
    n_epochs=100,            # è®­ç»ƒè½®æ•°
    lr=0.001,                # å­¦ä¹ ç‡
    early_stop=20,           # æ—©åœè€å¿ƒå€¼
    
    # ä¼˜åŒ–å™¨å’ŒæŸå¤±
    optimizer='adam',        # 'adam', 'sgd', 'adamw'
    loss_fn='mse',           # 'mse', 'mae', 'huber', 'ic', 'mse_corr', 'unified' ç­‰
    loss_kwargs={},          # æŸå¤±å‡½æ•°é¢å¤–å‚æ•°
    
    # å­¦ä¹ ç‡è°ƒåº¦å™¨
    use_scheduler=True,
    scheduler_type='plateau',  # 'plateau', 'cosine', 'step'
    scheduler_patience=5,
    scheduler_factor=0.5,
    scheduler_min_lr=1e-6,
    
    # ğŸ†• ç›¸å…³æ€§æ­£åˆ™åŒ–ï¼ˆæŠ‘åˆ¶ç‰¹å¾å†—ä½™ï¼‰
    lambda_corr=0.0,         # >0 å¯ç”¨ï¼Œæ¨è 0.001~0.1
    
    # æ£€æŸ¥ç‚¹
    checkpoint_dir=None,
    save_best_only=True,
)
```

### æ”¯æŒçš„æŸå¤±å‡½æ•°

| æŸå¤±å‡½æ•° | è¯´æ˜ |
|----------|------|
| `mse` | å‡æ–¹è¯¯å·® |
| `mae` | å¹³å‡ç»å¯¹è¯¯å·® |
| `huber` | Huber æŸå¤± |
| `ic` | æ’åº IC Loss |
| `mse_corr` / `mae_corr` / `huber_corr` / `ic_corr` | å¸¦ç›¸å…³æ€§æ­£åˆ™åŒ– |
| `combined` | ç»„åˆæŸå¤± |
| `unified` | ç»Ÿä¸€æŸå¤± (UnifiedLoss) |

## ğŸ’¾ æ¨¡å‹ä¿å­˜å’ŒåŠ è½½

```python
# è®­ç»ƒæ—¶è‡ªåŠ¨ä¿å­˜
model.fit(train_loader, valid_loader, save_path='output/best_model.pth')

# æ‰‹åŠ¨ä¿å­˜
model.save_model('output/my_model.pth')

# åŠ è½½æ¨¡å‹
new_model = LSTMModel(d_feat=20, hidden_size=64)
new_model.load_model('output/best_model.pth')

# ç»§ç»­è®­ç»ƒ
new_model.fit(train_loader, valid_loader)
```

## ğŸ”— ä¸å…¶ä»–æ¨¡å—é›†æˆ

### å›¾æ„å»ºæ¶æ„ (2026-01 é‡æ„)

```
graph_builder.py (HOW)        daily_graph_loader.py (WHEN)      base_model.py (WHO)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ GraphBuilderFactoryâ”‚â—„â”€â”€â”€â”€â”€â”€â”‚ DailyGraphDataLoader â”‚        â”‚ _parse_batch_dataâ”‚
â”‚ â”œâ”€ industry        â”‚       â”‚   collate_daily()    â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”‚   (x,y,adj,...)  â”‚
â”‚ â”œâ”€ correlation     â”‚       â”‚   æ¯æ—¥è§¦å‘å›¾æ„å»º     â”‚        â”‚                  â”‚
â”‚ â””â”€ hybrid          â”‚       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
å”¯ä¸€å®ç°å…¥å£                  æ•°æ®åŠ è½½æ—¶è°ƒç”¨                  æ¨¡å‹è‡ªåŠ¨è§£æ
```

- **å›¾æ„å»ºç»Ÿä¸€å…¥å£**: `data_processor/graph_builder.py` çš„ `GraphBuilderFactory`
- **âš ï¸ å·²åºŸå¼ƒ**: `model/utils/adj_matrix_builder.py`ï¼Œè¯·ä½¿ç”¨ `AdjMatrixUtils`

### ä¸ DataManager é›†æˆ

```python
from quantclassic.data_set import DataManager, DataConfig
from quantclassic.model import LSTMModel

# 1. æ•°æ®å‡†å¤‡
config = DataConfig(base_dir='rq_data_parquet')
manager = DataManager(config)
loaders = manager.run_full_pipeline()

# 2. æ¨¡å‹è®­ç»ƒ
model = LSTMModel(d_feat=len(manager.feature_cols))
model.fit(loaders.train, loaders.val)

# 3. é¢„æµ‹
predictions = model.predict(loaders.test)
```

### æ»šåŠ¨è®­ç»ƒ

```python
from quantclassic.model.train import RollingWindowTrainer, RollingTrainerConfig

# é…ç½®æ»šåŠ¨è®­ç»ƒ
config = RollingTrainerConfig(
    n_epochs=50,
    weight_inheritance=True,    # ç»§æ‰¿ä¸Šçª—å£æƒé‡
    reset_optimizer=False,      # ğŸ†• ä¿ç•™ä¼˜åŒ–å™¨çŠ¶æ€ï¼ˆåŠ¨é‡ï¼‰
    reset_scheduler=False,
    save_each_window=True,
)

# åˆ›å»ºæ»šåŠ¨è®­ç»ƒå™¨
trainer = RollingWindowTrainer(model_factory, config)
results = trainer.train(rolling_loaders)
```

## ğŸ“Š å®Œæ•´å·¥ä½œæµç¤ºä¾‹

```python
"""å®Œæ•´çš„é‡åŒ–ç ”ç©¶æµç¨‹"""

# 1. æ•°æ®å‡†å¤‡
from quantclassic.data_set import DataManager, DataConfig
config = DataConfig(
    base_dir='rq_data_parquet',
    window_size=20,
    split_strategy='time_series'
)
manager = DataManager(config)
loaders = manager.run_full_pipeline()

# 2. æ¨¡å‹è®­ç»ƒ (ä½¿ç”¨æ¨¡å—åŒ–é…ç½®)
from quantclassic.model.modular_config import ModelConfigBuilder
from quantclassic.model import create_model_from_composite_config

model_config = ModelConfigBuilder() \
    .set_input(d_feat=len(manager.feature_cols)) \
    .add_temporal(rnn_type='lstm', hidden_size=128, num_layers=3, use_attention=True) \
    .add_fusion(hidden_sizes=[128, 64]) \
    .set_training(n_epochs=200, lr=0.0005, early_stop=20, lambda_corr=0.01) \
    .build()

model = create_model_from_composite_config(model_config)
model.fit(
    loaders.train,
    loaders.val,
    save_path='output/best_model.pth'
)

# 3. ç”Ÿæˆé¢„æµ‹ï¼ˆè‡ªåŠ¨æ”¯æŒå„ç§ batch æ ¼å¼ï¼‰
predictions = model.predict(loaders.test)

# 4. å›æµ‹åˆ†æ
from quantclassic.Factorsystem import FactorBacktestSystem, BacktestConfig
backtest_config = BacktestConfig(
    output_dir='output/backtest',
    save_plots=True
)
system = FactorBacktestSystem(backtest_config)

# å‡†å¤‡å›æµ‹æ•°æ®
test_df = manager.split_data[2]  # æµ‹è¯•é›†
test_df['factor'] = predictions

# è¿è¡Œå›æµ‹
results = system.run_backtest(test_df)

# 5. æŸ¥çœ‹ç»“æœ
print(f"ICå‡å€¼: {results['ic_stats']['ic_mean']:.4f}")
print(f"å¤æ™®æ¯”ç‡: {results['performance_metrics']['long_short']['sharpe_ratio']:.4f}")
print(f"å¹´åŒ–æ”¶ç›Š: {results['performance_metrics']['long_short']['annual_return']:.2%}")
```

## ğŸŒŸ VAE æ¨¡å‹è¯¦è§£

### VAE (Variational Autoencoder) ç‰¹æ€§

VAE æ˜¯ä¸€ç§ç”Ÿæˆæ¨¡å‹ï¼Œåœ¨é‡åŒ–é‡‘èä¸­ç‰¹åˆ«é€‚åˆï¼š
- **å› å­æå–**: ä»é«˜ç»´ç‰¹å¾ä¸­æå–ä½ç»´æ½œåœ¨å› å­
- **å¼‚å¸¸æ£€æµ‹**: é€šè¿‡é‡æ„è¯¯å·®æ£€æµ‹å¼‚å¸¸äº¤æ˜“æ¨¡å¼
- **ç‰¹å¾å­¦ä¹ **: å­¦ä¹ æ•°æ®çš„éšå«ç»“æ„

### VAE æ¨¡å‹æ¶æ„

```
è¾“å…¥åºåˆ— [batch, window, features]
    â†“
ç¼–ç å™¨ (GRU) â†’ æ½œåœ¨ç©ºé—´ (Î¼, Ïƒ)
    â†“
é‡å‚æ•°åŒ– z = Î¼ + ÎµÂ·Ïƒ
    â†“
    â”œâ†’ è§£ç å™¨ â†’ é‡æ„åºåˆ—
    â””â†’ é¢„æµ‹å¤´ â†’ æ”¶ç›Šé¢„æµ‹
```

### VAE ä½¿ç”¨ç¤ºä¾‹

```python
from quantclassic.model import VAEModel

# åˆ›å»º VAE æ¨¡å‹
vae_model = VAEModel(
    d_feat=20,              # è¾“å…¥ç‰¹å¾ç»´åº¦
    hidden_dim=128,         # GRUéšè—å±‚ç»´åº¦
    latent_dim=16,          # æ½œåœ¨ç©ºé—´ç»´åº¦
    window_size=40,         # æ—¶é—´çª—å£
    dropout=0.3,
    
    # VAE æŸå¤±æƒé‡
    alpha_recon=0.1,        # é‡æ„æŸå¤±æƒé‡
    beta_kl=0.001,          # KLæ•£åº¦æƒé‡
    gamma_pred=1.0,         # é¢„æµ‹æŸå¤±æƒé‡
    
    n_epochs=50,
    lr=0.001
)

# è®­ç»ƒ
vae_model.fit(train_loader, valid_loader, save_path='output/vae_model.pth')

# é¢„æµ‹ + æå–æ½œåœ¨ç‰¹å¾
predictions, latent_features = vae_model.predict(
    test_loader, 
    return_latent=True
)

# æˆ–å•ç‹¬æå–æ½œåœ¨ç‰¹å¾ï¼ˆç”¨äºå› å­ç”Ÿæˆï¼‰
# ğŸ†• æ”¯æŒå›¾/æ—¥çº§ loaderï¼Œä½¿ç”¨ _parse_batch_data è§£æ
mu, z = vae_model.extract_latent(test_loader)
```

### VAE æ½œåœ¨ç‰¹å¾å¯è§†åŒ–

```python
import matplotlib.pyplot as plt
from sklearn.manifold import TSNE

# æå–æ½œåœ¨ç‰¹å¾
mu, z = vae_model.extract_latent(test_loader)

# t-SNE é™ç»´åˆ° 2D
tsne = TSNE(n_components=2)
z_2d = tsne.fit_transform(z)

# å¯è§†åŒ–
plt.figure(figsize=(10, 8))
scatter = plt.scatter(z_2d[:, 0], z_2d[:, 1], c=labels, cmap='viridis', alpha=0.6)
plt.colorbar(scatter, label='Return')
plt.title('VAE Latent Space (t-SNE)')
plt.show()
```

## âš™ï¸ é…ç½®ç³»ç»Ÿè¿ç§»æŒ‡å—

### æ—§é…ç½® â†’ æ–°é…ç½®

```python
# âŒ æ—§æ–¹å¼ï¼ˆå·²åºŸå¼ƒï¼Œä¼šè§¦å‘ DeprecationWarningï¼‰
from quantclassic.model.model_config import LSTMConfig, ModelConfigFactory
config = LSTMConfig(hidden_size=64, num_layers=2)

# âœ… æ–°æ–¹å¼ 1: ä½¿ç”¨æ¨¡æ¿
from quantclassic.model.modular_config import ConfigTemplates
config = ConfigTemplates.pure_temporal(d_feat=20, model_size='default')

# âœ… æ–°æ–¹å¼ 2: ä½¿ç”¨ Builder
from quantclassic.model.modular_config import ModelConfigBuilder
config = ModelConfigBuilder() \
    .set_input(d_feat=20) \
    .add_temporal(rnn_type='lstm', hidden_size=64, num_layers=2) \
    .add_fusion(hidden_sizes=[64]) \
    .build()

# âœ… æ–°æ–¹å¼ 3: ç›´æ¥æ„é€ 
from quantclassic.model.modular_config import CompositeModelConfig, TemporalModuleConfig
config = CompositeModelConfig(
    temporal=TemporalModuleConfig(rnn_type='lstm', hidden_size=64),
    graph=None,
    d_feat=20
)
```

### é…ç½®è‡ªåŠ¨è½¬æ¢

```python
# å¦‚æœæœ‰æ—§é…ç½®å¯¹è±¡ï¼Œå¯è‡ªåŠ¨è½¬æ¢
from quantclassic.model.model_config import to_composite_config
old_config = LSTMConfig(...)
new_config = to_composite_config(old_config)
```

## ğŸ¯ å·²å®Œæˆé‡æ„ (2026-01-11)

| åŠŸèƒ½ | çŠ¶æ€ | è¯´æ˜ |
|------|------|------|
| ç»Ÿä¸€ `predict()` åˆ°åŸºç±» | âœ… | æ”¯æŒ `(x,y)` / `(x,y,adj,...)` / `dict` æ ¼å¼ |
| `fit()` ä»£ç†åˆ° SimpleTrainer | âœ… | ä¿æŒæ¥å£å…¼å®¹ï¼Œå†…éƒ¨ä½¿ç”¨ç»Ÿä¸€è®­ç»ƒå¼•æ“ |
| VAE.extract_latent æ‰¹æ¬¡è§£åŒ… | âœ… | ä½¿ç”¨ `_parse_batch_data` ä¿®å¤ unpack é”™è¯¯ |
| é…ç½®ç³»ç»Ÿå…¼å®¹å±‚ | âœ… | æ—§é…ç½®è§¦å‘åºŸå¼ƒè­¦å‘Šï¼Œæä¾›è½¬æ¢å‡½æ•° |
| å›¾æ„å»ºç»Ÿä¸€å…¥å£ | âœ… | `data_processor/graph_builder.py` |
| æ»šåŠ¨è®­ç»ƒä¼˜åŒ–å™¨çŠ¶æ€ä¿å­˜ | âœ… | `reset_optimizer=False` ç”Ÿæ•ˆ |
| æŸå¤±å‡½æ•°ç™½åå•æ‰©å±• | âœ… | æ”¯æŒ `mae_corr`, `unified` ç­‰ |
| DailyRollingConfig å¯¼å‡º | âœ… | `from model.train import DailyRollingConfig` |


## ğŸ“– å‚è€ƒ

- **Qlib**: https://github.com/microsoft/qlib
- **è®¾è®¡ç†å¿µ**: å‚ç…§ Qlib çš„æ¨¡å‹æ¥å£è®¾è®¡
- **VAE**: Kingma & Welling (2013) "Auto-Encoding Variational Bayes"

## ğŸ“ æ›´æ–°æ—¥å¿—

- **v2.0.0** (2026-01-11)
  - ğŸ†• ç»Ÿä¸€è®­ç»ƒå¼•æ“ `model/train/`ï¼Œ`fit()` ä»£ç†åˆ° `SimpleTrainer`
  - ğŸ†• ç»Ÿä¸€ `predict()` æ–¹æ³•åˆ° `PyTorchModel` åŸºç±»
  - ğŸ†• æ¨¡å—åŒ–é…ç½®ç³»ç»Ÿ `CompositeModelConfig`ï¼Œæ—§é…ç½®æ ‡è®°åºŸå¼ƒ
  - ğŸ†• å›¾æ„å»ºåˆå¹¶åˆ° `data_processor/graph_builder.py`
  - âœ… ä¿®å¤ VAE.extract_latent æ‰¹æ¬¡è§£åŒ…é—®é¢˜
  - âœ… ä¿®å¤æ»šåŠ¨è®­ç»ƒä¼˜åŒ–å™¨çŠ¶æ€ä¸¢å¤±é—®é¢˜
  - âœ… æ‰©å±• TrainerConfig æŸå¤±å‡½æ•°æ”¯æŒåˆ—è¡¨

- **v1.1.0** (2025-11-19)
  - âœ¨ æ·»åŠ  VAE (Variational Autoencoder) æ¨¡å‹
  - âœ¨ æ”¯æŒæ½œåœ¨ç‰¹å¾æå–ç”¨äºå› å­ç”Ÿæˆ
  - âœ… å®Œå–„æ¨¡å‹æ–‡æ¡£å’Œä½¿ç”¨ç¤ºä¾‹

- **v1.0.0** (2025-11-19)
  - âœ… åˆ›å»ºæ¨¡å‹åŸºç±»ç³»ç»Ÿ
  - âœ… å®ç°æ¨¡å‹å·¥å‚å’Œæ³¨å†Œæœºåˆ¶
  - âœ… æ·»åŠ  LSTM/GRU/Transformer æ¨¡å‹
  - âœ… å®Œæ•´çš„ä½¿ç”¨ç¤ºä¾‹

---

**Author**: QuantClassic Team  
**License**: Internal Use
