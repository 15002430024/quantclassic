"""
Base Model Classes - æ¨¡å‹åŸºç±»

å‚ç…§ Qlib çš„è®¾è®¡ï¼Œæä¾›æ ‡å‡†åŒ–çš„æ¨¡å‹æ¥å£
"""

import abc
import torch
import pickle
from typing import Any, Dict, Optional, Union
from pathlib import Path
import logging


class BaseModel(abc.ABC):
    """
    åŸºç¡€æ¨¡å‹ç±» - æ‰€æœ‰æ¨¡å‹çš„æŠ½è±¡åŸºç±»
    
    å‚ç…§ Qlib çš„ BaseModel è®¾è®¡ï¼Œå®šä¹‰æœ€åŸºæœ¬çš„æ¨¡å‹æ¥å£
    """
    
    @abc.abstractmethod
    def predict(self, *args, **kwargs) -> Any:
        """
        æ¨¡å‹é¢„æµ‹æ–¹æ³•
        
        Returns:
            é¢„æµ‹ç»“æœ
        """
        raise NotImplementedError("predict method must be implemented")
    
    def __call__(self, *args, **kwargs) -> Any:
        """
        ä½¿æ¨¡å‹å¯ä»¥åƒå‡½æ•°ä¸€æ ·è°ƒç”¨
        
        Example:
            prediction = model(x_test)  # ç­‰ä»·äº model.predict(x_test)
        """
        return self.predict(*args, **kwargs)
    
    def save(self, save_path: str):
        """
        ä¿å­˜æ¨¡å‹åˆ°ç£ç›˜
        
        Args:
            save_path: ä¿å­˜è·¯å¾„
        """
        Path(save_path).parent.mkdir(parents=True, exist_ok=True)
        
        # è·å–æ‰€æœ‰ä¸ä»¥ '_' å¼€å¤´çš„å±æ€§ï¼ˆå…¬å…±å±æ€§ï¼‰
        state = {
            key: value for key, value in self.__dict__.items()
            if not key.startswith('_')
        }
        
        with open(save_path, 'wb') as f:
            pickle.dump(state, f)
    
    def load(self, load_path: str):
        """
        ä»ç£ç›˜åŠ è½½æ¨¡å‹
        
        Args:
            load_path: åŠ è½½è·¯å¾„
        """
        with open(load_path, 'rb') as f:
            state = pickle.load(f)
        
        for key, value in state.items():
            setattr(self, key, value)


class Model(BaseModel):
    """
    å¯è®­ç»ƒæ¨¡å‹ç±» - ç»§æ‰¿è‡ª BaseModel
    
    å‚ç…§ Qlib çš„ Model è®¾è®¡ï¼Œå¢åŠ è®­ç»ƒæ¥å£
    """
    
    def __init__(self):
        """åˆå§‹åŒ–æ¨¡å‹"""
        self.fitted = False
        self.logger = self._setup_logger()
    
    def _setup_logger(self) -> logging.Logger:
        """é…ç½®æ—¥å¿—"""
        logger = logging.getLogger(self.__class__.__name__)
        logger.setLevel(logging.INFO)
        
        if not logger.handlers:
            handler = logging.StreamHandler()
            formatter = logging.Formatter(
                '%(asctime)s - %(name)s - %(levelname)s - %(message)s',
                datefmt='%Y-%m-%d %H:%M:%S'
            )
            handler.setFormatter(formatter)
            logger.addHandler(handler)
        
        return logger
    
    @abc.abstractmethod
    def fit(self, train_data, valid_data=None, **kwargs):
        """
        è®­ç»ƒæ¨¡å‹
        
        Args:
            train_data: è®­ç»ƒæ•°æ®
            valid_data: éªŒè¯æ•°æ®ï¼ˆå¯é€‰ï¼‰
            **kwargs: å…¶ä»–è®­ç»ƒå‚æ•°
            
        Note:
            è®­ç»ƒååº”è®¾ç½® self.fitted = True
        """
        raise NotImplementedError("fit method must be implemented")
    
    @abc.abstractmethod
    def predict(self, test_data, **kwargs) -> Any:
        """
        é¢„æµ‹
        
        Args:
            test_data: æµ‹è¯•æ•°æ®
            **kwargs: å…¶ä»–é¢„æµ‹å‚æ•°
            
        Returns:
            é¢„æµ‹ç»“æœ
            
        Raises:
            ValueError: å¦‚æœæ¨¡å‹æœªè®­ç»ƒ
        """
        if not self.fitted:
            raise ValueError("Model must be fitted before prediction")
        raise NotImplementedError("predict method must be implemented")


class PyTorchModel(Model):
    """
    PyTorch æ¨¡å‹åŸºç±» - ä¸ºæ·±åº¦å­¦ä¹ æ¨¡å‹æä¾›é€šç”¨åŠŸèƒ½
    
    å°è£… PyTorch æ¨¡å‹çš„å¸¸ç”¨æ“ä½œï¼š
    - è‡ªåŠ¨ GPU ç®¡ç†
    - æ¨¡å‹ä¿å­˜/åŠ è½½
    - æ—©åœæœºåˆ¶
    - è®­ç»ƒå¾ªç¯
    - ğŸ†• å­¦ä¹ ç‡è‡ªåŠ¨è°ƒæ•´ (ReduceLROnPlateau)
    - ğŸ†• ç›¸å…³æ€§æ­£åˆ™åŒ– (Correlation Regularization)
    
    âš ï¸ ç›¸å…³æ€§æ­£åˆ™åŒ–ä½¿ç”¨è¯´æ˜:
    --------------------------------
    è¦å¯ç”¨ç›¸å…³æ€§æ­£åˆ™åŒ– (lambda_corr > 0)ï¼Œæ¨¡å‹å¿…é¡»æ»¡è¶³ä»¥ä¸‹è¦æ±‚ï¼š
    
    1. æ¨¡å‹çš„ forward() æ–¹æ³•å¿…é¡»æ”¯æŒ `return_hidden` å‚æ•°ï¼š
       ```python
       def forward(self, x, return_hidden=False):
           ...
           if return_hidden:
               return pred, hidden_features  # è¿”å› (é¢„æµ‹å€¼, éšè—ç‰¹å¾)
           return pred
       ```
    
    2. hidden_features åº”ä¸ºè¿›å…¥è¾“å‡ºå±‚å‰çš„èåˆç‰¹å¾ï¼Œå½¢çŠ¶ä¸º [batch_size, hidden_dim]
    
    3. å¦‚æœæ¨¡å‹ä¸æ”¯æŒ return_hiddenï¼Œç³»ç»Ÿä¼šè‡ªåŠ¨é™çº§å¹¶å‘å‡ºè­¦å‘Šï¼Œ
       ä½†ç›¸å…³æ€§æ­£åˆ™åŒ–å°†ä¸ä¼šç”Ÿæ•ˆã€‚
    
    ç¤ºä¾‹:
        # å¯ç”¨ç›¸å…³æ€§æ­£åˆ™åŒ–
        model = MyModel(lambda_corr=0.01, ...)
        
        # ä¸å¯ç”¨ï¼ˆé»˜è®¤ï¼‰
        model = MyModel(lambda_corr=0.0, ...)
    """
    
    def __init__(
        self,
        device: Optional[str] = None,
        n_epochs: int = 100,
        batch_size: int = 256,
        lr: float = 0.001,
        early_stop: int = 20,
        optimizer: str = 'adam',
        loss_fn: str = 'mse',
        # ğŸ†• å­¦ä¹ ç‡è°ƒåº¦å™¨å‚æ•°
        use_scheduler: bool = True,
        scheduler_type: str = 'plateau',  # 'plateau' | 'cosine' | 'step' | None
        scheduler_patience: int = 5,      # ReduceLROnPlateau çš„è€å¿ƒå€¼
        scheduler_factor: float = 0.5,    # å­¦ä¹ ç‡è¡°å‡å› å­
        scheduler_min_lr: float = 1e-6,   # æœ€å°å­¦ä¹ ç‡
        # ğŸ†• ç›¸å…³æ€§æ­£åˆ™åŒ–å‚æ•°
        lambda_corr: float = 0.0,         # ç›¸å…³æ€§æ­£åˆ™åŒ–æƒé‡ï¼Œ0 è¡¨ç¤ºä¸ä½¿ç”¨
                                          # âš ï¸ è®¾ç½® > 0 æ—¶éœ€è¦æ¨¡å‹æ”¯æŒ return_hidden å‚æ•°
        **kwargs
    ):
        """
        Args:
            device: è®¾å¤‡ ('cuda', 'cpu' æˆ– None è‡ªåŠ¨æ£€æµ‹)
            n_epochs: è®­ç»ƒè½®æ•°
            batch_size: æ‰¹é‡å¤§å°
            lr: å­¦ä¹ ç‡
            early_stop: æ—©åœè€å¿ƒå€¼
            optimizer: ä¼˜åŒ–å™¨åç§°
            loss_fn: æŸå¤±å‡½æ•°åç§°
        """
        super().__init__()
        
        # è®¾å¤‡ç®¡ç†
        if device is None:
            self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        else:
            self.device = torch.device(device)
        
        # è®­ç»ƒå‚æ•°
        self.n_epochs = n_epochs
        self.batch_size = batch_size
        self.lr = lr
        self.early_stop = early_stop
        self.optimizer_name = optimizer.lower()
        self.loss_fn_name = loss_fn.lower()
        
        # æ¨¡å‹å’Œä¼˜åŒ–å™¨ï¼ˆå­ç±»ä¸­åˆå§‹åŒ–ï¼‰
        self.model = None
        self.optimizer = None
        self.criterion = None
        self.scheduler = None  # ğŸ†• å­¦ä¹ ç‡è°ƒåº¦å™¨
        
        # ğŸ†• å­¦ä¹ ç‡è°ƒåº¦å™¨é…ç½®
        self.use_scheduler = use_scheduler
        self.scheduler_type = scheduler_type.lower() if scheduler_type else None
        self.scheduler_patience = scheduler_patience
        self.scheduler_factor = scheduler_factor
        self.scheduler_min_lr = scheduler_min_lr
        
        # ğŸ†• ç›¸å…³æ€§æ­£åˆ™åŒ–é…ç½®
        self.lambda_corr = lambda_corr
        self._use_corr_loss = lambda_corr > 0  # æ ‡å¿—ä½ï¼šæ˜¯å¦ä½¿ç”¨ç›¸å…³æ€§æ­£åˆ™åŒ–æŸå¤±
        
        # è®­ç»ƒå†å²
        self.train_losses = []
        self.valid_losses = []
        self.lr_history = []  # ğŸ†• è®°å½•å­¦ä¹ ç‡å˜åŒ–
        self.best_score = float('-inf')
        self.best_epoch = 0
        
        self.logger.info(f"åˆå§‹åŒ– PyTorchModel:")
        self.logger.info(f"  è®¾å¤‡: {self.device}")
        self.logger.info(f"  è®­ç»ƒè½®æ•°: {n_epochs}")
        self.logger.info(f"  æ‰¹é‡å¤§å°: {batch_size}")
        self.logger.info(f"  å­¦ä¹ ç‡: {lr}")
        if use_scheduler:
            self.logger.info(f"  å­¦ä¹ ç‡è°ƒåº¦å™¨: {scheduler_type} (patience={scheduler_patience}, factor={scheduler_factor})")
        if lambda_corr > 0:
            self.logger.info(f"  ğŸ†• ç›¸å…³æ€§æ­£åˆ™åŒ–: lambda={lambda_corr}")
    
    def _get_optimizer(self):
        """åˆ›å»ºä¼˜åŒ–å™¨"""
        if self.optimizer_name == 'adam':
            return torch.optim.Adam(self.model.parameters(), lr=self.lr)
        elif self.optimizer_name == 'sgd':
            return torch.optim.SGD(self.model.parameters(), lr=self.lr)
        elif self.optimizer_name == 'adamw':
            return torch.optim.AdamW(self.model.parameters(), lr=self.lr)
        else:
            raise ValueError(f"Unknown optimizer: {self.optimizer_name}")
    
    def _get_scheduler(self):
        """
        ğŸ†• åˆ›å»ºå­¦ä¹ ç‡è°ƒåº¦å™¨
        
        æ”¯æŒçš„è°ƒåº¦å™¨ç±»å‹:
        - 'plateau': ReduceLROnPlateau - å½“éªŒè¯æŸå¤±åœæ­¢ä¸‹é™æ—¶é™ä½å­¦ä¹ ç‡
        - 'cosine': CosineAnnealingLR - ä½™å¼¦é€€ç«
        - 'step': StepLR - å›ºå®šæ­¥é•¿è¡°å‡
        
        Returns:
            å­¦ä¹ ç‡è°ƒåº¦å™¨æˆ– None
        """
        if not self.use_scheduler or self.scheduler_type is None:
            return None
        
        if self.optimizer is None:
            self.logger.warning("ä¼˜åŒ–å™¨æœªåˆå§‹åŒ–ï¼Œæ— æ³•åˆ›å»ºè°ƒåº¦å™¨")
            return None
        
        if self.scheduler_type == 'plateau':
            # å½“éªŒè¯æŸå¤±åœæ­¢ä¸‹é™æ—¶è‡ªåŠ¨é™ä½å­¦ä¹ ç‡
            scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
                self.optimizer,
                mode='min',                      # ç›‘æ§æŸå¤±ï¼ˆè¶Šå°è¶Šå¥½ï¼‰
                factor=self.scheduler_factor,    # å­¦ä¹ ç‡ä¹˜ä»¥è¿™ä¸ªå› å­
                patience=self.scheduler_patience, # ç­‰å¾…å¤šå°‘ä¸ª epoch
                min_lr=self.scheduler_min_lr,    # æœ€å°å­¦ä¹ ç‡
                verbose=True                     # æ‰“å°å­¦ä¹ ç‡å˜åŒ–
            )
            self.logger.info(f"  âœ… åˆ›å»º ReduceLROnPlateau è°ƒåº¦å™¨")
            
        elif self.scheduler_type == 'cosine':
            # ä½™å¼¦é€€ç«
            scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
                self.optimizer,
                T_max=self.n_epochs,
                eta_min=self.scheduler_min_lr
            )
            self.logger.info(f"  âœ… åˆ›å»º CosineAnnealingLR è°ƒåº¦å™¨")
            
        elif self.scheduler_type == 'step':
            # å›ºå®šæ­¥é•¿è¡°å‡
            scheduler = torch.optim.lr_scheduler.StepLR(
                self.optimizer,
                step_size=self.scheduler_patience,
                gamma=self.scheduler_factor
            )
            self.logger.info(f"  âœ… åˆ›å»º StepLR è°ƒåº¦å™¨")
            
        else:
            self.logger.warning(f"æœªçŸ¥çš„è°ƒåº¦å™¨ç±»å‹: {self.scheduler_type}ï¼Œä¸ä½¿ç”¨è°ƒåº¦å™¨")
            return None
        
        return scheduler
    
    def _step_scheduler(self, val_loss: float = None):
        """
        ğŸ†• æ›´æ–°å­¦ä¹ ç‡è°ƒåº¦å™¨
        
        Args:
            val_loss: éªŒè¯æŸå¤±ï¼ˆReduceLROnPlateau éœ€è¦ï¼‰
        """
        if self.scheduler is None:
            return
        
        # è®°å½•å½“å‰å­¦ä¹ ç‡
        current_lr = self.optimizer.param_groups[0]['lr']
        self.lr_history.append(current_lr)
        
        # æ ¹æ®è°ƒåº¦å™¨ç±»å‹è°ƒç”¨ step
        if self.scheduler_type == 'plateau':
            if val_loss is not None:
                self.scheduler.step(val_loss)
        else:
            self.scheduler.step()
        
        # æ£€æŸ¥å­¦ä¹ ç‡æ˜¯å¦å˜åŒ–
        new_lr = self.optimizer.param_groups[0]['lr']
        if new_lr != current_lr:
            self.logger.info(f"  ğŸ“‰ å­¦ä¹ ç‡è°ƒæ•´: {current_lr:.2e} â†’ {new_lr:.2e}")
    
    def _get_loss_fn(self):
        """
        åˆ›å»ºæŸå¤±å‡½æ•°ï¼ˆğŸ†• æ”¯æŒç›¸å…³æ€§æ­£åˆ™åŒ–ï¼‰
        
        å¦‚æœ lambda_corr > 0ï¼Œä½¿ç”¨å¸¦ç›¸å…³æ€§æ­£åˆ™åŒ–çš„æŸå¤±å‡½æ•°ï¼Œ
        å¦åˆ™ä½¿ç”¨æ ‡å‡†æŸå¤±å‡½æ•°ã€‚
        
        Returns:
            æŸå¤±å‡½æ•°æ¨¡å—
        """
        # å¦‚æœå¯ç”¨ç›¸å…³æ€§æ­£åˆ™åŒ–ï¼Œä½¿ç”¨ loss.py ä¸­çš„æŸå¤±å‡½æ•°
        if self._use_corr_loss:
            try:
                from .loss import get_loss_fn
                loss_type = self.loss_fn_name
                if loss_type in ['mse', 'mae', 'huber', 'ic']:
                    loss_type = f"{loss_type}_corr"
                return get_loss_fn(loss_type=loss_type, lambda_corr=self.lambda_corr)
            except ImportError:
                self.logger.warning("æ— æ³•å¯¼å…¥ loss æ¨¡å—ï¼Œå›é€€åˆ°æ ‡å‡†æŸå¤±å‡½æ•°")
                self._use_corr_loss = False
        
        # æ ‡å‡†æŸå¤±å‡½æ•°
        if self.loss_fn_name == 'mse':
            return torch.nn.MSELoss()
        elif self.loss_fn_name == 'mae':
            return torch.nn.L1Loss()
        elif self.loss_fn_name == 'huber':
            return torch.nn.HuberLoss()
        else:
            raise ValueError(f"Unknown loss function: {self.loss_fn_name}")
    
    def save_model(self, save_path: str, save_optimizer: bool = False):
        """
        ä¿å­˜ PyTorch æ¨¡å‹
        
        Args:
            save_path: ä¿å­˜è·¯å¾„
            save_optimizer: æ˜¯å¦ä¿å­˜ä¼˜åŒ–å™¨çŠ¶æ€
        """
        Path(save_path).parent.mkdir(parents=True, exist_ok=True)
        
        state = {
            'model_state_dict': self.model.state_dict(),
            'config': {
                'n_epochs': self.n_epochs,
                'batch_size': self.batch_size,
                'lr': self.lr,
                'early_stop': self.early_stop,
                'optimizer': self.optimizer_name,
                'loss_fn': self.loss_fn_name,
            },
            'train_losses': self.train_losses,
            'valid_losses': self.valid_losses,
            'best_score': self.best_score,
            'best_epoch': self.best_epoch,
        }
        
        if save_optimizer and self.optimizer is not None:
            state['optimizer_state_dict'] = self.optimizer.state_dict()
        
        torch.save(state, save_path)
        self.logger.info(f"æ¨¡å‹å·²ä¿å­˜: {save_path}")
    
    def load_model(self, load_path: str, load_optimizer: bool = False):
        """
        åŠ è½½ PyTorch æ¨¡å‹
        
        Args:
            load_path: åŠ è½½è·¯å¾„
            load_optimizer: æ˜¯å¦åŠ è½½ä¼˜åŒ–å™¨çŠ¶æ€
        """
        checkpoint = torch.load(load_path, map_location=self.device)
        
        self.model.load_state_dict(checkpoint['model_state_dict'])
        
        if load_optimizer and 'optimizer_state_dict' in checkpoint:
            if self.optimizer is None:
                self.optimizer = self._get_optimizer()
            self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        
        # æ¢å¤è®­ç»ƒå†å²
        self.train_losses = checkpoint.get('train_losses', [])
        self.valid_losses = checkpoint.get('valid_losses', [])
        self.best_score = checkpoint.get('best_score', float('-inf'))
        self.best_epoch = checkpoint.get('best_epoch', 0)
        
        self.fitted = True
        self.logger.info(f"æ¨¡å‹å·²åŠ è½½: {load_path}")
    
    def _train_epoch(self, train_loader):
        """
        è®­ç»ƒä¸€ä¸ª epochï¼ˆğŸ†• æ”¯æŒç›¸å…³æ€§æ­£åˆ™åŒ–ï¼‰
        
        Args:
            train_loader: è®­ç»ƒæ•°æ®åŠ è½½å™¨
            
        Returns:
            å¹³å‡è®­ç»ƒæŸå¤±
            
        Note:
            å¦‚æœå¯ç”¨äº†ç›¸å…³æ€§æ­£åˆ™åŒ– (_use_corr_loss=True)ï¼Œ
            æ¨¡å‹éœ€è¦è¿”å› (predictions, hidden_features) å…ƒç»„ã€‚
            å¯é€šè¿‡ model(x, return_hidden=True) å®ç°ã€‚
            å¦‚æœæ¨¡å‹ä¸æ”¯æŒï¼Œä¼šè‡ªåŠ¨é™çº§å¹¶å‘å‡ºè­¦å‘Šã€‚
        """
        self.model.train()
        total_loss = 0
        n_batches = 0
        
        for batch_x, batch_y in train_loader:
            batch_x = batch_x.to(self.device)
            batch_y = batch_y.to(self.device)
            
            # å‰å‘ä¼ æ’­
            self.optimizer.zero_grad()
            
            # ğŸ†• æ ¹æ®æ˜¯å¦ä½¿ç”¨ç›¸å…³æ€§æ­£åˆ™åŒ–å†³å®šå‰å‘ä¼ æ’­æ–¹å¼
            if self._use_corr_loss:
                # éœ€è¦éšè—ç‰¹å¾ç”¨äºç›¸å…³æ€§æ­£åˆ™åŒ–
                try:
                    output = self.model(batch_x, return_hidden=True)
                    if isinstance(output, tuple) and len(output) >= 2:
                        predictions = output[0]
                        hidden_features = output[-1]  # æœ€åä¸€ä¸ªæ˜¯èåˆç‰¹å¾
                    else:
                        # æ¨¡å‹è¿”å›æ ¼å¼ä¸ç¬¦åˆé¢„æœŸï¼Œé™çº§å¤„ç†
                        self.logger.warning(
                            "âš ï¸ æ¨¡å‹è¿”å›æ ¼å¼ä¸ç¬¦åˆé¢„æœŸï¼ˆåº”è¿”å› (pred, hidden) å…ƒç»„ï¼‰ï¼Œ"
                            "ç›¸å…³æ€§æ­£åˆ™åŒ–å·²ç¦ç”¨ã€‚"
                        )
                        self._use_corr_loss = False
                        self._corr_loss_disabled_logged = True
                        predictions = output if not isinstance(output, tuple) else output[0]
                        hidden_features = None
                except TypeError as e:
                    # ğŸ†• æ¨¡å‹ä¸æ”¯æŒ return_hidden å‚æ•°ï¼Œè‡ªåŠ¨é™çº§
                    if 'return_hidden' in str(e):
                        self.logger.warning(
                            f"âš ï¸ æ¨¡å‹ä¸æ”¯æŒ return_hidden å‚æ•°ï¼Œç›¸å…³æ€§æ­£åˆ™åŒ–å·²è‡ªåŠ¨ç¦ç”¨ã€‚"
                            f"\n   è¦å¯ç”¨ç›¸å…³æ€§æ­£åˆ™åŒ–ï¼Œè¯·ç¡®ä¿æ¨¡å‹çš„ forward() æ–¹æ³•æ”¯æŒ return_hidden=True å‚æ•°ã€‚"
                        )
                        self._use_corr_loss = False
                        self._corr_loss_disabled_logged = True
                        # å›é€€åˆ°æ™®é€šå‰å‘ä¼ æ’­
                        predictions = self.model(batch_x)
                        hidden_features = None
                    else:
                        # å…¶ä»– TypeErrorï¼Œé‡æ–°æŠ›å‡º
                        raise
                
                # è®¡ç®—æŸå¤±
                if self._use_corr_loss and hidden_features is not None:
                    loss = self.criterion(predictions, batch_y, hidden_features)
                else:
                    loss = self.criterion(predictions, batch_y)
            else:
                predictions = self.model(batch_x)
                loss = self.criterion(predictions, batch_y)
            
            # åå‘ä¼ æ’­
            loss.backward()
            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
            self.optimizer.step()
            
            total_loss += loss.item()
            n_batches += 1
        
        return total_loss / n_batches if n_batches > 0 else 0
    
    def _valid_epoch(self, valid_loader):
        """
        éªŒè¯ä¸€ä¸ª epochï¼ˆğŸ†• æ”¯æŒç›¸å…³æ€§æ­£åˆ™åŒ–ï¼‰
        
        Args:
            valid_loader: éªŒè¯æ•°æ®åŠ è½½å™¨
            
        Returns:
            å¹³å‡éªŒè¯æŸå¤±
        """
        self.model.eval()
        total_loss = 0
        n_batches = 0
        
        with torch.no_grad():
            for batch_x, batch_y in valid_loader:
                batch_x = batch_x.to(self.device)
                batch_y = batch_y.to(self.device)
                
                # ğŸ†• æ ¹æ®æ˜¯å¦ä½¿ç”¨ç›¸å…³æ€§æ­£åˆ™åŒ–å†³å®šå‰å‘ä¼ æ’­æ–¹å¼
                if self._use_corr_loss:
                    try:
                        output = self.model(batch_x, return_hidden=True)
                        if isinstance(output, tuple) and len(output) >= 2:
                            predictions = output[0]
                            hidden_features = output[-1]
                        else:
                            predictions = output if not isinstance(output, tuple) else output[0]
                            hidden_features = None
                    except TypeError as e:
                        if 'return_hidden' in str(e):
                            # åœ¨ _train_epoch ä¸­å·²ç»è®°å½•è¿‡è­¦å‘Šï¼Œè¿™é‡Œä¸é‡å¤
                            if not getattr(self, '_corr_loss_disabled_logged', False):
                                self.logger.warning(
                                    f"âš ï¸ æ¨¡å‹ä¸æ”¯æŒ return_hidden å‚æ•°ï¼Œç›¸å…³æ€§æ­£åˆ™åŒ–å·²è‡ªåŠ¨ç¦ç”¨ã€‚"
                                )
                            self._use_corr_loss = False
                            predictions = self.model(batch_x)
                            hidden_features = None
                        else:
                            raise
                    
                    if self._use_corr_loss and hidden_features is not None:
                        loss = self.criterion(predictions, batch_y, hidden_features)
                    else:
                        loss = self.criterion(predictions, batch_y)
                else:
                    predictions = self.model(batch_x)
                    loss = self.criterion(predictions, batch_y)
                
                total_loss += loss.item()
                n_batches += 1
        
        return total_loss / n_batches if n_batches > 0 else 0


class FineTunableModel(Model):
    """
    å¯å¾®è°ƒæ¨¡å‹ç±»
    
    å‚ç…§ Qlib çš„ ModelFT è®¾è®¡
    """
    
    @abc.abstractmethod
    def finetune(self, train_data, valid_data=None, **kwargs):
        """
        å¾®è°ƒæ¨¡å‹
        
        Args:
            train_data: è®­ç»ƒæ•°æ®
            valid_data: éªŒè¯æ•°æ®ï¼ˆå¯é€‰ï¼‰
            **kwargs: å…¶ä»–å¾®è°ƒå‚æ•°
        """
        raise NotImplementedError("finetune method must be implemented")


if __name__ == '__main__':
    print("=" * 80)
    print("Base Model Classes æµ‹è¯•")
    print("=" * 80)
    
    # æµ‹è¯•åŸºç±»å®šä¹‰
    print("\nâœ… BaseModel å®šä¹‰å®Œæˆ")
    print("âœ… Model å®šä¹‰å®Œæˆ")
    print("âœ… PyTorchModel å®šä¹‰å®Œæˆ")
    print("âœ… FineTunableModel å®šä¹‰å®Œæˆ")
    
    print("\næ¨¡å‹åŸºç±»ç³»ç»Ÿå·²å‡†å¤‡å°±ç»ªï¼")
