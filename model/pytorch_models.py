"""
PyTorch Models - PyTorch æ·±åº¦å­¦ä¹ æ¨¡å‹å®ç°

æä¾›å¸¸ç”¨çš„æ—¶åºé¢„æµ‹æ¨¡å‹

ğŸ†• é‡æ„è¯´æ˜ (2026-01):
- fit() æ–¹æ³•å·²ä»£ç†åˆ° model/train/SimpleTrainer
- ä¿æŒåŸæœ‰æ¥å£å…¼å®¹ï¼Œå†…éƒ¨ä½¿ç”¨ç»Ÿä¸€è®­ç»ƒå¼•æ“
"""

import torch
import torch.nn as nn
import pandas as pd
import numpy as np
from typing import Optional, Tuple
from pathlib import Path
from tqdm import tqdm

from .base_model import PyTorchModel
from .model_factory import register_model


# ==================== LSTM æ¨¡å‹ ====================

class LSTMNet(nn.Module):
    """LSTM ç¥ç»ç½‘ç»œ"""
    
    def __init__(self, d_feat: int, hidden_size: int, num_layers: int, dropout: float):
        super().__init__()
        
        self.lstm = nn.LSTM(
            input_size=d_feat,
            hidden_size=hidden_size,
            num_layers=num_layers,
            batch_first=True,
            dropout=dropout if num_layers > 1 else 0
        )
        
        self.fc = nn.Linear(hidden_size, 1)
        self.dropout = nn.Dropout(dropout)
    
    def forward(self, x, return_hidden: bool = False):
        """
        å‰å‘ä¼ æ’­
        
        Args:
            x: [batch, seq_len, features] è¾“å…¥åºåˆ—
            return_hidden: æ˜¯å¦è¿”å›éšè—ç‰¹å¾ï¼ˆç”¨äºç›¸å…³æ€§æ­£åˆ™åŒ–ï¼‰
            
        Returns:
            å¦‚æœ return_hidden=False: é¢„æµ‹å€¼ [batch]
            å¦‚æœ return_hidden=True: (é¢„æµ‹å€¼, éšè—ç‰¹å¾) å…ƒç»„
        """
        # x: [batch, seq_len, features]
        lstm_out, _ = self.lstm(x)
        # å–æœ€åä¸€ä¸ªæ—¶é—´æ­¥
        last_out = lstm_out[:, -1, :]  # [batch, hidden_size]
        
        if return_hidden:
            # è¿”å›å…ƒç»„ï¼š(é¢„æµ‹å€¼, éšè—ç‰¹å¾)
            out = self.dropout(last_out)
            pred = self.fc(out).squeeze(-1)
            return pred, last_out  # è¿”å›æœªç»è¿‡fcå‰çš„éšè—ç‰¹å¾
        else:
            out = self.dropout(last_out)
            return self.fc(out).squeeze(-1)


@register_model('lstm')
@register_model('LSTM')
class LSTMModel(PyTorchModel):
    """
    LSTM æ—¶åºé¢„æµ‹æ¨¡å‹
    
    Example:
        model = LSTMModel(
            d_feat=20,
            hidden_size=64,
            num_layers=2,
            dropout=0.1,
            n_epochs=100,
            lr=0.001
        )
        model.fit(train_loader, valid_loader)
        predictions = model.predict(test_loader)
    """
    
    def __init__(
        self,
        d_feat: int = 20,
        hidden_size: int = 64,
        num_layers: int = 2,
        dropout: float = 0.1,
        **kwargs
    ):
        """
        Args:
            d_feat: ç‰¹å¾ç»´åº¦
            hidden_size: LSTM éšè—å±‚å¤§å°
            num_layers: LSTM å±‚æ•°
            dropout: Dropout æ¦‚ç‡
        """
        super().__init__(**kwargs)
        
        self.d_feat = d_feat
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        self.dropout_rate = dropout
        
        # åˆ›å»ºæ¨¡å‹
        self.model = LSTMNet(d_feat, hidden_size, num_layers, dropout).to(self.device)
        
        # åˆ›å»ºä¼˜åŒ–å™¨å’ŒæŸå¤±å‡½æ•°
        self.optimizer = self._get_optimizer()
        self.criterion = self._get_loss_fn()
        
        self.logger.info(f"LSTM æ¨¡å‹å‚æ•°:")
        self.logger.info(f"  ç‰¹å¾ç»´åº¦: {d_feat}")
        self.logger.info(f"  éšè—å±‚å¤§å°: {hidden_size}")
        self.logger.info(f"  å±‚æ•°: {num_layers}")
        self.logger.info(f"  Dropout: {dropout}")
    
    def fit(self, train_loader, valid_loader=None, save_path: Optional[str] = None):
        """
        è®­ç»ƒæ¨¡å‹
        
        ğŸ†• é‡æ„: ä»£ç†åˆ° SimpleTrainerï¼Œä¿æŒæ¥å£å…¼å®¹
        
        Args:
            train_loader: è®­ç»ƒæ•°æ®åŠ è½½å™¨
            valid_loader: éªŒè¯æ•°æ®åŠ è½½å™¨
            save_path: æ¨¡å‹ä¿å­˜è·¯å¾„
            
        Returns:
            è®­ç»ƒç»“æœå­—å…¸ï¼ˆåŒ…å« train_losses, valid_losses, best_epoch ç­‰ï¼‰
        """
        self.logger.info("å¼€å§‹è®­ç»ƒ LSTM æ¨¡å‹...")
        
        # ğŸ†• ä½¿ç”¨ SimpleTrainer ä»£ç†è®­ç»ƒ
        from .train import SimpleTrainer, TrainerConfig
        
        trainer_config = TrainerConfig(
            n_epochs=self.n_epochs,
            lr=self.lr,
            early_stop=self.early_stop,
            optimizer=self.optimizer_name,
            loss_fn=self.loss_fn_name,
            loss_kwargs=self.loss_kwargs,
            use_scheduler=self.use_scheduler,
            scheduler_type=self.scheduler_type,
            scheduler_patience=self.scheduler_patience,
            scheduler_factor=self.scheduler_factor,
            scheduler_min_lr=self.scheduler_min_lr,
            lambda_corr=self.lambda_corr,
            checkpoint_dir=str(Path(save_path).parent) if save_path else None,
            save_best_only=True,
        )
        
        trainer = SimpleTrainer(self.model, trainer_config, str(self.device))
        result = trainer.train(train_loader, valid_loader, save_path=save_path)
        
        # åŒæ­¥è®­ç»ƒçŠ¶æ€åˆ°å½“å‰å®ä¾‹
        self.train_losses = result['train_losses']
        self.valid_losses = result['valid_losses']
        self.lr_history = result['lr_history']
        self.best_score = -result['best_score']  # SimpleTrainer å­˜çš„æ˜¯ lossï¼Œè½¬ä¸ºè´Ÿ
        self.best_epoch = result['best_epoch']
        self.fitted = True
        
        self.logger.info(f"è®­ç»ƒå®Œæˆ! æœ€ä½³ epoch: {self.best_epoch + 1}")
        
        return result
    
    # ğŸ†• predict() æ–¹æ³•å·²ç§»è‡³åŸºç±» PyTorchModelï¼ˆ2026-01-11 é‡æ„ï¼‰
    # å¦‚éœ€è‡ªå®šä¹‰å‰å‘é€»è¾‘ï¼Œå¯è¦†å†™ _forward_for_predict() é’©å­


# ==================== GRU æ¨¡å‹ ====================

class GRUNet(nn.Module):
    """GRU ç¥ç»ç½‘ç»œ"""
    
    def __init__(self, d_feat: int, hidden_size: int, num_layers: int, dropout: float):
        super().__init__()
        
        self.gru = nn.GRU(
            input_size=d_feat,
            hidden_size=hidden_size,
            num_layers=num_layers,
            batch_first=True,
            dropout=dropout if num_layers > 1 else 0
        )
        
        self.fc = nn.Linear(hidden_size, 1)
        self.dropout = nn.Dropout(dropout)
    
    def forward(self, x, return_hidden: bool = False):
        """
        å‰å‘ä¼ æ’­
        
        Args:
            x: [batch, seq_len, features] è¾“å…¥åºåˆ—
            return_hidden: æ˜¯å¦è¿”å›éšè—ç‰¹å¾ï¼ˆç”¨äºç›¸å…³æ€§æ­£åˆ™åŒ–ï¼‰
            
        Returns:
            å¦‚æœ return_hidden=False: é¢„æµ‹å€¼ [batch]
            å¦‚æœ return_hidden=True: (é¢„æµ‹å€¼, éšè—ç‰¹å¾) å…ƒç»„
        """
        gru_out, _ = self.gru(x)
        last_out = gru_out[:, -1, :]  # [batch, hidden_size]
        
        if return_hidden:
            out = self.dropout(last_out)
            pred = self.fc(out).squeeze(-1)
            return pred, last_out
        else:
            out = self.dropout(last_out)
            return self.fc(out).squeeze(-1)


@register_model('gru')
@register_model('GRU')
class GRUModel(PyTorchModel):
    """
    GRU æ—¶åºé¢„æµ‹æ¨¡å‹
    
    ç±»ä¼¼ LSTMï¼Œä½†å‚æ•°æ›´å°‘ï¼Œè®­ç»ƒæ›´å¿«
    """
    
    def __init__(
        self,
        d_feat: int = 20,
        hidden_size: int = 64,
        num_layers: int = 2,
        dropout: float = 0.1,
        **kwargs
    ):
        super().__init__(**kwargs)
        
        self.d_feat = d_feat
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        self.dropout_rate = dropout
        
        self.model = GRUNet(d_feat, hidden_size, num_layers, dropout).to(self.device)
        self.optimizer = self._get_optimizer()
        self.criterion = self._get_loss_fn()
        
        self.logger.info(f"GRU æ¨¡å‹å‚æ•°:")
        self.logger.info(f"  ç‰¹å¾ç»´åº¦: {d_feat}")
        self.logger.info(f"  éšè—å±‚å¤§å°: {hidden_size}")
        self.logger.info(f"  å±‚æ•°: {num_layers}")
    
    def fit(self, train_loader, valid_loader=None, save_path: Optional[str] = None):
        """
        è®­ç»ƒæ¨¡å‹
        
        ğŸ†• é‡æ„: ä»£ç†åˆ° SimpleTrainerï¼Œä¿æŒæ¥å£å…¼å®¹
        
        Args:
            train_loader: è®­ç»ƒæ•°æ®åŠ è½½å™¨
            valid_loader: éªŒè¯æ•°æ®åŠ è½½å™¨
            save_path: æ¨¡å‹ä¿å­˜è·¯å¾„
            
        Returns:
            è®­ç»ƒç»“æœå­—å…¸
        """
        self.logger.info("å¼€å§‹è®­ç»ƒ GRU æ¨¡å‹...")
        
        # ğŸ†• ä½¿ç”¨ SimpleTrainer ä»£ç†è®­ç»ƒ
        from .train import SimpleTrainer, TrainerConfig
        
        trainer_config = TrainerConfig(
            n_epochs=self.n_epochs,
            lr=self.lr,
            early_stop=self.early_stop,
            optimizer=self.optimizer_name,
            loss_fn=self.loss_fn_name,
            loss_kwargs=self.loss_kwargs,
            use_scheduler=self.use_scheduler,
            scheduler_type=self.scheduler_type,
            scheduler_patience=self.scheduler_patience,
            scheduler_factor=self.scheduler_factor,
            scheduler_min_lr=self.scheduler_min_lr,
            lambda_corr=self.lambda_corr,
            checkpoint_dir=str(Path(save_path).parent) if save_path else None,
            save_best_only=True,
        )
        
        trainer = SimpleTrainer(self.model, trainer_config, str(self.device))
        result = trainer.train(train_loader, valid_loader, save_path=save_path)
        
        # åŒæ­¥è®­ç»ƒçŠ¶æ€
        self.train_losses = result['train_losses']
        self.valid_losses = result['valid_losses']
        self.lr_history = result['lr_history']
        self.best_score = -result['best_score']
        self.best_epoch = result['best_epoch']
        self.fitted = True
        
        self.logger.info(f"è®­ç»ƒå®Œæˆ! æœ€ä½³ epoch: {self.best_epoch + 1}")
        
        return result
    
    # ğŸ†• predict() æ–¹æ³•å·²ç§»è‡³åŸºç±» PyTorchModelï¼ˆ2026-01-11 é‡æ„ï¼‰
    # ä¿®å¤ï¼šæ—§ç‰ˆ `for batch_x, _` æ— æ³•å¤„ç†å›¾/æ—¥çº§ loader çš„ unpack é”™è¯¯


# ==================== Transformer æ¨¡å‹ ====================

class TransformerNet(nn.Module):
    """Transformer ç¥ç»ç½‘ç»œ"""
    
    def __init__(
        self,
        d_feat: int,
        d_model: int,
        nhead: int,
        num_layers: int,
        dropout: float
    ):
        super().__init__()
        
        # è¾“å…¥æŠ•å½±
        self.input_proj = nn.Linear(d_feat, d_model)
        
        # Transformer Encoder
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=d_model,
            nhead=nhead,
            dim_feedforward=d_model * 4,
            dropout=dropout,
            batch_first=True
        )
        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers)
        
        # è¾“å‡ºå±‚
        self.fc = nn.Linear(d_model, 1)
        self.dropout = nn.Dropout(dropout)
    
    def forward(self, x, return_hidden: bool = False):
        """
        å‰å‘ä¼ æ’­
        
        Args:
            x: [batch, seq_len, features] è¾“å…¥åºåˆ—
            return_hidden: æ˜¯å¦è¿”å›éšè—ç‰¹å¾ï¼ˆç”¨äºç›¸å…³æ€§æ­£åˆ™åŒ–ï¼‰
            
        Returns:
            å¦‚æœ return_hidden=False: é¢„æµ‹å€¼ [batch]
            å¦‚æœ return_hidden=True: (é¢„æµ‹å€¼, éšè—ç‰¹å¾) å…ƒç»„
        """
        # x: [batch, seq_len, features]
        x = self.input_proj(x)
        x = self.transformer(x)
        # å–æœ€åä¸€ä¸ªæ—¶é—´æ­¥
        x = x[:, -1, :]  # [batch, d_model]
        
        if return_hidden:
            x_hidden = x.clone()  # ä¿å­˜dropoutå‰çš„éšè—ç‰¹å¾
            x = self.dropout(x)
            pred = self.fc(x).squeeze(-1)
            return pred, x_hidden
        else:
            x = self.dropout(x)
            return self.fc(x).squeeze(-1)


@register_model('transformer')
@register_model('Transformer')
class TransformerModel(PyTorchModel):
    """
    Transformer æ—¶åºé¢„æµ‹æ¨¡å‹
    
    ä½¿ç”¨è‡ªæ³¨æ„åŠ›æœºåˆ¶æ•æ‰é•¿æœŸä¾èµ–
    """
    
    def __init__(
        self,
        d_feat: int = 20,
        d_model: int = 64,
        nhead: int = 4,
        num_layers: int = 2,
        dropout: float = 0.1,
        **kwargs
    ):
        """
        Args:
            d_feat: ç‰¹å¾ç»´åº¦
            d_model: Transformer éšè—ç»´åº¦
            nhead: æ³¨æ„åŠ›å¤´æ•°
            num_layers: Transformer å±‚æ•°
            dropout: Dropout æ¦‚ç‡
        """
        super().__init__(**kwargs)
        
        self.d_feat = d_feat
        self.d_model = d_model
        self.nhead = nhead
        self.num_layers = num_layers
        self.dropout_rate = dropout
        
        self.model = TransformerNet(
            d_feat, d_model, nhead, num_layers, dropout
        ).to(self.device)
        
        self.optimizer = self._get_optimizer()
        self.criterion = self._get_loss_fn()
        
        self.logger.info(f"Transformer æ¨¡å‹å‚æ•°:")
        self.logger.info(f"  ç‰¹å¾ç»´åº¦: {d_feat}")
        self.logger.info(f"  éšè—ç»´åº¦: {d_model}")
        self.logger.info(f"  æ³¨æ„åŠ›å¤´æ•°: {nhead}")
        self.logger.info(f"  å±‚æ•°: {num_layers}")
    
    def fit(self, train_loader, valid_loader=None, save_path: Optional[str] = None):
        """è®­ç»ƒæ¨¡å‹"""
        self.logger.info("å¼€å§‹è®­ç»ƒ Transformer æ¨¡å‹...")
        
        # ğŸ†• åˆ›å»ºå­¦ä¹ ç‡è°ƒåº¦å™¨
        self.scheduler = self._get_scheduler()
        
        best_valid_loss = float('inf')
        patience_counter = 0
        
        for epoch in range(self.n_epochs):
            train_loss = self._train_epoch(train_loader)
            self.train_losses.append(train_loss)
            
            if valid_loader is not None:
                valid_loss = self._valid_epoch(valid_loader)
                self.valid_losses.append(valid_loss)
                
                self.logger.info(
                    f"Epoch {epoch+1}/{self.n_epochs} - "
                    f"Train Loss: {train_loss:.6f}, Valid Loss: {valid_loss:.6f}"
                )
                
                # ğŸ†• è°ƒç”¨å­¦ä¹ ç‡è°ƒåº¦å™¨
                self._step_scheduler(valid_loss)
                
                if valid_loss < best_valid_loss:
                    best_valid_loss = valid_loss
                    self.best_score = -valid_loss
                    self.best_epoch = epoch
                    patience_counter = 0
                    
                    if save_path:
                        self.save_model(save_path)
                else:
                    patience_counter += 1
                    if patience_counter >= self.early_stop:
                        self.logger.info(f"æ—©åœè§¦å‘ at epoch {epoch+1}")
                        break
            else:
                self.logger.info(f"Epoch {epoch+1}/{self.n_epochs} - Train Loss: {train_loss:.6f}")
                self._step_scheduler(train_loss)
                self.best_epoch = epoch
        
        self.fitted = True
        if valid_loader is not None:
            self.logger.info(f"è®­ç»ƒå®Œæˆ! æœ€ä½³ epoch: {self.best_epoch+1}")
        else:
            self.logger.info(f"è®­ç»ƒå®Œæˆ! (æ— éªŒè¯é›†ï¼Œå…± {self.n_epochs} epochs)")
    
    # ğŸ†• predict() æ–¹æ³•å·²ç§»è‡³åŸºç±» PyTorchModelï¼ˆ2026-01-11 é‡æ„ï¼‰


# ==================== VAE æ¨¡å‹ ====================

class VAENet(nn.Module):
    """VAE (Variational Autoencoder) ç¥ç»ç½‘ç»œ"""
    
    def __init__(
        self,
        input_dim: int,
        hidden_dim: int,
        latent_dim: int,
        window_size: int,
        dropout: float
    ):
        super().__init__()
        self.window_size = window_size
        self.hidden_dim = hidden_dim
        self.latent_dim = latent_dim
        
        # ç¼–ç å™¨ - GRU
        self.encoder_rnn = nn.GRU(
            input_size=input_dim,
            hidden_size=hidden_dim,
            num_layers=2,
            batch_first=True,
            dropout=dropout if dropout > 0 else 0
        )
        
        # VAE æ½œåœ¨ç©ºé—´
        self.fc_mu = nn.Linear(hidden_dim, latent_dim)
        self.fc_logvar = nn.Linear(hidden_dim, latent_dim)
        
        # è§£ç å™¨
        self.decoder = nn.Sequential(
            nn.Linear(latent_dim, hidden_dim),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim // 2, input_dim * window_size)
        )
        
        # é¢„æµ‹å¤´ - ä½¿ç”¨LayerNormé¿å…BatchNormçš„å•æ ·æœ¬é—®é¢˜
        # ç§»é™¤Tanhé™åˆ¶ï¼Œæ”¯æŒé¢„æµ‹ä»»æ„èŒƒå›´çš„Alphaæ”¶ç›Šç‡ï¼ˆç ”æŠ¥å¯¹é½ï¼šç«¯åˆ°ç«¯ç›‘ç£å­¦ä¹ ï¼‰
        self.predictor = nn.Sequential(
            nn.Linear(latent_dim, hidden_dim),
            nn.LayerNorm(hidden_dim),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim, 64),
            nn.LayerNorm(64),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(64, 32),
            nn.ReLU(),
            nn.Dropout(dropout * 0.5),
            nn.Linear(32, 1)
            # ç§»é™¤ nn.Tanh()ï¼šå…è®¸è¾“å‡ºä»»æ„èŒƒå›´çš„å€¼ï¼Œé€‚é…ä¸­æ€§åŒ–åçš„æ”¶ç›Šç‡æ ‡ç­¾
        )
        
        self._init_weights()
    
    def _init_weights(self):
        """åˆå§‹åŒ–æƒé‡"""
        for m in self.modules():
            if isinstance(m, nn.Linear):
                nn.init.xavier_normal_(m.weight, gain=0.1)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, (nn.GRU, nn.LSTM)):
                for param in m.parameters():
                    if len(param.shape) >= 2:
                        nn.init.orthogonal_(param)
                    else:
                        nn.init.zeros_(param)
            elif isinstance(m, nn.LayerNorm):
                nn.init.ones_(m.weight)
                nn.init.zeros_(m.bias)
    
    def encode(self, x):
        """ç¼–ç å™¨"""
        _, hidden = self.encoder_rnn(x)
        h = hidden[-1]
        mu = self.fc_mu(h)
        logvar = self.fc_logvar(h)
        logvar = torch.clamp(logvar, -10, 2)
        return mu, logvar
    
    def reparameterize(self, mu, logvar):
        """é‡å‚æ•°åŒ–æŠ€å·§"""
        if self.training:
            std = torch.exp(0.5 * logvar)
            eps = torch.randn_like(std)
            return mu + eps * std
        else:
            return mu
    
    def decode(self, z, window_size, input_dim):
        """è§£ç å™¨"""
        x_flat = self.decoder(z)
        x_recon = x_flat.view(-1, window_size, input_dim)
        return x_recon
    
    def predict(self, z):
        """é¢„æµ‹å¤´"""
        return self.predictor(z)
    
    def forward(self, x):
        """å‰å‘ä¼ æ’­"""
        batch_size, window_size, input_dim = x.shape
        mu, logvar = self.encode(x)
        z = self.reparameterize(mu, logvar)
        x_recon = self.decode(z, window_size, input_dim)
        y_pred = self.predict(z)
        return x_recon, y_pred.squeeze(), mu, logvar, z


@register_model('vae')
@register_model('VAE')
class VAEModel(PyTorchModel):
    """
    VAE (Variational Autoencoder) æ—¶åºé¢„æµ‹æ¨¡å‹
    
    ç»“åˆäº†è‡ªç¼–ç å™¨çš„ç‰¹å¾æå–èƒ½åŠ›å’Œå˜åˆ†æ¨æ–­çš„æ­£åˆ™åŒ–ä¼˜åŠ¿ï¼Œ
    ç‰¹åˆ«é€‚åˆç”¨äºå› å­æå–å’Œå¼‚å¸¸æ£€æµ‹ã€‚
    
    Example:
        model = VAEModel(
            d_feat=20,
            hidden_dim=128,
            latent_dim=16,
            window_size=40,
            n_epochs=50,
            alpha_recon=0.1,
            beta_kl=0.001,
            gamma_pred=1.0
        )
        model.fit(train_loader, valid_loader)
        predictions, latent_features = model.predict(test_loader, return_latent=True)
    """
    
    def __init__(
        self,
        d_feat: int = 20,
        hidden_dim: int = 128,
        latent_dim: int = 16,
        window_size: int = 40,
        dropout: float = 0.3,
        alpha_recon: float = 0.1,
        beta_kl: float = 0.001,
        gamma_pred: float = 1.0,
        **kwargs
    ):
        """
        Args:
            d_feat: ç‰¹å¾ç»´åº¦
            hidden_dim: éšè—å±‚å¤§å°
            latent_dim: æ½œåœ¨ç©ºé—´ç»´åº¦
            window_size: æ—¶é—´çª—å£å¤§å°
            dropout: Dropout æ¦‚ç‡
            alpha_recon: é‡æ„æŸå¤±æƒé‡
            beta_kl: KLæ•£åº¦æŸå¤±æƒé‡
            gamma_pred: é¢„æµ‹æŸå¤±æƒé‡
        """
        super().__init__(**kwargs)
        
        self.d_feat = d_feat
        self.hidden_dim = hidden_dim
        self.latent_dim = latent_dim
        self.window_size = window_size
        self.dropout_rate = dropout
        self.alpha_recon = alpha_recon
        self.beta_kl = beta_kl
        self.gamma_pred = gamma_pred
        
        # åˆ›å»ºæ¨¡å‹
        self.model = VAENet(
            d_feat, hidden_dim, latent_dim, window_size, dropout
        ).to(self.device)
        
        # åˆ›å»ºä¼˜åŒ–å™¨
        self.optimizer = self._get_optimizer()
        
        self.logger.info(f"VAE æ¨¡å‹å‚æ•°:")
        self.logger.info(f"  ç‰¹å¾ç»´åº¦: {d_feat}")
        self.logger.info(f"  éšè—ç»´åº¦: {hidden_dim}")
        self.logger.info(f"  æ½œåœ¨ç»´åº¦: {latent_dim}")
        self.logger.info(f"  çª—å£å¤§å°: {window_size}")
        self.logger.info(f"  æŸå¤±æƒé‡ - é‡æ„: {alpha_recon}, KL: {beta_kl}, é¢„æµ‹: {gamma_pred}")
    
    def _compute_loss(self, x_recon, x_true, y_pred, y_true, mu, logvar):
        """è®¡ç®— VAE æŸå¤±å‡½æ•°"""
        # é‡æ„æŸå¤±
        recon_loss = nn.functional.mse_loss(x_recon, x_true, reduction='mean')
        
        # KLæ•£åº¦æŸå¤±
        kl_loss = -0.5 * torch.mean(1 + logvar - mu.pow(2) - logvar.exp())
        
        # é¢„æµ‹æŸå¤±
        if y_pred.dim() == 0:
            y_pred = y_pred.unsqueeze(0)
        if y_true.dim() == 2:
            y_true = y_true.squeeze()
        pred_loss = nn.functional.mse_loss(y_pred, y_true, reduction='mean')
        
        # æ€»æŸå¤±
        total_loss = (
            self.alpha_recon * recon_loss + 
            self.beta_kl * kl_loss + 
            self.gamma_pred * pred_loss
        )
        
        return total_loss, recon_loss, kl_loss, pred_loss
    
    def _train_epoch(self, data_loader):
        """è®­ç»ƒä¸€ä¸ª epoch (é‡å†™ä»¥æ”¯æŒVAEæŸå¤±)"""
        self.model.train()
        total_loss = 0
        n_batches = 0
        
        for batch_x, batch_y in tqdm(data_loader, desc="è®­ç»ƒ", leave=False):
            batch_x = batch_x.to(self.device)
            batch_y = batch_y.to(self.device)
            
            self.optimizer.zero_grad()
            
            # å‰å‘ä¼ æ’­
            x_recon, y_pred, mu, logvar, z = self.model(batch_x)
            
            # è®¡ç®—æŸå¤±
            loss, _, _, _ = self._compute_loss(x_recon, batch_x, y_pred, batch_y, mu, logvar)
            
            # åå‘ä¼ æ’­
            loss.backward()
            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
            self.optimizer.step()
            
            total_loss += loss.item()
            n_batches += 1
        
        return total_loss / n_batches if n_batches > 0 else 0
    
    def _valid_epoch(self, data_loader):
        """éªŒè¯ä¸€ä¸ª epoch (é‡å†™ä»¥æ”¯æŒVAEæŸå¤±)"""
        self.model.eval()
        total_loss = 0
        n_batches = 0
        
        with torch.no_grad():
            for batch_x, batch_y in data_loader:
                batch_x = batch_x.to(self.device)
                batch_y = batch_y.to(self.device)
                
                x_recon, y_pred, mu, logvar, z = self.model(batch_x)
                loss, _, _, _ = self._compute_loss(x_recon, batch_x, y_pred, batch_y, mu, logvar)
                
                total_loss += loss.item()
                n_batches += 1
        
        return total_loss / n_batches if n_batches > 0 else 0
    
    def fit(self, train_loader, valid_loader=None, save_path: Optional[str] = None):
        """è®­ç»ƒæ¨¡å‹"""
        self.logger.info("å¼€å§‹è®­ç»ƒ VAE æ¨¡å‹...")
        
        # ğŸ†• åˆ›å»ºå­¦ä¹ ç‡è°ƒåº¦å™¨
        self.scheduler = self._get_scheduler()
        
        best_valid_loss = float('inf')
        patience_counter = 0
        
        for epoch in range(self.n_epochs):
            train_loss = self._train_epoch(train_loader)
            self.train_losses.append(train_loss)
            
            if valid_loader is not None:
                valid_loss = self._valid_epoch(valid_loader)
                self.valid_losses.append(valid_loss)
                
                self.logger.info(
                    f"Epoch {epoch+1}/{self.n_epochs} - "
                    f"Train Loss: {train_loss:.6f}, Valid Loss: {valid_loss:.6f}"
                )
                
                # ğŸ†• è°ƒç”¨å­¦ä¹ ç‡è°ƒåº¦å™¨
                self._step_scheduler(valid_loss)
                
                if valid_loss < best_valid_loss:
                    best_valid_loss = valid_loss
                    self.best_score = -valid_loss
                    self.best_epoch = epoch
                    patience_counter = 0
                    
                    if save_path:
                        self.save_model(save_path)
                else:
                    patience_counter += 1
                    if patience_counter >= self.early_stop:
                        self.logger.info(f"æ—©åœè§¦å‘ at epoch {epoch+1}")
                        break
            else:
                self.logger.info(f"Epoch {epoch+1}/{self.n_epochs} - Train Loss: {train_loss:.6f}")
                self._step_scheduler(train_loss)
                self.best_epoch = epoch
        
        self.fitted = True
        if valid_loader is not None:
            self.logger.info(f"è®­ç»ƒå®Œæˆ! æœ€ä½³ epoch: {self.best_epoch+1}")
        else:
            self.logger.info(f"è®­ç»ƒå®Œæˆ! (æ— éªŒè¯é›†ï¼Œå…± {self.n_epochs} epochs)")
    
    def predict(self, test_loader, return_numpy: bool = True, return_latent: bool = False):
        """
        é¢„æµ‹ï¼ˆğŸ†• ç²¾ç®€ç‰ˆï¼šreturn_latent=False æ—¶å¤ç”¨åŸºç±»å®ç°ï¼‰
        
        Args:
            test_loader: æµ‹è¯•æ•°æ®åŠ è½½å™¨
            return_numpy: æ˜¯å¦è¿”å› numpy æ•°ç»„
            return_latent: æ˜¯å¦åŒæ—¶è¿”å›æ½œåœ¨ç‰¹å¾
            
        Returns:
            å¦‚æœ return_latent=False: é¢„æµ‹ç»“æœ
            å¦‚æœ return_latent=True: (é¢„æµ‹ç»“æœ, æ½œåœ¨ç‰¹å¾)
        """
        # ğŸ†• æ–¹æ¡ˆ Aï¼šä¸éœ€è¦æ½œå˜é‡æ—¶ï¼Œç›´æ¥å¤ç”¨åŸºç±»å®ç°
        if not return_latent:
            return super().predict(test_loader, return_numpy)
        
        # éœ€è¦æ½œå˜é‡æ—¶ï¼Œä½¿ç”¨è‡ªå®šä¹‰é€»è¾‘
        if not self.fitted:
            raise ValueError("æ¨¡å‹æœªè®­ç»ƒï¼Œè¯·å…ˆè°ƒç”¨ fit()")
        
        self.model.eval()
        predictions = []
        latent_features = []
        
        with torch.no_grad():
            for batch_data in test_loader:
                batch_x, _, _, _ = self._parse_batch_data(batch_data)
                batch_x = batch_x.to(self.device)
                _, y_pred, _, _, z = self.model(batch_x)
                predictions.append(y_pred.cpu())
                latent_features.append(z.cpu())
        
        # ç©ºå¤„ç†
        if len(predictions) == 0:
            import numpy as np
            empty = np.array([]) if return_numpy else torch.tensor([])
            return empty, empty
        
        predictions = torch.cat(predictions, dim=0)
        latent_features = torch.cat(latent_features, dim=0)
        
        if return_numpy:
            return predictions.numpy(), latent_features.numpy()
        return predictions, latent_features
    
    def _forward_for_predict(self, x, adj=None, idx=None):
        """ğŸ†• VAE é¢„æµ‹å‰å‘é’©å­ - ä»…è¿”å› y_pred"""
        _, y_pred, _, _, _ = self.model(x)
        return y_pred
    
    def extract_latent(self, test_loader, return_numpy: bool = True):
        """
        æå–æ½œåœ¨ç‰¹å¾ï¼ˆç”¨äºå› å­ç”Ÿæˆï¼‰
        
        ğŸ†• 2026-01-11 ä¿®å¤ï¼šä½¿ç”¨ _parse_batch_data æ”¯æŒå›¾/æ—¥çº§ loader
        
        Args:
            test_loader: æ•°æ®åŠ è½½å™¨ï¼ˆæ”¯æŒ (x,y), (x,y,adj,...), dict ç­‰æ ¼å¼ï¼‰
            return_numpy: æ˜¯å¦è¿”å› numpy æ•°ç»„
            
        Returns:
            æ½œåœ¨ç‰¹å¾ (mu, z)
        """
        if not self.fitted:
            raise ValueError("æ¨¡å‹æœªè®­ç»ƒï¼Œè¯·å…ˆè°ƒç”¨ fit()")
        
        self.model.eval()
        mu_list = []
        z_list = []
        
        with torch.no_grad():
            for batch_data in test_loader:
                # ğŸ†• ä½¿ç”¨åŸºç±»ç»Ÿä¸€çš„ batch è§£æï¼ˆæ›¿ä»£ for batch_x, _ in ...)
                batch_x, _, _, _ = self._parse_batch_data(batch_data)
                
                batch_x = batch_x.to(self.device)
                mu, logvar = self.model.encode(batch_x)
                z = self.model.reparameterize(mu, logvar)
                mu_list.append(mu.cpu())
                z_list.append(z.cpu())
        
        # å¤„ç†ç©ºè¾“å…¥
        if len(mu_list) == 0:
            import numpy as np
            empty = np.array([]) if return_numpy else torch.tensor([])
            return empty, empty
        
        mu_features = torch.cat(mu_list, dim=0)
        z_features = torch.cat(z_list, dim=0)
        
        if return_numpy:
            return mu_features.numpy(), z_features.numpy()
        return mu_features, z_features


if __name__ == '__main__':
    print("=" * 80)
    print("PyTorch Models æµ‹è¯•")
    print("=" * 80)
    
    # æµ‹è¯•æ¨¡å‹åˆ›å»º
    print("\n1. æµ‹è¯• LSTM æ¨¡å‹åˆ›å»º:")
    lstm_model = LSTMModel(
        d_feat=20,
        hidden_size=64,
        num_layers=2,
        n_epochs=10,
        batch_size=256
    )
    print(f"âœ… LSTM æ¨¡å‹åˆ›å»ºæˆåŠŸ")
    
    print("\n2. æµ‹è¯• GRU æ¨¡å‹åˆ›å»º:")
    gru_model = GRUModel(
        d_feat=20,
        hidden_size=64,
        num_layers=2
    )
    print(f"âœ… GRU æ¨¡å‹åˆ›å»ºæˆåŠŸ")
    
    print("\n3. æµ‹è¯• Transformer æ¨¡å‹åˆ›å»º:")
    transformer_model = TransformerModel(
        d_feat=20,
        d_model=64,
        nhead=4,
        num_layers=2
    )
    print(f"âœ… Transformer æ¨¡å‹åˆ›å»ºæˆåŠŸ")
    
    print("\n4. æµ‹è¯• VAE æ¨¡å‹åˆ›å»º:")
    vae_model = VAEModel(
        d_feat=20,
        hidden_dim=128,
        latent_dim=16,
        window_size=40,
        n_epochs=10
    )
    print(f"âœ… VAE æ¨¡å‹åˆ›å»ºæˆåŠŸ")
    
    # æµ‹è¯•æ¨¡å‹æ³¨å†Œ
    from model_factory import ModelRegistry
    print(f"\nå·²æ³¨å†Œçš„æ¨¡å‹: {ModelRegistry.list_models()}")
    
    print("\nâœ… PyTorch Models æµ‹è¯•å®Œæˆ")
