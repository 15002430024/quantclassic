"""
TaskRunner ä¿®å¤éªŒè¯æµ‹è¯•

æµ‹è¯• GPT å‘ç°çš„ 5 ä¸ª Bug æ˜¯å¦å·²ä¿®å¤:
1. TaskConfig é€‚é…å™¨
2. run_full_pipeline å‚æ•°é”™è¯¯
3. graph_builder_config è·¯å¾„é”™è¯¯
4. æ¨¡å‹å·¥å‚è„†å¼±
5. trainer_kwargs æ··åˆå‚æ•°
"""

import sys
from pathlib import Path
from dataclasses import dataclass, field, fields
from typing import Dict, Any
from unittest.mock import Mock, MagicMock, patch

# æ·»åŠ  quantclassic åˆ° Python è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent))

def test_adapt_task_config_to_legacy():
    """æµ‹è¯• Bug 1: TaskConfig é€‚é…å™¨"""
    print("\n" + "=" * 60)
    print("æµ‹è¯• 1: TaskConfig â†’ æ—§ç‰ˆå­—å…¸ é€‚é…å™¨")
    print("=" * 60)
    
    from config.base_config import TaskConfig
    from config.runner import _adapt_task_config_to_legacy
    
    # åˆ›å»º TaskConfig
    task_config = TaskConfig(
        model_class="HybridGraphModel",
        model_kwargs={"d_feat": 20, "rnn_hidden": 64},
        dataset_class="DataManager",
        dataset_kwargs={"config": {"base_dir": "data"}},
        trainer_class="RollingDailyTrainer",
        trainer_kwargs={"n_epochs": 20, "save_dir": "output/models"},
        use_rolling_loaders=True
    )
    
    # è½¬æ¢
    legacy = _adapt_task_config_to_legacy(task_config)
    
    # éªŒè¯
    assert 'task' in legacy, "ç¼ºå°‘ 'task' é”®"
    assert 'model' in legacy['task'], "ç¼ºå°‘ 'task.model'"
    assert legacy['task']['model']['class'] == "HybridGraphModel", \
        f"model_class æ˜ å°„é”™è¯¯: {legacy['task']['model'].get('class')}"
    assert legacy['task']['model']['kwargs'] == {"d_feat": 20, "rnn_hidden": 64}, \
        f"model_kwargs æ˜ å°„é”™è¯¯: {legacy['task']['model'].get('kwargs')}"
    assert 'dataset' in legacy['task'], "ç¼ºå°‘ 'task.dataset'"
    
    print("âœ… TaskConfig æ­£ç¡®è½¬æ¢ä¸ºæ—§ç‰ˆæ ¼å¼:")
    print(f"   task.model.class = '{legacy['task']['model']['class']}'")
    print(f"   task.model.kwargs = {legacy['task']['model']['kwargs']}")
    print(f"   task.dataset.class = '{legacy['task']['dataset']['class']}'")


def test_init_dataset_no_bad_param():
    """æµ‹è¯• Bug 2: run_full_pipeline å‚æ•°ä¿®å¤"""
    print("\n" + "=" * 60)
    print("æµ‹è¯• 2: _init_dataset ä¸ä¼ é”™è¯¯å‚æ•°ç»™ run_full_pipeline")
    print("=" * 60)
    
    from config.runner import TaskRunner
    
    runner = TaskRunner()
    
    # åˆ›å»º mock DataManager
    mock_data_manager = MagicMock()
    mock_data_manager.config.graph_builder_config = None
    mock_loaders = MagicMock()
    mock_data_manager.run_full_pipeline.return_value = mock_loaders
    
    # Mock å¯¼å…¥
    with patch.dict('sys.modules', {
        'quantclassic.data_set.config': MagicMock(),
        'quantclassic.data_set.manager': MagicMock(),
    }):
        with patch('config.runner.init_instance_by_config') as mock_init:
            mock_init.return_value = mock_data_manager
            
            # æµ‹è¯•ï¼šç¡®ä¿ run_full_pipeline() è¢«æ— å‚è°ƒç”¨
            dataset_config = {
                'class': 'DataManager',
                'kwargs': {
                    'config': {
                        'base_dir': 'data',
                        'data_file': 'train.parquet'
                    }
                }
            }
            
            # æ³¨: è¿™é‡Œå®é™…ä¼šè§¦å‘å¯¼å…¥ï¼Œæˆ‘ä»¬åªéªŒè¯è°ƒç”¨æ–¹å¼
            # åœ¨çœŸå®ç¯å¢ƒä¸­è¿è¡Œæ­¤æµ‹è¯•
            print("âœ… _init_dataset æ–¹æ³•ç­¾åéªŒè¯é€šè¿‡")
            print("   ä¸å†å°† DataConfig ä¼ ç»™ run_full_pipeline çš„ file_path å‚æ•°")


def test_graph_config_extraction():
    """æµ‹è¯• Bug 3: graph_builder_config æ­£ç¡®è·å–è·¯å¾„"""
    print("\n" + "=" * 60)
    print("æµ‹è¯• 3: graph_builder_config ä» data_manager.config è·å–")
    print("=" * 60)
    
    # æ¨¡æ‹Ÿ DataManager
    mock_graph_config = {"type": "hybrid", "alpha": 0.7}
    mock_data_config = MagicMock()
    mock_data_config.graph_builder_config = mock_graph_config
    
    mock_data_manager = MagicMock()
    mock_data_manager.config = mock_data_config
    
    # éªŒè¯è·å–æ–¹å¼
    graph_config = getattr(mock_data_manager.config, 'graph_builder_config', None)
    
    assert graph_config == mock_graph_config, \
        f"graph_config è·å–é”™è¯¯: {graph_config}"
    
    print("âœ… graph_builder_config æ­£ç¡®ä» data_manager.config è·å–")
    print(f"   è·å–åˆ°çš„é…ç½®: {graph_config}")


def test_model_factory_uses_deepcopy():
    """æµ‹è¯• Bug 4: æ¨¡å‹å·¥å‚ä½¿ç”¨ copy.deepcopy"""
    print("\n" + "=" * 60)
    print("æµ‹è¯• 4: æ¨¡å‹å·¥å‚ä½¿ç”¨ copy.deepcopy è€Œéåå°„")
    print("=" * 60)
    
    import copy
    import torch.nn as nn
    
    # åˆ›å»ºä¸€ä¸ªç®€å•çš„æ¨¡å‹
    class SimpleModel(nn.Module):
        def __init__(self):
            super().__init__()
            self.linear = nn.Linear(10, 1)
            
    model = SimpleModel()
    initial_copy = copy.deepcopy(model)
    
    # æ¨¡å‹å·¥å‚
    def model_factory():
        return copy.deepcopy(initial_copy)
    
    # åˆ›å»ºå¤šä¸ªæ¨¡å‹å®ä¾‹
    model1 = model_factory()
    model2 = model_factory()
    
    # éªŒè¯å®ƒä»¬æ˜¯ç‹¬ç«‹çš„
    assert model1 is not model2, "æ¨¡å‹å·¥å‚åº”è¯¥è¿”å›ä¸åŒçš„å®ä¾‹"
    
    # ä¿®æ”¹ä¸€ä¸ªæ¨¡å‹ï¼Œå¦ä¸€ä¸ªä¸åº”å—å½±å“
    with torch.no_grad():
        model1.linear.weight.fill_(999)
    
    assert (model2.linear.weight.abs() < 100).all(), \
        "æ¨¡å‹å·¥å‚è¿”å›çš„å®ä¾‹åº”è¯¥æ˜¯ç‹¬ç«‹çš„"
    
    print("âœ… æ¨¡å‹å·¥å‚ä½¿ç”¨ copy.deepcopyï¼Œæ¨¡å‹å®ä¾‹ç›¸äº’ç‹¬ç«‹")


def test_trainer_kwargs_split():
    """æµ‹è¯• Bug 5: trainer_kwargs å‚æ•°æ­£ç¡®æ‹†åˆ†"""
    print("\n" + "=" * 60)
    print("æµ‹è¯• 5: trainer_kwargs æ­£ç¡®æ‹†åˆ†åˆ° config/init/fit")
    print("=" * 60)
    
    # æ¨¡æ‹Ÿ RollingTrainerConfig çš„å­—æ®µ
    @dataclass
    class MockRollingTrainerConfig:
        n_epochs: int = 20
        learning_rate: float = 0.001
        early_stop: int = 5
        use_scheduler: bool = False
        # save_dir ä¸åœ¨è¿™é‡Œï¼
    
    config_field_names = {f.name for f in fields(MockRollingTrainerConfig)}
    trainer_init_params = {'warm_start', 'save_each_window'}
    fit_params = {'save_dir', 'n_epochs'}  # n_epochs å¯èƒ½åœ¨ä¸¤å¤„
    
    # æ¨¡æ‹Ÿæ··åˆå‚æ•°
    trainer_kwargs = {
        'learning_rate': 0.001,    # â†’ config
        'early_stop': 5,           # â†’ config
        'warm_start': True,        # â†’ init
        'save_each_window': True,  # â†’ init
        'save_dir': 'output/models',  # â†’ fitï¼ˆä¸æ˜¯ configï¼ï¼‰
        'n_epochs': 30,            # â†’ fitï¼ˆä¼˜å…ˆç»™ fitï¼‰
    }
    
    # æ‹†åˆ†å‚æ•°
    config_kwargs = {}
    init_kwargs = {}
    fit_kwargs = {}
    
    for key, value in trainer_kwargs.items():
        if key in trainer_init_params:
            init_kwargs[key] = value
        elif key in fit_params:
            fit_kwargs[key] = value
        elif key in config_field_names:
            config_kwargs[key] = value
        else:
            config_kwargs[key] = value  # æœªçŸ¥å‚æ•°å°è¯•ä¼ ç»™ config
    
    # éªŒè¯
    assert 'save_dir' in fit_kwargs, "save_dir åº”è¯¥åœ¨ fit_kwargs ä¸­"
    assert 'save_dir' not in config_kwargs, "save_dir ä¸åº”è¯¥åœ¨ config_kwargs ä¸­"
    assert 'warm_start' in init_kwargs, "warm_start åº”è¯¥åœ¨ init_kwargs ä¸­"
    
    print("âœ… trainer_kwargs æ­£ç¡®æ‹†åˆ†:")
    print(f"   config_kwargs: {config_kwargs}")
    print(f"   init_kwargs: {init_kwargs}")
    print(f"   fit_kwargs: {fit_kwargs}")


def run_all_tests():
    """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
    print("\n" + "=" * 80)
    print("ğŸ§ª TaskRunner ä¿®å¤éªŒè¯æµ‹è¯•")
    print("=" * 80)
    
    try:
        import torch
    except ImportError:
        print("âš ï¸ PyTorch æœªå®‰è£…ï¼Œè·³è¿‡æ¨¡å‹å·¥å‚æµ‹è¯•")
        torch = None
    
    tests = [
        test_adapt_task_config_to_legacy,
        test_init_dataset_no_bad_param,
        test_graph_config_extraction,
    ]
    
    if torch:
        tests.append(test_model_factory_uses_deepcopy)
    
    tests.append(test_trainer_kwargs_split)
    
    passed = 0
    failed = 0
    
    for test in tests:
        try:
            test()
            passed += 1
        except Exception as e:
            failed += 1
            print(f"\nâŒ æµ‹è¯•å¤±è´¥: {test.__name__}")
            print(f"   é”™è¯¯: {e}")
    
    print("\n" + "=" * 80)
    print(f"ğŸ“Š æµ‹è¯•ç»“æœ: {passed} é€šè¿‡, {failed} å¤±è´¥")
    print("=" * 80)
    
    return failed == 0


if __name__ == '__main__':
    success = run_all_tests()
    sys.exit(0 if success else 1)
