"""
TaskRunner - ä»»åŠ¡è¿è¡Œå™¨

æ”¯æŒä¸¤ç§é…ç½®æ¨¡å¼:
1. å­—å…¸é…ç½® (å‘åå…¼å®¹): {'task': {'model': {...}, 'dataset': {...}}}
2. Dataclass é…ç½® (æ–°æ¨¡å¼): TaskConfig å¯¹è±¡

æ”¯æŒå¤šç§è®­ç»ƒæ¨¡å¼:
1. é»˜è®¤è®­ç»ƒ: ä½¿ç”¨æ¨¡å‹çš„ fit æ–¹æ³•
2. æ»šåŠ¨çª—å£è®­ç»ƒ: ä½¿ç”¨ RollingDailyTrainer
3. åŠ¨æ€å›¾è®­ç»ƒ: ä½¿ç”¨ DynamicGraphTrainer
"""

import logging
import copy
from typing import Dict, Any, Optional, Union
from pathlib import Path
from dataclasses import fields

from .utils import init_instance_by_config
from .base_config import BaseConfig, TaskConfig


def _is_dataclass_config(obj) -> bool:
    """æ£€æŸ¥å¯¹è±¡æ˜¯å¦ä¸º BaseConfig å­ç±»å®ä¾‹"""
    return isinstance(obj, BaseConfig)


def _config_to_dict(config) -> Dict[str, Any]:
    """å°†é…ç½®å¯¹è±¡è½¬æ¢ä¸ºå­—å…¸ï¼ˆæ”¯æŒ BaseConfig å’Œæ™®é€šå­—å…¸ï¼‰"""
    if _is_dataclass_config(config):
        return config.to_dict()
    elif isinstance(config, dict):
        return config
    else:
        raise TypeError(f"ä¸æ”¯æŒçš„é…ç½®ç±»å‹: {type(config)}")


def _adapt_task_config_to_legacy(task_config: TaskConfig) -> Dict[str, Any]:
    """
    ğŸ†• TaskConfig é€‚é…å™¨
    
    å°†æ–°ç‰ˆ TaskConfigï¼ˆæ‰å¹³ç»“æ„ï¼‰è½¬æ¢ä¸ºæ—§ç‰ˆ Runner æœŸæœ›çš„åµŒå¥—å­—å…¸ç»“æ„
    
    TaskConfig ç»“æ„:
        model_class, model_kwargs, dataset_class, dataset_kwargs, ...
        
    æ—§ç‰ˆç»“æ„:
        {'task': {'model': {'class': ..., 'kwargs': ...}, 'dataset': {...}}}
    
    Args:
        task_config: TaskConfig å¯¹è±¡
        
    Returns:
        æ—§ç‰ˆæ ¼å¼çš„é…ç½®å­—å…¸
    """
    legacy_config = {'task': {}}
    
    # è½¬æ¢æ¨¡å‹é…ç½®
    if task_config.model_class:
        legacy_config['task']['model'] = {
            'class': task_config.model_class,
            'module_path': 'quantclassic.model',  # é»˜è®¤æ¨¡å—è·¯å¾„
            'kwargs': task_config.model_kwargs or {}
        }
    
    # è½¬æ¢æ•°æ®é›†é…ç½®
    if task_config.dataset_class:
        legacy_config['task']['dataset'] = {
            'class': task_config.dataset_class,
            'module_path': 'quantclassic.data_set',  # é»˜è®¤æ¨¡å—è·¯å¾„
            'kwargs': task_config.dataset_kwargs or {}
        }
    
    # è½¬æ¢å›æµ‹é…ç½®
    if task_config.backtest_enabled:
        legacy_config['task']['backtest'] = task_config.backtest_kwargs or {}
    
    return legacy_config


class TaskRunner:
    """ä»»åŠ¡è¿è¡Œå™¨ - æ‰§è¡Œé…ç½®å®šä¹‰çš„å®Œæ•´å·¥ä½œæµ"""
    
    def __init__(self, log_level: str = 'INFO'):
        self.logger = self._setup_logger(log_level)
        # ğŸ†• ä¿å­˜æ¨¡å‹é…ç½®ç”¨äºé‡å»ºæ¨¡å‹å·¥å‚
        self._model_config: Optional[Dict[str, Any]] = None
    
    def _setup_logger(self, log_level: str) -> logging.Logger:
        logger = logging.getLogger('TaskRunner')
        logger.setLevel(getattr(logging, log_level))
        
        if not logger.handlers:
            handler = logging.StreamHandler()
            formatter = logging.Formatter(
                '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
            )
            handler.setFormatter(formatter)
            logger.addHandler(handler)
        
        return logger
    
    def run(self, 
            config: Union[Dict[str, Any], TaskConfig],
            experiment_name: str = 'default_experiment',
            recorder_name: Optional[str] = None) -> Dict[str, Any]:
        """
        è¿è¡Œå®Œæ•´çš„è®­ç»ƒä»»åŠ¡
        
        Args:
            config: é…ç½®å­—å…¸æˆ– TaskConfig å¯¹è±¡
            experiment_name: å®éªŒåç§°
            recorder_name: è®°å½•å™¨åç§°
            
        Returns:
            è®­ç»ƒç»“æœå­—å…¸
        """
        self.logger.info(f"å¼€å§‹è¿è¡Œä»»åŠ¡: {experiment_name}")
        
        # ğŸ†• ç»Ÿä¸€å¤„ç†é…ç½®æ ¼å¼ - ä½¿ç”¨é€‚é…å™¨
        task_config: Optional[TaskConfig] = None
        
        if isinstance(config, TaskConfig):
            # æ–°ç‰ˆ TaskConfig å¯¹è±¡ â†’ è½¬æ¢ä¸ºæ—§ç‰ˆå­—å…¸
            self.logger.info("æ£€æµ‹åˆ° TaskConfig å¯¹è±¡ï¼Œä½¿ç”¨é€‚é…å™¨è½¬æ¢...")
            task_config = config
            config_dict = _adapt_task_config_to_legacy(config)
            self.logger.debug(f"è½¬æ¢åçš„é…ç½®: {config_dict}")
        elif _is_dataclass_config(config):
            # å…¶ä»– BaseConfig å­ç±»
            self.logger.info("æ£€æµ‹åˆ° BaseConfig å¯¹è±¡ï¼Œè½¬æ¢ä¸ºå­—å…¸...")
            config_dict = {'task': _config_to_dict(config)}
        elif isinstance(config, dict):
            # æ—§ç‰ˆå­—å…¸é…ç½®
            config_dict = config
            # å°è¯•ä»å­—å…¸æ„å»º TaskConfigï¼ˆç”¨äºæ£€æµ‹é«˜çº§åŠŸèƒ½ï¼‰
            if 'task' in config_dict:
                try:
                    task_config = TaskConfig.from_dict(config_dict['task'])
                except Exception:
                    pass  # è€æ ¼å¼é…ç½®ï¼Œä¸å¼ºåˆ¶è½¬æ¢
        else:
            raise TypeError(f"ä¸æ”¯æŒçš„é…ç½®ç±»å‹: {type(config)}")
        
        # éªŒè¯é…ç½®ç»“æ„
        if 'task' not in config_dict:
            raise ValueError("é…ç½®å¿…é¡»åŒ…å« 'task' é”®")
        
        try:
            from ..workflow import R
            use_recorder = True
            self.logger.info("ä½¿ç”¨ workflow.R è®°å½•å®éªŒ")
        except ImportError:
            use_recorder = False
            self.logger.warning("workflowæ¨¡å—ä¸å¯ç”¨")
        
        if use_recorder:
            ctx = R.start(experiment_name=experiment_name, recorder_name=recorder_name)
            ctx.__enter__()
            R.log_params(**self._flatten_config(config_dict.get('task', {})))
        
        try:
            dataset = None
            data_manager = None
            rolling_loaders = None
            daily_loaders = None
            
            # ==================== æ­¥éª¤ 1: åˆå§‹åŒ–æ•°æ®é›† ====================
            if 'dataset' in config_dict['task']:
                self.logger.info("æ­¥éª¤ 1/4: åˆå§‹åŒ–æ•°æ®é›†...")
                dataset, data_manager = self._init_dataset(config_dict['task']['dataset'])
                self.logger.info(f"æ•°æ®é›†åˆå§‹åŒ–å®Œæˆ: {type(dataset).__name__}")
                
                # ğŸ†• æ£€æµ‹æ˜¯å¦éœ€è¦åˆ›å»ºæ»šåŠ¨/æ—¥æ‰¹æ¬¡åŠ è½½å™¨
                # ç›´æ¥ä» data_manager.config è·å– graph_builder_configï¼Œä¸ä»åµŒå¥—å­—å…¸æŠ 
                if task_config and data_manager is not None:
                    graph_config = getattr(data_manager.config, 'graph_builder_config', None)
                    
                    if task_config.use_rolling_loaders:
                        self.logger.info("åˆ›å»ºæ»šåŠ¨çª—å£æ—¥æ‰¹æ¬¡åŠ è½½å™¨...")
                        rolling_loaders = data_manager.create_rolling_daily_loaders(
                            graph_builder_config=graph_config
                        )
                        self.logger.info(f"æ»šåŠ¨çª—å£æ•°é‡: {len(rolling_loaders)}")
                    elif task_config.use_daily_loaders:
                        self.logger.info("åˆ›å»ºæ—¥æ‰¹æ¬¡åŠ è½½å™¨...")
                        daily_loaders = data_manager.create_daily_loaders(
                            graph_builder_config=graph_config
                        )
                        self.logger.info("æ—¥æ‰¹æ¬¡åŠ è½½å™¨åˆ›å»ºå®Œæˆ")
            
            # ==================== æ­¥éª¤ 2: åˆå§‹åŒ–æ¨¡å‹ ====================
            model = None
            if 'model' in config_dict['task']:
                self.logger.info("æ­¥éª¤ 2/4: åˆå§‹åŒ–æ¨¡å‹...")
                # ğŸ†• ä¿å­˜æ¨¡å‹é…ç½®ç”¨äºåç»­é‡å»º
                self._model_config = config_dict['task']['model']
                model = self._init_model(config_dict['task']['model'])
                self.logger.info(f"æ¨¡å‹åˆå§‹åŒ–å®Œæˆ: {type(model).__name__}")
            
            # ==================== æ­¥éª¤ 3: è®­ç»ƒæ¨¡å‹ ====================
            train_results = {}
            if model is not None and dataset is not None:
                self.logger.info("æ­¥éª¤ 3/4: è®­ç»ƒæ¨¡å‹...")
                
                # ğŸ†• æ ¹æ®é…ç½®é€‰æ‹©è®­ç»ƒæ–¹å¼ (æ”¯æŒæ–°è®­ç»ƒæ¶æ„)
                trainer_class = task_config.trainer_class if task_config else ''
                trainer_kwargs = task_config.trainer_kwargs if task_config else {}
                
                if trainer_class == 'RollingDailyTrainer' and rolling_loaders:
                    train_results = self._train_rolling(
                        model, rolling_loaders, trainer_kwargs or {}
                    )
                elif trainer_class == 'RollingWindowTrainer' and rolling_loaders:
                    # ğŸ†• æ–°å¢: æ”¯æŒ RollingWindowTrainer
                    train_results = self._train_rolling_window(
                        model, rolling_loaders, trainer_kwargs or {}
                    )
                elif trainer_class == 'SimpleTrainer':
                    # ğŸ†• æ–°å¢: æ”¯æŒ SimpleTrainer
                    train_results = self._train_simple(
                        model, dataset, trainer_kwargs or {}
                    )
                elif trainer_class == 'DynamicGraphTrainer' and daily_loaders:
                    # âš ï¸ DynamicGraphTrainer å·²åºŸå¼ƒï¼Œå†…éƒ¨æ”¹ç”¨ SimpleTrainer
                    self.logger.warning(
                        "âš ï¸ trainer_class='DynamicGraphTrainer' å·²åºŸå¼ƒï¼\n"
                        "   å®é™…ä½¿ç”¨ SimpleTrainer æ‰§è¡Œã€‚å»ºè®®æ”¹ç”¨ trainer_class='SimpleTrainer'ã€‚"
                    )
                    train_results = self._train_dynamic_graph(
                        model, daily_loaders, trainer_kwargs or {}
                    )
                else:
                    train_results = self._train_model(model, dataset, config_dict['task'])
                
                self.logger.info("æ¨¡å‹è®­ç»ƒå®Œæˆ")
                
                if use_recorder and train_results:
                    R.log_metrics(**train_results.get('metrics', {}))
                    if 'model' in train_results:
                        R.save_objects(model=train_results['model'])
            
            # ==================== æ­¥éª¤ 4: å›æµ‹ ====================
            backtest_results = {}
            if 'backtest' in config_dict['task']:
                self.logger.info("æ­¥éª¤ 4/4: æ‰§è¡Œå›æµ‹...")
                backtest_results = self._run_backtest(model, dataset, config_dict['task']['backtest'])
                self.logger.info("å›æµ‹å®Œæˆ")
                
                if use_recorder and backtest_results:
                    R.log_metrics(**backtest_results.get('metrics', {}))
            
            results = {
                'model': model,
                'dataset': dataset,
                'data_manager': data_manager,
                'rolling_loaders': rolling_loaders,
                'daily_loaders': daily_loaders,
                'train_results': train_results,
                'backtest_results': backtest_results,
                'experiment_name': experiment_name
            }
            
            if use_recorder:
                R.save_objects(config=config_dict, results=results)
            
            self.logger.info(f"ä»»åŠ¡å®Œæˆ: {experiment_name}")
            return results
            
        except Exception as e:
            self.logger.error(f"ä»»åŠ¡æ‰§è¡Œå¤±è´¥: {e}", exc_info=True)
            raise
        
        finally:
            if use_recorder:
                ctx.__exit__(None, None, None)
    
    def _init_dataset(self, dataset_config: Dict[str, Any]):
        """
        åˆå§‹åŒ–æ•°æ®é›†
        
        ğŸ†• ä¿®å¤: æ­£ç¡®å¤„ç† DataConfigï¼Œä¸å†å°†å…¶ä¼ ç»™ run_full_pipeline çš„ file_path å‚æ•°
        
        Args:
            dataset_config: æ•°æ®é›†é…ç½®å­—å…¸
            
        Returns:
            (loaders, data_manager): æ•°æ®åŠ è½½å™¨å’Œç®¡ç†å™¨
        """
        # æ”¯æŒ BaseConfig å¯¹è±¡
        if _is_dataclass_config(dataset_config):
            dataset_config = _config_to_dict(dataset_config)
        
        data_manager = None
        
        if dataset_config.get('class') == 'DataManager':
            # ğŸ†• ä¿®å¤: æ£€æŸ¥ kwargs ä¸­æ˜¯å¦æœ‰ config
            kwargs = dataset_config.get('kwargs', {})
            config_dict = kwargs.get('config', {})
            
            if config_dict:
                from ..data_set.config import DataConfig
                from ..data_set.manager import DataManager
                
                # æ„å»º DataConfig å¯¹è±¡
                if isinstance(config_dict, dict):
                    data_config = DataConfig(**config_dict)
                else:
                    data_config = config_dict
                
                # ğŸ†• æ£€æŸ¥å¹¶è­¦å‘Šå›¾æ„å»ºå™¨é…ç½®ç¼ºå¤±
                if data_config.graph_builder_config is None:
                    self.logger.warning(
                        "âš ï¸ graph_builder_config æœªé…ç½®ï¼\n"
                        "   æ¨¡å‹å°†ä½¿ç”¨å•ä½çŸ©é˜µï¼ˆæ— å›¾äº¤äº’ï¼‰ï¼Œå¯èƒ½å¯¼è‡´æ€§èƒ½ä¸‹é™ã€‚\n"
                        "   å»ºè®®åœ¨ DataConfig ä¸­é…ç½® graph_builder_configã€‚"
                    )
                
                # ğŸ†• ä¿®å¤: ç”¨ DataConfig åˆå§‹åŒ– DataManagerï¼Œè€Œä¸æ˜¯ä¼ ç»™ run_full_pipeline
                manager = DataManager(config=data_config)
                data_manager = manager
                
                # ğŸ†• ä¿®å¤: run_full_pipeline ä¸ä¼ å‚æ•°ï¼ˆæˆ–ä»…ä¼ å¯é€‰å‚æ•°ï¼‰
                loaders = manager.run_full_pipeline()
                return loaders, data_manager
            else:
                # æ²¡æœ‰ configï¼Œä½¿ç”¨æ—§ç‰ˆåˆå§‹åŒ–æ–¹å¼
                manager = init_instance_by_config(dataset_config)
                data_manager = manager
                return manager, data_manager
        
        return init_instance_by_config(dataset_config), None
    
    def _init_model(self, model_config: Dict[str, Any]):
        """åˆå§‹åŒ–æ¨¡å‹ï¼Œæ”¯æŒ BaseConfig å¯¹è±¡"""
        if _is_dataclass_config(model_config):
            model_config = _config_to_dict(model_config)
        
        return init_instance_by_config(model_config)
    
    def _train_model(self, model, dataset, task_config: Dict[str, Any]) -> Dict[str, Any]:
        """é»˜è®¤è®­ç»ƒæ–¹å¼ï¼šè°ƒç”¨æ¨¡å‹çš„ fit æ–¹æ³•"""
        results = {}
        
        if not hasattr(model, 'fit'):
            self.logger.warning("æ¨¡å‹æ²¡æœ‰fitæ–¹æ³•ï¼Œè·³è¿‡è®­ç»ƒ")
            return results
        
        if hasattr(dataset, 'train') and hasattr(dataset, 'val'):
            train_loader = dataset.train
            val_loader = dataset.val
            test_loader = dataset.test if hasattr(dataset, 'test') else None
            
            model.fit(train_loader, val_loader)
            
            if hasattr(model, 'best_metrics'):
                results['metrics'] = model.best_metrics
            
            if test_loader is not None:
                predictions = model.predict(test_loader)
                results['predictions'] = predictions
        else:
            model.fit(dataset)
        
        results['model'] = model
        return results
    
    # ==================== ğŸ†• SimpleTrainer è®­ç»ƒæ–¹æ³• ====================
    
    def _train_simple(self, model, dataset, trainer_kwargs: Dict[str, Any]) -> Dict[str, Any]:
        """
        ä½¿ç”¨ SimpleTrainer è¿›è¡Œè®­ç»ƒ
        
        Args:
            model: æ¨¡å‹
            dataset: æ•°æ®é›†ï¼ˆéœ€è¦æœ‰ train, val å±æ€§ï¼‰
            trainer_kwargs: è®­ç»ƒå™¨å‚æ•°
            
        Returns:
            è®­ç»ƒç»“æœå­—å…¸
        """
        from ..model.train import SimpleTrainer, TrainerConfig
        
        self.logger.info("ä½¿ç”¨ SimpleTrainer è¿›è¡Œè®­ç»ƒ")
        
        # è·å–åº•å±‚ nn.Module
        if hasattr(model, 'model'):
            nn_model = model.model
        else:
            nn_model = model
        
        # åˆ›å»ºé…ç½®
        config = TrainerConfig(**trainer_kwargs) if trainer_kwargs else TrainerConfig()
        
        # åˆ›å»ºè®­ç»ƒå™¨
        trainer = SimpleTrainer(nn_model, config)
        
        # è·å–æ•°æ®åŠ è½½å™¨
        train_loader = dataset.train if hasattr(dataset, 'train') else dataset
        val_loader = dataset.val if hasattr(dataset, 'val') else None
        test_loader = dataset.test if hasattr(dataset, 'test') else None
        
        # è®­ç»ƒ
        result = trainer.train(train_loader, val_loader)
        
        # é¢„æµ‹
        if test_loader is not None:
            predictions = trainer.predict(test_loader)
            result['predictions'] = predictions
        
        result['model'] = model
        result['trainer'] = trainer
        
        return result
    
    # ==================== ğŸ†• RollingWindowTrainer è®­ç»ƒæ–¹æ³• ====================
    
    def _train_rolling_window(self, model, rolling_loaders, trainer_kwargs: Dict[str, Any]) -> Dict[str, Any]:
        """
        ä½¿ç”¨ RollingWindowTrainer è¿›è¡Œæ»šåŠ¨çª—å£è®­ç»ƒ
        
        Args:
            model: æ¨¡å‹
            rolling_loaders: æ»šåŠ¨çª—å£æ•°æ®åŠ è½½å™¨
            trainer_kwargs: è®­ç»ƒå™¨å‚æ•°
            
        Returns:
            è®­ç»ƒç»“æœå­—å…¸
        """
        from ..model.train import RollingWindowTrainer, RollingTrainerConfig
        
        self.logger.info("ä½¿ç”¨ RollingWindowTrainer è¿›è¡Œæ»šåŠ¨çª—å£è®­ç»ƒ")
        
        # è·å–åº•å±‚ nn.Module
        if hasattr(model, 'model'):
            nn_model = model.model
        else:
            nn_model = model
        
        # åˆ›å»ºæ¨¡å‹å·¥å‚
        initial_model_copy = copy.deepcopy(nn_model)
        
        def model_factory():
            return copy.deepcopy(initial_model_copy)
        
        # ğŸ†• ä¿®å¤: init_params ä¸­çš„å‚æ•°åŒæ—¶é€ä¼ ç»™ config
        init_params = {'weight_inheritance', 'save_each_window', 'device'}
        fit_params = {'save_dir', 'n_epochs'}
        
        init_kwargs = {}
        fit_kwargs = {}
        config_kwargs = {}
        
        for key, value in trainer_kwargs.items():
            if key in init_params:
                init_kwargs[key] = value
                # ğŸ†• weight_inheritance å’Œ save_each_window åŒæ—¶ä¼ å…¥ config
                if key in {'weight_inheritance', 'save_each_window'}:
                    config_kwargs[key] = value
            elif key in fit_params:
                fit_kwargs[key] = value
            else:
                config_kwargs[key] = value
        
        # åˆ›å»ºé…ç½®
        config = RollingTrainerConfig(**config_kwargs) if config_kwargs else RollingTrainerConfig()
        
        # åˆ›å»ºè®­ç»ƒå™¨
        trainer = RollingWindowTrainer(
            model_factory=model_factory,
            config=config,
            device=init_kwargs.get('device', 'cuda')
        )
        
        # è®­ç»ƒ
        save_dir = fit_kwargs.get('save_dir', 'output/rolling_models')
        n_epochs = fit_kwargs.get('n_epochs')
        
        results = trainer.train(rolling_loaders, n_epochs=n_epochs, save_dir=save_dir)
        
        # è·å–é¢„æµ‹
        all_predictions = trainer.get_all_predictions()
        results['predictions'] = all_predictions
        results['model'] = model
        results['trainer'] = trainer
        
        return results
    
    def _run_backtest(self, model, dataset, backtest_config: Dict[str, Any]) -> Dict[str, Any]:
        """æ‰§è¡Œå›æµ‹"""
        from ..backtest import BacktestSystem
        
        if hasattr(dataset, 'test'):
            test_loader = dataset.test
            predictions = model.predict(test_loader)
        else:
            self.logger.warning("æ•°æ®é›†æ²¡æœ‰testéƒ¨åˆ†ï¼Œè·³è¿‡å›æµ‹")
            return {}
        
        backtest_system = BacktestSystem(**backtest_config)
        backtest_results = backtest_system.run_backtest(predictions=predictions, **backtest_config)
        
        return {'metrics': backtest_results, 'predictions': predictions}
    
    # ==================== ğŸ†• æ»šåŠ¨è®­ç»ƒæ–¹æ³•ï¼ˆé‡æ„ï¼‰ ====================
    
    def _train_rolling(self, model, rolling_loaders, trainer_kwargs: Dict[str, Any]) -> Dict[str, Any]:
        """
        ä½¿ç”¨ RollingDailyTrainer è¿›è¡Œæ»šåŠ¨çª—å£è®­ç»ƒ
        
        ğŸ†• é‡æ„: ä½¿ç”¨æ–°çš„ model/train/ æ¨¡å—
        
        Args:
            model: æ¨¡å‹ï¼ˆéœ€è¦æ˜¯ nn.Module æˆ–æœ‰ .model å±æ€§ï¼‰
            rolling_loaders: RollingDailyLoaderCollection
            trainer_kwargs: è®­ç»ƒå™¨å‚æ•°
            
        Returns:
            è®­ç»ƒç»“æœå­—å…¸
        """
        # ğŸ†• ä¼˜å…ˆä½¿ç”¨æ–°çš„è®­ç»ƒæ¶æ„
        try:
            from ..model.train import RollingDailyTrainer, RollingTrainerConfig
            use_new_trainer = True
        except ImportError:
            from ..model.rolling_daily_trainer import RollingDailyTrainer, RollingTrainerConfig
            use_new_trainer = False
        
        self.logger.info(f"ä½¿ç”¨ {'æ–°' if use_new_trainer else 'æ—§'} RollingDailyTrainer è¿›è¡Œæ»šåŠ¨çª—å£è®­ç»ƒ")
        
        # è·å–åº•å±‚ nn.Module
        if hasattr(model, 'model'):
            nn_model = model.model
        else:
            nn_model = model
        
        # ä½¿ç”¨ copy.deepcopy åˆ›å»ºæ¨¡å‹å·¥å‚
        initial_model_copy = copy.deepcopy(nn_model)
        
        def model_factory():
            """æ¨¡å‹å·¥å‚ï¼šè¿”å›åˆå§‹çŠ¶æ€æ¨¡å‹çš„æ·±æ‹·è´"""
            return copy.deepcopy(initial_model_copy)
        
        # ğŸ†• æ‹†åˆ† trainer_kwargs
        from dataclasses import fields as dc_fields
        
        try:
            config_field_names = {f.name for f in dc_fields(RollingTrainerConfig)}
        except Exception:
            config_field_names = set()
        
        # RollingDailyTrainer æ„é€ å‡½æ•°æ¥å—çš„å‚æ•°
        trainer_init_params = {'warm_start', 'save_each_window', 'device'}
        
        # trainer.fit() æ¥å—çš„å‚æ•°
        fit_params = {'save_dir', 'n_epochs'}
        
        # åˆ†ç¦»å‚æ•°
        config_kwargs = {}
        init_kwargs = {}
        fit_kwargs = {}
        
        for key, value in trainer_kwargs.items():
            if key in trainer_init_params:
                init_kwargs[key] = value
            elif key in fit_params:
                fit_kwargs[key] = value
            elif key in config_field_names:
                config_kwargs[key] = value
            else:
                config_kwargs[key] = value
        
        # åˆ›å»ºè®­ç»ƒé…ç½®
        if use_new_trainer:
            from ..model.train.rolling_daily_trainer import DailyRollingConfig
            try:
                config = DailyRollingConfig(**config_kwargs) if config_kwargs else DailyRollingConfig()
            except TypeError as e:
                self.logger.warning(f"åˆ›å»º DailyRollingConfig å¤±è´¥: {e}ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
                config = DailyRollingConfig()
        else:
            config = RollingTrainerConfig(**config_kwargs) if config_kwargs else RollingTrainerConfig()
        
        # åˆ›å»ºè®­ç»ƒå™¨
        trainer = RollingDailyTrainer(
            model_factory=model_factory,
            config=config,
            warm_start=init_kwargs.get('warm_start', True),
            save_each_window=init_kwargs.get('save_each_window', True),
            device=init_kwargs.get('device', 'cuda')
        )
        
        # è®­ç»ƒ
        save_dir = fit_kwargs.get('save_dir', 'output/rolling_models')
        n_epochs = fit_kwargs.get('n_epochs', config.n_epochs)
        
        results = trainer.fit(rolling_loaders, n_epochs=n_epochs, save_dir=save_dir)
        
        # è·å–æ‰€æœ‰é¢„æµ‹
        all_predictions = trainer.get_all_predictions()
        results['predictions'] = all_predictions
        results['trainer'] = trainer
        
        return results
    
    def _train_dynamic_graph(self, model, daily_loaders, trainer_kwargs: Dict[str, Any]) -> Dict[str, Any]:
        """
        ä½¿ç”¨ SimpleTrainer è¿›è¡ŒåŠ¨æ€å›¾è®­ç»ƒ
        
        ğŸ†• é‡æ„: DynamicGraphTrainer å·²åºŸå¼ƒï¼Œæ”¹ç”¨ SimpleTrainer
        
        Args:
            model: æ¨¡å‹
            daily_loaders: DailyLoaderCollection (train, val, test)
            trainer_kwargs: è®­ç»ƒå™¨å‚æ•°
            
        Returns:
            è®­ç»ƒç»“æœå­—å…¸
        """
        # ğŸ†• ä½¿ç”¨æ–°çš„ SimpleTrainer æ›¿ä»£å·²åºŸå¼ƒçš„ DynamicGraphTrainer
        from ..model.train import SimpleTrainer, TrainerConfig
        
        self.logger.info("ä½¿ç”¨ SimpleTrainer è¿›è¡ŒåŠ¨æ€å›¾è®­ç»ƒ (DynamicGraphTrainer å·²åºŸå¼ƒ)")
        
        # è·å–åº•å±‚ nn.Module
        if hasattr(model, 'model'):
            nn_model = model.model
        else:
            nn_model = model
        
        # æ‹†åˆ†å‚æ•°
        fit_params = {'save_path', 'n_epochs'}
        
        config_kwargs = {}
        fit_kwargs = {}
        
        for key, value in trainer_kwargs.items():
            if key in fit_params:
                fit_kwargs[key] = value
            else:
                config_kwargs[key] = value
        
        # åˆ›å»ºè®­ç»ƒé…ç½®
        config = TrainerConfig(**config_kwargs) if config_kwargs else TrainerConfig()
        
        # åˆ›å»ºè®­ç»ƒå™¨
        trainer = SimpleTrainer(
            model=nn_model,
            config=config,
            device=trainer_kwargs.get('device', 'cuda')
        )
        
        # è®­ç»ƒ
        save_path = fit_kwargs.get('save_path', 'output/best_model.pth')
        n_epochs = fit_kwargs.get('n_epochs', config.n_epochs)
        
        # å‡†å¤‡ DataLoader
        train_loader = daily_loaders.train if hasattr(daily_loaders, 'train') else daily_loaders
        val_loader = daily_loaders.val if hasattr(daily_loaders, 'val') else None
        
        results = trainer.train(
            train_loader=train_loader,
            val_loader=val_loader,
            n_epochs=n_epochs,
            save_path=save_path
        )
        
        # é¢„æµ‹
        if hasattr(daily_loaders, 'test') and daily_loaders.test:
            predictions = trainer.predict(daily_loaders.test)
            results['predictions'] = predictions
        
        results['trainer'] = trainer
        
        return results
    
    def _flatten_config(self, config: Dict[str, Any], parent_key: str = '', sep: str = '_') -> Dict[str, Any]:
        """å°†åµŒå¥—é…ç½®å±•å¹³ä¸ºå•å±‚å­—å…¸"""
        items = []
        for k, v in config.items():
            new_key = f"{parent_key}{sep}{k}" if parent_key else k
            
            if isinstance(v, dict) and k not in ['kwargs']:
                items.extend(self._flatten_config(v, new_key, sep=sep).items())
            elif isinstance(v, (str, int, float, bool)) or v is None:
                items.append((new_key, v))
        
        return dict(items)
